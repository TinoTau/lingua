//! åŸºäº Speaker Embedding çš„è¯´è¯è€…è¯†åˆ«
//! 
//! è¿™æ˜¯ä¸€ä¸ªå‡†ç¡®çš„å®ç°ï¼Œé€‚ç”¨äºä»˜è´¹ç”¨æˆ·ï¼š
//! - ä½¿ç”¨è½»é‡çº§ Speaker Embedding æ¨¡å‹ï¼ˆå¦‚ ECAPA-TDNNï¼‰
//! - æå–éŸ³é¢‘ç‰‡æ®µçš„è¯´è¯è€…ç‰¹å¾å‘é‡
//! - ä¸å·²æœ‰è¯´è¯è€…çš„ embedding æ¯”è¾ƒï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºæ–°è¯´è¯è€…
//! 
//! æ³¨æ„ï¼šå½“å‰ä¸ºå ä½ç¬¦å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦é›†æˆ Speaker Embedding æ¨¡å‹

use async_trait::async_trait;
use std::sync::Arc;
use tokio::sync::RwLock;
use std::collections::HashMap;

use crate::error::{EngineError, EngineResult};
use crate::types::AudioFrame;
use super::{SpeakerIdentifier, SpeakerIdentificationResult, SpeakerEmbeddingClient, SpeakerEmbeddingClientConfig, EmbeddingBasedMode};

/// æå– embedding çš„ç»“æœ
struct ExtractResult {
    embedding: Option<Vec<f32>>,
    estimated_gender: Option<String>,
}

/// åŸºäº Speaker Embedding çš„è¯´è¯è€…è¯†åˆ«å™¨
pub struct EmbeddingBasedSpeakerIdentifier {
    /// HTTP å®¢æˆ·ç«¯ï¼ˆç”¨äºè°ƒç”¨ Python æœåŠ¡ï¼‰
    embedding_client: SpeakerEmbeddingClient,
    /// ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯åŒä¸€è¯´è¯è€…
    similarity_threshold: f32,
    /// è¯†åˆ«æ¨¡å¼ï¼šå•äººæ¨¡å¼æˆ–å¤šäººæ¨¡å¼ï¼ˆå¯åŠ¨æ€åˆ‡æ¢ï¼‰
    mode: Arc<RwLock<EmbeddingBasedMode>>,
    /// å·²æœ‰è¯´è¯è€…çš„ embedding åº“ï¼ˆæŒ‰æ¨¡å¼åˆ†å¼€å­˜å‚¨ï¼‰
    /// Key: speaker_id, Value: embedding vector
    /// å•äººæ¨¡å¼ä½¿ç”¨ "single_user" ä½œä¸º keyï¼Œå¤šäººæ¨¡å¼ä½¿ç”¨ "default_male"/"default_female" ç­‰
    speaker_embeddings: Arc<RwLock<HashMap<String, Vec<f32>>>>,
    /// ä¸‹ä¸€ä¸ªè¯´è¯è€… ID çš„è®¡æ•°å™¨ï¼ˆå¤šäººæ¨¡å¼ä½¿ç”¨ï¼‰
    next_speaker_id: Arc<RwLock<u32>>,
    /// æ¯ä¸ªè¯´è¯è€…çš„å‚è€ƒéŸ³é¢‘ç‰‡æ®µåˆ—è¡¨ï¼ˆç”¨äºåˆå¹¶ï¼ŒæŒ‰æ¨¡å¼åˆ†å¼€å­˜å‚¨ï¼‰
    /// Key: speaker_id, Value: Vec<å‚è€ƒéŸ³é¢‘ç‰‡æ®µ>
    /// å½“ç´¯ç§¯åˆ°è¶³å¤Ÿé•¿åº¦æ—¶ï¼Œä¼šåˆå¹¶æˆä¸€ä¸ªæ›´é•¿çš„å‚è€ƒéŸ³é¢‘
    speaker_reference_audio_segments: Arc<RwLock<HashMap<String, Vec<Vec<f32>>>>>,
    /// åˆå¹¶å‚è€ƒéŸ³é¢‘çš„æœ€å°æ€»é•¿åº¦ï¼ˆæ ·æœ¬æ•°ï¼Œ16kHzï¼Œçº¦ 10 ç§’ï¼‰
    min_merged_audio_samples: usize,
    /// å•äººæ¨¡å¼ä¸‹çš„å›ºå®š speaker_id
    single_user_speaker_id: Arc<RwLock<Option<String>>>,
}

impl EmbeddingBasedSpeakerIdentifier {
    /// åˆ›å»ºæ–°çš„åŸºäº Speaker Embedding çš„è¯´è¯è€…è¯†åˆ«å™¨
    /// 
    /// # Arguments
    /// * `service_url` - HTTP æœåŠ¡ç«¯ç‚¹ï¼ˆä¾‹å¦‚ï¼šhttp://127.0.0.1:5003ï¼‰
    /// * `similarity_threshold` - ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
    /// * `mode` - è¯†åˆ«æ¨¡å¼ï¼šå•äººæ¨¡å¼æˆ–å¤šäººæ¨¡å¼
    pub fn new(
        service_url: Option<String>,
        similarity_threshold: f32,
        mode: EmbeddingBasedMode,
    ) -> EngineResult<Self> {
        let config = SpeakerEmbeddingClientConfig {
            endpoint: service_url.unwrap_or_else(|| "http://127.0.0.1:5003".to_string()),
            timeout_ms: 5000,
        };
        
        let embedding_client = SpeakerEmbeddingClient::new(config)?;
        
        Ok(Self {
            embedding_client,
            similarity_threshold,
            mode: Arc::new(RwLock::new(mode)),  // ä½¿ç”¨ Arc<RwLock> ä»¥æ”¯æŒåŠ¨æ€åˆ‡æ¢
            speaker_embeddings: Arc::new(RwLock::new(HashMap::new())),
            next_speaker_id: Arc::new(RwLock::new(1)),
            speaker_reference_audio_segments: Arc::new(RwLock::new(HashMap::new())),
            min_merged_audio_samples: 160000,  // 16kHz * 10ç§’ = 160000 æ ·æœ¬
            single_user_speaker_id: Arc::new(RwLock::new(None)),
        })
    }
    
    /// ç”Ÿæˆæ–°çš„è¯´è¯è€… ID
    async fn generate_speaker_id(&self) -> String {
        let mut counter = self.next_speaker_id.write().await;
        let id = format!("speaker_{}", *counter);
        *counter += 1;
        id
    }
    
    /// æå–éŸ³é¢‘çš„ speaker embedding
    /// 
    /// é€šè¿‡ HTTP æœåŠ¡è°ƒç”¨ Python æœåŠ¡æå–ç‰¹å¾å‘é‡
    /// å¦‚æœéŸ³é¢‘å¤ªçŸ­ï¼Œè¿”å› None å’Œä¼°è®¡çš„æ€§åˆ«
    async fn extract_embedding(&self, audio_segment: &[AudioFrame]) -> EngineResult<ExtractResult> {
        use std::time::Instant;
        let start_time = Instant::now();
        
        if audio_segment.is_empty() {
            return Err(crate::error::EngineError::new("Empty audio segment"));
        }
        
        eprintln!("[SpeakerIdentifier] ===== Extract Embedding Started =====");
        eprintln!("[SpeakerIdentifier] Audio segment: {} frames", audio_segment.len());
        
        // 1. åˆå¹¶éŸ³é¢‘å¸§
        let merge_start = Instant::now();
        let mut merged_audio = Vec::new();
        let mut total_samples = 0;
        let mut sample_rate = 16000u32;
        for frame in audio_segment {
            // ç¡®ä¿é‡‡æ ·ç‡æ˜¯ 16kHzï¼ˆECAPA-TDNN è¦æ±‚ï¼‰
            if frame.sample_rate != 16000 {
                // TODO: é‡é‡‡æ ·åˆ° 16kHzï¼ˆå½“å‰å‡è®¾å·²ç»æ˜¯ 16kHzï¼‰
                eprintln!("[SpeakerIdentifier] âš  Warning: Audio sample rate is {}Hz, expected 16kHz", frame.sample_rate);
            }
            sample_rate = frame.sample_rate;
            merged_audio.extend_from_slice(&frame.data);
            total_samples += frame.data.len();
        }
        let merge_ms = merge_start.elapsed().as_millis() as u64;
        let duration_sec = total_samples as f32 / sample_rate as f32;
        let duration_ms = (duration_sec * 1000.0) as u64;
        eprintln!("[SpeakerIdentifier] Merged {} frames into {} samples in {}ms", 
                  audio_segment.len(), total_samples, merge_ms);
        eprintln!("[SpeakerIdentifier] Input audio duration: {:.2}s ({:.0}ms) at {}Hz", 
                  duration_sec, duration_ms, sample_rate);
        
        // 2. è°ƒç”¨ HTTP æœåŠ¡æå– embedding
        eprintln!("[SpeakerIdentifier] Calling Speaker Embedding service...");
        let extract_result = self.embedding_client.extract_embedding(&merged_audio).await?;
        
        let total_ms = start_time.elapsed().as_millis() as u64;
        
        if extract_result.use_default {
            let gender = extract_result.estimated_gender.as_deref().unwrap_or("unknown");
            eprintln!("[SpeakerIdentifier] âš  Using default voice (audio too short, estimated gender: {})", gender);
            eprintln!("[SpeakerIdentifier] âœ… Extract embedding completed in {}ms (using default voice)", total_ms);
            eprintln!("[SpeakerIdentifier] ==========================================");
            return Ok(ExtractResult {
                embedding: None,
                estimated_gender: extract_result.estimated_gender,
            });
        }
        
        let embedding = extract_result.embedding.ok_or_else(|| {
            EngineError::new("Embedding extraction returned no embedding")
        })?;
        
        eprintln!("[SpeakerIdentifier] âœ… Extract embedding completed in {}ms (merge: {}ms, service: {}ms)", 
                  total_ms, merge_ms, total_ms - merge_ms);
        eprintln!("[SpeakerIdentifier] ==========================================");
        
        Ok(ExtractResult {
            embedding: Some(embedding),
            estimated_gender: None,
        })
    }
    
    /// è®¡ç®—ä¸¤ä¸ª embedding çš„ä½™å¼¦ç›¸ä¼¼åº¦
    fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
        if a.len() != b.len() {
            return 0.0;
        }
        
        let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
        let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
        let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
        
        if norm_a == 0.0 || norm_b == 0.0 {
            return 0.0;
        }
        
        dot_product / (norm_a * norm_b)
    }
    
    /// æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„è¯´è¯è€…
    async fn find_most_similar_speaker(
        &self,
        embedding: &[f32],
    ) -> Option<(String, f32)> {
        let embeddings = self.speaker_embeddings.read().await;
        
        if embeddings.is_empty() {
            eprintln!("[SpeakerIdentifier] ğŸ“Š No existing speakers in database");
            return None;
        }
        
        eprintln!("[SpeakerIdentifier] ğŸ“Š Comparing with {} existing speaker(s)...", embeddings.len());
        
        let mut best_match: Option<(String, f32)> = None;
        let mut all_similarities: Vec<(String, f32)> = Vec::new();
        
        for (speaker_id, speaker_embedding) in embeddings.iter() {
            let similarity = Self::cosine_similarity(embedding, speaker_embedding);
            all_similarities.push((speaker_id.clone(), similarity));
            
            if let Some((_, best_sim)) = best_match {
                if similarity > best_sim {
                    best_match = Some((speaker_id.clone(), similarity));
                }
            } else {
                best_match = Some((speaker_id.clone(), similarity));
            }
        }
        
        // æ‰“å°æ‰€æœ‰ç›¸ä¼¼åº¦å€¼ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        eprintln!("[SpeakerIdentifier] ğŸ“Š Similarity scores:");
        for (sid, sim) in all_similarities.iter() {
            eprintln!("[SpeakerIdentifier]   - {}: {:.4}", sid, sim);
        }
        
        if let Some((best_id, best_sim)) = best_match.as_ref() {
            eprintln!("[SpeakerIdentifier] ğŸ¯ Best match: {} (similarity: {:.4})", best_id, best_sim);
        }
        
        best_match
    }
    
    /// å•äººæ¨¡å¼ï¼šæ‰€æœ‰è¯­éŸ³è§†ä¸ºåŒä¸€ç”¨æˆ·ï¼Œåˆå¹¶ä¸è¶³7ç§’çš„éŸ³é¢‘åˆ°10ç§’å·¦å³ï¼ŒæŒç»­ä¼˜åŒ–éŸ³è‰²
    async fn identify_single_user_mode(
        &self,
        audio_segment: &[AudioFrame],
    ) -> EngineResult<SpeakerIdentificationResult> {
        eprintln!("[SpeakerIdentifier] ğŸ”µ Single User Mode: treating all audio as same user");
        
        // 1. è·å–æˆ–åˆ›å»ºå›ºå®šçš„ speaker_id
        let speaker_id = {
            let mut single_id = self.single_user_speaker_id.write().await;
            if single_id.is_none() {
                *single_id = Some("single_user".to_string());
                eprintln!("[SpeakerIdentifier] ğŸ†• Created single user speaker_id: single_user");
            }
            single_id.clone().unwrap()
        };
        
        // 2. åˆå¹¶å½“å‰éŸ³é¢‘ç‰‡æ®µ
        let mut current_audio = Vec::new();
        for frame in audio_segment {
            current_audio.extend_from_slice(&frame.data);
        }
        
        let current_duration_sec = current_audio.len() as f32 / 16000.0;
        eprintln!("[SpeakerIdentifier] ğŸ“Š Current audio segment: {:.2}s ({} samples @ 16kHz)", 
                 current_duration_sec, current_audio.len());
        
        // 3. ç´¯ç§¯éŸ³é¢‘ç‰‡æ®µï¼ˆåˆå¹¶ä¸è¶³7ç§’çš„éŸ³é¢‘åˆ°10ç§’å·¦å³ï¼‰
        let mut segments = self.speaker_reference_audio_segments.write().await;
        let segments_list = segments.entry(speaker_id.clone()).or_insert_with(Vec::new);
        segments_list.push(current_audio.clone());
        
        // è®¡ç®—ç´¯ç§¯çš„æ€»é•¿åº¦
        let total_samples: usize = segments_list.iter().map(|seg| seg.len()).sum();
        let total_duration_sec = total_samples as f32 / 16000.0;
        eprintln!("[SpeakerIdentifier] ğŸ“Š Accumulated audio: {} segments, {:.2}s total", 
                 segments_list.len(), total_duration_sec);
        
        // 4. å¦‚æœç´¯ç§¯çš„éŸ³é¢‘è¾¾åˆ°çº¦7ç§’ï¼ˆ112000æ ·æœ¬ï¼‰ï¼Œå°è¯•æå–ç‰¹å¾
        // å¦‚æœè¾¾åˆ°10ç§’ï¼ˆ160000æ ·æœ¬ï¼‰ï¼Œåˆå¹¶å¹¶æå–ç‰¹å¾
        let min_samples_for_extraction = 112000;  // 7ç§’ @ 16kHz
        let reference_audio = if total_samples >= self.min_merged_audio_samples {
            // è¾¾åˆ°10ç§’ï¼Œåˆå¹¶æ‰€æœ‰ç‰‡æ®µ
            eprintln!("[SpeakerIdentifier] ğŸ”— Merging {} reference audio segments (total: {:.2}s)", 
                     segments_list.len(), total_duration_sec);
            let merged: Vec<f32> = segments_list.iter().flat_map(|seg| seg.iter().cloned()).collect();
            // ä¿ç•™åˆå¹¶åçš„éŸ³é¢‘ï¼Œä½†ä¸æ¸…ç©ºï¼ˆç»§ç»­ç´¯ç§¯ä»¥æŒç»­ä¼˜åŒ–ï¼‰
            segments_list.clear();
            segments_list.push(merged.clone());
            eprintln!("[SpeakerIdentifier] âœ… Merged reference audio ready ({} samples, {:.2}s)", 
                     merged.len(), merged.len() as f32 / 16000.0);
            Some(merged)
        } else if total_samples >= min_samples_for_extraction {
            // è¾¾åˆ°7ç§’ï¼Œå¯ä»¥æå–ç‰¹å¾ï¼Œä½†ç»§ç»­ç´¯ç§¯åˆ°10ç§’
            eprintln!("[SpeakerIdentifier] âš ï¸  Audio reached {:.2}s (>= 7s), can extract features, but continuing to accumulate to 10s", 
                     total_duration_sec);
            // åˆå¹¶å½“å‰æ‰€æœ‰ç‰‡æ®µç”¨äºç‰¹å¾æå–
            let merged: Vec<f32> = segments_list.iter().flat_map(|seg| seg.iter().cloned()).collect();
            Some(merged)
        } else {
            // ä¸è¶³7ç§’ï¼Œç»§ç»­ç´¯ç§¯
            eprintln!("[SpeakerIdentifier] â³ Audio only {:.2}s (< 7s), continuing to accumulate", 
                     total_duration_sec);
            Some(current_audio)
        };
        
        // 5. æå– embedding å’Œæ€§åˆ«ä¿¡æ¯ï¼ˆå¦‚æœéŸ³é¢‘è¶³å¤Ÿé•¿ï¼‰
        let (embedding, estimated_gender) = if total_samples >= min_samples_for_extraction {
            // ä½¿ç”¨ç´¯ç§¯çš„éŸ³é¢‘æå– embedding
            let merged_for_embedding: Vec<f32> = segments_list.iter()
                .flat_map(|seg| seg.iter().cloned())
                .collect();
            
            // åˆ›å»ºä¸´æ—¶çš„ AudioFrame ç”¨äºæå– embedding
            let temp_frames: Vec<AudioFrame> = vec![AudioFrame {
                data: merged_for_embedding,
                sample_rate: 16000,
                channels: 1,
                timestamp_ms: 0,
            }];
            
            let extract_result = self.extract_embedding(&temp_frames).await?;
            let gender = extract_result.estimated_gender.clone();
            
            if let Some(emb) = extract_result.embedding {
                // æ›´æ–°æˆ–ä¿å­˜ embeddingï¼ˆæŒç»­ä¼˜åŒ–ï¼‰
                let mut embeddings = self.speaker_embeddings.write().await;
                if let Some(existing_emb) = embeddings.get_mut(&speaker_id) {
                    // ä½¿ç”¨åŠ æƒå¹³å‡æ›´æ–° embeddingï¼ˆæŒç»­ä¼˜åŒ–éŸ³è‰²ï¼‰
                    for (i, new_val) in emb.iter().enumerate() {
                        if i < existing_emb.len() {
                            existing_emb[i] = existing_emb[i] * 0.7 + new_val * 0.3;
                        }
                    }
                    eprintln!("[SpeakerIdentifier] ğŸ”„ Updated embedding for single user (weighted average: 0.7 old + 0.3 new)");
                } else {
                    // é¦–æ¬¡ä¿å­˜ embedding
                    embeddings.insert(speaker_id.clone(), emb.clone());
                    eprintln!("[SpeakerIdentifier] ğŸ’¾ Saved initial embedding for single user");
                }
                (Some(emb), gender)
            } else {
                // æå–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å½“å‰ç‰‡æ®µæå–æ€§åˆ«ä¿¡æ¯
                let current_extract = self.extract_embedding(audio_segment).await?;
                (None, current_extract.estimated_gender)
            }
        } else {
            // éŸ³é¢‘ä¸è¶³7ç§’ï¼Œæ— æ³•æå– embeddingï¼Œä½†å¯ä»¥æå–æ€§åˆ«ä¿¡æ¯
            let extract_result = self.extract_embedding(audio_segment).await?;
            (None, extract_result.estimated_gender)
        };
        
        Ok(SpeakerIdentificationResult {
            speaker_id,
            is_new_speaker: false,  // å•äººæ¨¡å¼ä¸‹å§‹ç»ˆæ˜¯åŒä¸€ç”¨æˆ·
            confidence: 1.0,  // å•äººæ¨¡å¼ä¸‹ç½®ä¿¡åº¦æœ€é«˜
            voice_embedding: embedding,
            reference_audio,
            estimated_gender,
        })
    }
    
    /// å¤šäººæ¨¡å¼ï¼šä»…åŒºåˆ†ç”·å¥³ï¼Œä½¿ç”¨é»˜è®¤çš„ç”·å£°æˆ–å¥³å£°
    async fn identify_multi_user_mode(
        &self,
        audio_segment: &[AudioFrame],
    ) -> EngineResult<SpeakerIdentificationResult> {
        eprintln!("[SpeakerIdentifier] ğŸŸ¢ Multi User Mode: only distinguishing gender");
        
        // 1. æå– embedding å’Œæ€§åˆ«ä¿¡æ¯
        let extract_result = self.extract_embedding(audio_segment).await?;
        
        // 2. æ ¹æ®æ€§åˆ«åˆ†é… speaker_idï¼ˆä»…åŒºåˆ†ç”·å¥³ï¼‰
        let estimated_gender = extract_result.estimated_gender.as_deref().unwrap_or("unknown");
        let speaker_id = match estimated_gender.to_lowercase().as_str() {
            "male" | "m" => "default_male".to_string(),
            "female" | "f" => "default_female".to_string(),
            _ => "default_speaker".to_string(),  // æœªçŸ¥æ€§åˆ«ä½¿ç”¨é€šç”¨é»˜è®¤
        };
        
        eprintln!("[SpeakerIdentifier] ğŸ‘¤ Gender-based speaker ID: {} (estimated gender: {})", 
                 speaker_id, estimated_gender);
        
        // 3. å¤šäººæ¨¡å¼ä¸‹ä¸ä½¿ç”¨å‚è€ƒéŸ³é¢‘å’Œ embeddingï¼ˆä½¿ç”¨é»˜è®¤éŸ³è‰²ï¼‰
        Ok(SpeakerIdentificationResult {
            speaker_id,
            is_new_speaker: false,  // é»˜è®¤è¯´è¯è€…ä¸ç®—æ–°è¯´è¯è€…
            confidence: 0.8,  // åŸºäºæ€§åˆ«çš„è¯†åˆ«ç½®ä¿¡åº¦
            voice_embedding: None,  // ä¸ä½¿ç”¨ embedding
            reference_audio: None,  // ä¸ä½¿ç”¨å‚è€ƒéŸ³é¢‘
            estimated_gender: extract_result.estimated_gender,
        })
    }
    
    /// åŠ¨æ€åˆ‡æ¢æ¨¡å¼ï¼ˆä¸ä¼šæ¸…ç©ºå¦ä¸€ç§æ¨¡å¼çš„æ•°æ®ï¼‰
    pub async fn set_mode(&self, new_mode: EmbeddingBasedMode) {
        let mut mode = self.mode.write().await;
        let old_mode = format!("{:?}", *mode);
        *mode = new_mode;
        let new_mode_str = format!("{:?}", *mode);
        eprintln!("[SpeakerIdentifier] ğŸ”„ Mode switched from {} to {} (data preserved)", old_mode, new_mode_str);
    }

    /// è·å–å½“å‰æ¨¡å¼
    pub async fn get_mode(&self) -> EmbeddingBasedMode {
        self.mode.read().await.clone()
    }
}

#[async_trait]
impl SpeakerIdentifier for EmbeddingBasedSpeakerIdentifier {
    async fn identify_speaker(
        &self,
        audio_segment: &[AudioFrame],
        _boundary_timestamp_ms: u64,
    ) -> EngineResult<SpeakerIdentificationResult> {
        let current_mode = self.mode.read().await.clone();
        match current_mode {
            EmbeddingBasedMode::SingleUser => {
                self.identify_single_user_mode(audio_segment).await
            }
            EmbeddingBasedMode::MultiUser => {
                self.identify_multi_user_mode(audio_segment).await
            }
        }
    }
    
    async fn reset(&self) -> EngineResult<()> {
        let mut embeddings = self.speaker_embeddings.write().await;
        let mut counter = self.next_speaker_id.write().await;
        let mut segments = self.speaker_reference_audio_segments.write().await;
        let mut single_id = self.single_user_speaker_id.write().await;
        
        embeddings.clear();
        segments.clear();
        *counter = 1;
        *single_id = None;  // é‡ç½®å•äººæ¨¡å¼çš„ speaker_id
        
        Ok(())
    }
    
    fn get_info(&self) -> String {
        // æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨ asyncï¼Œæ‰€ä»¥ä½¿ç”¨ try_read æˆ–è¿”å›å›ºå®šä¿¡æ¯
        format!(
            "EmbeddingBasedSpeakerIdentifier(threshold={})",
            self.similarity_threshold
        )
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    fn create_test_frame(timestamp_ms: u64) -> AudioFrame {
        AudioFrame {
            sample_rate: 16000,
            channels: 1,
            data: vec![0.0; 512],
            timestamp_ms,
        }
    }
    
    #[tokio::test]
    #[ignore]  // éœ€è¦ HTTP æœåŠ¡è¿è¡Œ
    async fn test_first_speaker() {
        let identifier = EmbeddingBasedSpeakerIdentifier::new(
            Some("http://127.0.0.1:5003".to_string()),
            0.7,
            EmbeddingBasedMode::MultiUser,  // ä½¿ç”¨å¤šäººæ¨¡å¼è¿›è¡Œæµ‹è¯•
        ).unwrap();
        
        let result = identifier.identify_speaker(&[create_test_frame(0)], 0).await.unwrap();
        // åœ¨å¤šäººæ¨¡å¼ä¸‹ï¼Œspeaker_id å¯èƒ½æ˜¯ default_male æˆ– default_female
        assert!(result.speaker_id.starts_with("default_") || result.speaker_id.starts_with("speaker_"));
    }
    
    #[tokio::test]
    #[ignore]  // éœ€è¦ HTTP æœåŠ¡è¿è¡Œ
    async fn test_single_user_mode() {
        let identifier = EmbeddingBasedSpeakerIdentifier::new(
            Some("http://127.0.0.1:5003".to_string()),
            0.7,
            EmbeddingBasedMode::SingleUser,
        ).unwrap();
        
        let result = identifier.identify_speaker(&[create_test_frame(0)], 0).await.unwrap();
        assert_eq!(result.speaker_id, "single_user");
        assert!(!result.is_new_speaker);  // å•äººæ¨¡å¼ä¸‹å§‹ç»ˆä¸æ˜¯æ–°è¯´è¯è€…
    }
    
    #[tokio::test]
    #[ignore]  // éœ€è¦ HTTP æœåŠ¡è¿è¡Œ
    async fn test_reset() {
        let identifier = EmbeddingBasedSpeakerIdentifier::new(
            Some("http://127.0.0.1:5003".to_string()),
            0.7,
            EmbeddingBasedMode::MultiUser,
        ).unwrap();
        
        identifier.identify_speaker(&[create_test_frame(0)], 0).await.unwrap();
        identifier.reset().await.unwrap();
        
        let result = identifier.identify_speaker(&[create_test_frame(1000)], 1000).await.unwrap();
        // é‡ç½®åï¼Œspeaker_id åº”è¯¥é‡æ–°å¼€å§‹
        assert!(result.speaker_id.starts_with("default_") || result.speaker_id.starts_with("speaker_"));
    }
    
    #[tokio::test]
    async fn test_cosine_similarity() {
        let a = vec![1.0, 0.0, 0.0];
        let b = vec![1.0, 0.0, 0.0];
        let similarity = EmbeddingBasedSpeakerIdentifier::cosine_similarity(&a, &b);
        assert!((similarity - 1.0).abs() < 0.001);
        
        let c = vec![0.0, 1.0, 0.0];
        let similarity = EmbeddingBasedSpeakerIdentifier::cosine_similarity(&a, &c);
        assert!((similarity - 0.0).abs() < 0.001);
    }
}

