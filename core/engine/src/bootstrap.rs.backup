use std::path::PathBuf;
use std::sync::Arc;

use crate::asr_streaming::{AsrStreaming, AsrResult};
use crate::asr_whisper::WhisperAsrStreaming;
use crate::audio_buffer::{AudioBufferManager, merge_frames};
use crate::speaker_voice_mapper::SpeakerVoiceMapper;
use crate::speaker_identifier::{SpeakerIdentifier, SpeakerIdentifierMode, VadBasedSpeakerIdentifier, EmbeddingBasedSpeakerIdentifier};
use crate::cache_manager::CacheManager;
use crate::config_manager::ConfigManager;
use crate::emotion_adapter::{EmotionAdapter, EmotionRequest, EmotionResponse};
use crate::error::{EngineError, EngineResult};
use crate::event_bus::{EventBus, CoreEvent, EventTopic};
use crate::nmt_incremental::{NmtIncremental, MarianNmtOnnx, M2M100NmtOnnx, TranslationRequest, TranslationResponse};
use crate::nmt_client::{LocalM2m100HttpClient, NmtClientAdapter};
use crate::persona_adapter::{PersonaAdapter, PersonaContext};
use crate::telemetry::{TelemetryDatum, TelemetrySink};
use crate::tts_streaming::{TtsStreaming, TtsRequest, TtsStreamChunk, VitsTtsEngine, PiperHttpTts, PiperHttpConfig, YourTtsHttp, YourTtsHttpConfig};
use crate::types::{PartialTranscript, StableTranscript};
use crate::vad::VoiceActivityDetector;
use crate::health_check::HealthChecker;
use crate::post_processing::TextPostProcessor;
use crate::performance_logger::{PerformanceLog, PerformanceLogger};
use crate::text_segmentation::TextSegmenter;
use crate::translation_quality::TranslationQualityChecker;
use crate::tts_audio_enhancement::{AudioEnhancer, AudioEnhancementConfig};
use serde_json::json;
use std::path::Path;
use std::time::Instant;
use uuid::Uuid;
use futures::future::join_all;


pub struct CoreEngine {
    event_bus: Arc<dyn EventBus>,
    vad: Arc<dyn VoiceActivityDetector>,
    asr: Arc<dyn AsrStreaming>,
    nmt: Arc<dyn NmtIncremental>,
    emotion: Arc<dyn EmotionAdapter>,
    persona: Arc<dyn PersonaAdapter>,
    tts: Arc<dyn TtsStreaming>,
    fallback_tts: Option<Arc<dyn TtsStreaming>>,  // å›é€€ TTS æœåŠ¡ï¼ˆå½“ä¸» TTS ä¸æ”¯æŒè¯¥è¯­è¨€æ—¶ä½¿ç”¨ï¼‰
    config: Arc<dyn ConfigManager>,
    cache: Arc<dyn CacheManager>,
    telemetry: Arc<dyn TelemetrySink>,
    // ä¼˜åŒ–æ¨¡å—
    post_processor: Option<Arc<TextPostProcessor>>,
    perf_logger: Option<Arc<PerformanceLogger>>,
    text_segmenter: Option<Arc<TextSegmenter>>,
    audio_enhancer: Option<Arc<AudioEnhancer>>,
    quality_checker: Option<Arc<TranslationQualityChecker>>,
    // æœåŠ¡ URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
    nmt_service_url: Option<String>,
    tts_service_url: Option<String>,
    // TTS å¢é‡æ’­æ”¾é…ç½®
    tts_incremental_enabled: bool,
    tts_buffer_sentences: usize,
    // è¿ç»­è¾“å…¥è¾“å‡ºæ”¯æŒ
    audio_buffer: Option<Arc<AudioBufferManager>>,
    continuous_mode: bool,
    // TTS å¤šè¯´è¯è€…éŸ³è‰²åŒºåˆ†
    speaker_voice_mapper: Option<Arc<SpeakerVoiceMapper>>,
    // è¯´è¯è€…è¯†åˆ«
    speaker_identifier: Option<Arc<dyn SpeakerIdentifier>>,
}

impl Clone for CoreEngine {
    fn clone(&self) -> Self {
        Self {
            event_bus: Arc::clone(&self.event_bus),
            vad: Arc::clone(&self.vad),
            asr: Arc::clone(&self.asr),
            nmt: Arc::clone(&self.nmt),
            emotion: Arc::clone(&self.emotion),
            persona: Arc::clone(&self.persona),
            tts: Arc::clone(&self.tts),
            fallback_tts: self.fallback_tts.as_ref().map(Arc::clone),
            config: Arc::clone(&self.config),
            cache: Arc::clone(&self.cache),
            telemetry: Arc::clone(&self.telemetry),
            post_processor: self.post_processor.as_ref().map(Arc::clone),
            perf_logger: self.perf_logger.as_ref().map(Arc::clone),
            text_segmenter: self.text_segmenter.as_ref().map(Arc::clone),
            audio_enhancer: self.audio_enhancer.as_ref().map(Arc::clone),
            quality_checker: self.quality_checker.as_ref().map(Arc::clone),
            nmt_service_url: self.nmt_service_url.clone(),
            tts_service_url: self.tts_service_url.clone(),
            tts_incremental_enabled: self.tts_incremental_enabled,
            tts_buffer_sentences: self.tts_buffer_sentences,
            audio_buffer: self.audio_buffer.as_ref().map(Arc::clone),
            continuous_mode: self.continuous_mode,
            speaker_voice_mapper: self.speaker_voice_mapper.as_ref().map(Arc::clone),
            speaker_identifier: self.speaker_identifier.as_ref().map(Arc::clone),
        }
    }
}

pub struct CoreEngineBuilder {
    event_bus: Option<Arc<dyn EventBus>>,
    vad: Option<Arc<dyn VoiceActivityDetector>>,
    asr: Option<Arc<dyn AsrStreaming>>,
    nmt: Option<Arc<dyn NmtIncremental>>,
    emotion: Option<Arc<dyn EmotionAdapter>>,
    persona: Option<Arc<dyn PersonaAdapter>>,
    tts: Option<Arc<dyn TtsStreaming>>,
    fallback_tts: Option<Arc<dyn TtsStreaming>>,  // å›é€€ TTS æœåŠ¡
    config: Option<Arc<dyn ConfigManager>>,
    cache: Option<Arc<dyn CacheManager>>,
    telemetry: Option<Arc<dyn TelemetrySink>>,
    // ä¼˜åŒ–æ¨¡å—
    post_processor: Option<Arc<TextPostProcessor>>,
    perf_logger: Option<Arc<PerformanceLogger>>,
    text_segmenter: Option<Arc<TextSegmenter>>,
    audio_enhancer: Option<Arc<AudioEnhancer>>,
    quality_checker: Option<Arc<TranslationQualityChecker>>,
    // æœåŠ¡ URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
    nmt_service_url: Option<String>,
    tts_service_url: Option<String>,
    // TTS å¢é‡æ’­æ”¾é…ç½®
    tts_incremental_enabled: bool,
    tts_buffer_sentences: usize,
    // è¿ç»­è¾“å…¥è¾“å‡ºæ”¯æŒ
    audio_buffer: Option<Arc<AudioBufferManager>>,
    continuous_mode: bool,
    // TTS å¤šè¯´è¯è€…éŸ³è‰²åŒºåˆ†
    speaker_voice_mapper: Option<Arc<SpeakerVoiceMapper>>,
    // è¯´è¯è€…è¯†åˆ«
    speaker_identifier: Option<Arc<dyn SpeakerIdentifier>>,
}

impl CoreEngineBuilder {
    pub fn new() -> Self {
        Self {
            event_bus: None,
            vad: None,
            asr: None,
            nmt: None,
            emotion: None,
            persona: None,
            tts: None,
            fallback_tts: None,
            config: None,
            cache: None,
            telemetry: None,
            post_processor: None,
            perf_logger: None,
            text_segmenter: None,
            audio_enhancer: None,
            quality_checker: None,
            nmt_service_url: None,
            tts_service_url: None,
            tts_incremental_enabled: false,
            tts_buffer_sentences: 0,
            audio_buffer: None,
            continuous_mode: false,
            speaker_voice_mapper: None,
            speaker_identifier: None,
        }
    }

    pub fn event_bus(mut self, event_bus: Arc<dyn EventBus>) -> Self {
        self.event_bus = Some(event_bus);
        self
    }

    pub fn vad(mut self, vad: Arc<dyn VoiceActivityDetector>) -> Self {
        self.vad = Some(vad);
        self
    }

    pub fn asr(mut self, asr: Arc<dyn AsrStreaming>) -> Self {
        self.asr = Some(asr);
        self
    }

    pub fn nmt(mut self, nmt: Arc<dyn NmtIncremental>) -> Self {
        self.nmt = Some(nmt);
        self
    }

    pub fn emotion(mut self, emotion: Arc<dyn EmotionAdapter>) -> Self {
        self.emotion = Some(emotion);
        self
    }

    pub fn persona(mut self, persona: Arc<dyn PersonaAdapter>) -> Self {
        self.persona = Some(persona);
        self
    }

    pub fn tts(mut self, tts: Arc<dyn TtsStreaming>) -> Self {
        self.tts = Some(tts);
        self
    }

    pub fn config(mut self, config: Arc<dyn ConfigManager>) -> Self {
        self.config = Some(config);
        self
    }

    pub fn cache(mut self, cache: Arc<dyn CacheManager>) -> Self {
        self.cache = Some(cache);
        self
    }

    pub fn telemetry(mut self, telemetry: Arc<dyn TelemetrySink>) -> Self {
        self.telemetry = Some(telemetry);
        self
    }

    /// ä½¿ç”¨é»˜è®¤çš„ Marian NMT ONNX æ¨¡å‹åˆå§‹åŒ– NMT æ¨¡å—
    /// 
    /// æ¨¡å‹è·¯å¾„ï¼š`core/engine/models/nmt/marian-en-zh/`
    pub fn nmt_with_default_marian_onnx(mut self) -> EngineResult<Self> {
        // 1. æ‰¾åˆ° core/engine ç›®å½•
        let crate_root = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        
        // 2. çº¦å®šçš„ Marian æ¨¡å‹ç›®å½•è·¯å¾„
        let model_dir = crate_root.join("models/nmt/marian-en-zh");

        // 3. æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if !model_dir.exists() {
            return Err(EngineError::new(format!(
                "Marian NMT model directory not found at: {}. Please ensure the model is exported.",
                model_dir.display()
            )));
        }

        // 4. åŠ è½½çœŸå®çš„ ONNX å®ç°
        let nmt_impl = MarianNmtOnnx::new_from_dir(&model_dir)
            .map_err(|e| EngineError::new(format!("Failed to load MarianNmtOnnx: {}", e)))?;

        // 5. å­˜å…¥ builder çš„ nmt å­—æ®µ
        self.nmt = Some(Arc::new(nmt_impl));

        Ok(self)
    }

    /// ä½¿ç”¨é»˜è®¤çš„ M2M100 NMT ONNX æ¨¡å‹åˆå§‹åŒ– NMT æ¨¡å—
    /// 
    /// æ¨¡å‹è·¯å¾„ï¼š`core/engine/models/nmt/m2m100-en-zh/`
    /// 
    /// æ³¨æ„ï¼šM2M100 æ˜¯æ–°çš„ NMT æ¨¡å‹ï¼Œæ¨èä½¿ç”¨æ­¤æ–¹æ³•æ›¿ä»£ Marian
    /// 
    /// @deprecated æ¨èä½¿ç”¨ `nmt_with_m2m100_http_client()` æ›¿ä»£ï¼Œä»¥è·å¾—æ›´å¥½çš„ç¿»è¯‘è´¨é‡å’Œç¨³å®šæ€§
    pub fn nmt_with_default_m2m100_onnx(mut self) -> EngineResult<Self> {
        // 1. æ‰¾åˆ° core/engine ç›®å½•
        let crate_root = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        
        // 2. çº¦å®šçš„ M2M100 æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ en-zhï¼‰
        let model_dir = crate_root.join("models/nmt/m2m100-en-zh");

        // 3. æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if !model_dir.exists() {
            return Err(EngineError::new(format!(
                "M2M100 NMT model directory not found at: {}. Please ensure the model is exported.",
                model_dir.display()
            )));
        }

        // 4. åŠ è½½çœŸå®çš„ ONNX å®ç°
        let nmt_impl = M2M100NmtOnnx::new_from_dir(&model_dir)
            .map_err(|e| EngineError::new(format!("Failed to load M2M100NmtOnnx: {}", e)))?;

        // 5. å­˜å…¥ builder çš„ nmt å­—æ®µ
        self.nmt = Some(Arc::new(nmt_impl));

        Ok(self)
    }

    /// ä½¿ç”¨ M2M100 HTTP å®¢æˆ·ç«¯åˆå§‹åŒ– NMT æ¨¡å—ï¼ˆæ¨èï¼‰
    /// 
    /// æ­¤æ–¹æ³•è¿æ¥åˆ°æœ¬åœ°è¿è¡Œçš„ Python M2M100 æœåŠ¡ï¼Œæä¾›æ›´ç¨³å®šå’Œé«˜è´¨é‡çš„ç¿»è¯‘ã€‚
    /// 
    /// # Arguments
    /// * `service_url` - Python æœåŠ¡çš„ URLï¼Œé»˜è®¤ä¸º "http://127.0.0.1:5008"
    /// 
    /// # å‰ç½®æ¡ä»¶
    /// éœ€è¦å…ˆå¯åŠ¨ Python M2M100 æœåŠ¡ï¼š
    /// ```bash
    /// cd services/nmt_m2m100
    /// uvicorn nmt_service:app --host 127.0.0.1 --port 5008
    /// ```
    /// 
    /// # ä¼˜åŠ¿
    /// - ç¿»è¯‘è´¨é‡æ›´ç¨³å®šï¼ˆä½¿ç”¨ HuggingFace Transformersï¼‰
    /// - ä»£ç æ›´ç®€æ´ï¼ˆæ— éœ€ç®¡ç†å¤æ‚çš„ KV Cacheï¼‰
    /// - æ˜“äºç»´æŠ¤å’Œè°ƒè¯•
    pub fn nmt_with_m2m100_http_client(mut self, service_url: Option<&str>) -> EngineResult<Self> {
        let url = service_url.unwrap_or("http://127.0.0.1:5008");
        
        // ä¿å­˜æœåŠ¡ URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
        self.nmt_service_url = Some(url.to_string());
        
        // åˆ›å»º HTTP å®¢æˆ·ç«¯
        let client = Arc::new(LocalM2m100HttpClient::new(url));
        
        // åˆ›å»ºé€‚é…å™¨ï¼ˆå®ç° NmtIncremental traitï¼‰
        let nmt_impl = NmtClientAdapter::new(client);
        
        // å­˜å…¥ builder çš„ nmt å­—æ®µ
        self.nmt = Some(Arc::new(nmt_impl));
        
        Ok(self)
    }

    /// ä½¿ç”¨æŒ‡å®šè¯­è¨€å¯¹çš„ M2M100 NMT ONNX æ¨¡å‹åˆå§‹åŒ– NMT æ¨¡å—
    /// 
    /// # Arguments
    /// * `direction` - ç¿»è¯‘æ–¹å‘ï¼Œå¦‚ "en-zh" æˆ– "zh-en"
    /// 
    /// æ¨¡å‹è·¯å¾„ï¼š
    /// - `core/engine/models/nmt/m2m100-en-zh/` (en-zh)
    /// - `core/engine/models/nmt/m2m100-zh-en/` (zh-en)
    pub fn nmt_with_m2m100_onnx(mut self, direction: &str) -> EngineResult<Self> {
        // 1. æ‰¾åˆ° core/engine ç›®å½•
        let crate_root = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        
        // 2. æ ¹æ®æ–¹å‘ç¡®å®šæ¨¡å‹ç›®å½•
        let model_dir = if direction == "en-zh" || direction == "en_zh" {
            crate_root.join("models/nmt/m2m100-en-zh")
        } else if direction == "zh-en" || direction == "zh_en" {
            crate_root.join("models/nmt/m2m100-zh-en")
        } else {
            return Err(EngineError::new(format!(
                "Invalid translation direction: {}. Supported: en-zh, zh-en",
                direction
            )));
        };

        // 3. æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if !model_dir.exists() {
            return Err(EngineError::new(format!(
                "M2M100 NMT model directory not found at: {}. Please ensure the model is exported.",
                model_dir.display()
            )));
        }

        // 4. åŠ è½½çœŸå®çš„ ONNX å®ç°
        let nmt_impl = M2M100NmtOnnx::new_from_dir(&model_dir)
            .map_err(|e| EngineError::new(format!("Failed to load M2M100NmtOnnx: {}", e)))?;

        // 5. å­˜å…¥ builder çš„ nmt å­—æ®µ
        self.nmt = Some(Arc::new(nmt_impl));

        Ok(self)
    }

    /// ä½¿ç”¨é»˜è®¤çš„ Marian NMT Stubï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
    /// 
    /// @deprecated è¯·ä½¿ç”¨ `nmt_with_default_marian_onnx()` ä»£æ›¿
    #[deprecated(note = "Use nmt_with_default_marian_onnx() instead")]
    pub fn nmt_with_default_marian_stub(self) -> EngineResult<Self> {
        // ä¸ºäº†å‘åå…¼å®¹ï¼Œè°ƒç”¨æ–°çš„æ–¹æ³•
        self.nmt_with_default_marian_onnx()
    }

    /// ä½¿ç”¨é»˜è®¤çš„ Whisper ASR æ¨¡å‹åˆå§‹åŒ– ASR æ¨¡å—
    /// 
    /// æ¨¡å‹è·¯å¾„ï¼š`core/engine/models/asr/whisper-base/`
    /// 
    /// # Returns
    /// è¿”å› `EngineResult<Self>`ï¼Œå¦‚æœæ¨¡å‹ç›®å½•ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥åˆ™è¿”å›é”™è¯¯
    pub fn asr_with_default_whisper(mut self) -> EngineResult<Self> {
        // 1. æ‰¾åˆ° core/engine ç›®å½•
        let crate_root = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        
        // 2. çº¦å®šçš„ Whisper æ¨¡å‹ç›®å½•è·¯å¾„
        let model_dir = crate_root.join("models/asr/whisper-base");

        // 3. æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if !model_dir.exists() {
            return Err(EngineError::new(format!(
                "Whisper ASR model directory not found at: {}. Please ensure the model is downloaded.",
                model_dir.display()
            )));
        }

        // 4. åŠ è½½ Whisper ASR å®ç°
        let asr_impl = WhisperAsrStreaming::new_from_dir(&model_dir)
            .map_err(|e| EngineError::new(format!("Failed to load WhisperAsrStreaming: {}", e)))?;

        // 5. å­˜å…¥ builder çš„ asr å­—æ®µ
        self.asr = Some(Arc::new(asr_impl));

        Ok(self)
    }

    /// ä½¿ç”¨é»˜è®¤çš„ VITS TTS æ¨¡å‹åˆå§‹åŒ– TTS æ¨¡å—ï¼ˆæ”¯æŒå¤šè¯­è¨€ï¼‰
    /// 
    /// æ¨¡å‹è·¯å¾„ï¼š`core/engine/models/tts/`
    /// - è‹±æ–‡æ¨¡å‹ï¼š`mms-tts-eng/`ï¼ˆå¿…éœ€ï¼‰
    /// - ä¸­æ–‡æ¨¡å‹ï¼š`mms-tts-zh-Hans/`ï¼ˆå¯é€‰ï¼‰
    /// 
    /// # Returns
    /// è¿”å› `EngineResult<Self>`ï¼Œå¦‚æœè‹±æ–‡æ¨¡å‹ç›®å½•ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥åˆ™è¿”å›é”™è¯¯
    pub fn tts_with_default_vits(mut self) -> EngineResult<Self> {
        // 1. æ‰¾åˆ° core/engine ç›®å½•
        let crate_root = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        
        // 2. çº¦å®šçš„ VITS TTS æ¨¡å‹æ ¹ç›®å½•è·¯å¾„
        let models_root = crate_root.join("models/tts");

        // 3. æ£€æŸ¥æ¨¡å‹æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨
        if !models_root.exists() {
            return Err(EngineError::new(format!(
                "VITS TTS models root directory not found at: {}. Please ensure the models are downloaded.",
                models_root.display()
            )));
        }

        // 4. åŠ è½½ VITS TTS å®ç°ï¼ˆæ”¯æŒå¤šè¯­è¨€ï¼‰
        let tts_impl = VitsTtsEngine::new_from_models_root(&models_root)
            .map_err(|e| EngineError::new(format!("Failed to load VitsTtsEngine: {}", e)))?;

        // 5. å­˜å…¥ builder çš„ tts å­—æ®µ
        self.tts = Some(Arc::new(tts_impl));

        Ok(self)
    }

    /// ä½¿ç”¨é»˜è®¤çš„ Piper HTTP TTS æœåŠ¡åˆå§‹åŒ– TTS æ¨¡å—
    /// 
    /// é…ç½®ï¼š
    /// - ç«¯ç‚¹ï¼šhttp://127.0.0.1:5005/tts
    /// - é»˜è®¤è¯­éŸ³ï¼šzh_CN-huayan-medium
    /// - è¶…æ—¶ï¼š8000ms
    /// 
    /// æ³¨æ„ï¼šæ­¤æ–¹æ³•éœ€è¦ WSL2 ä¸­è¿è¡Œ Piper HTTP æœåŠ¡
    pub fn tts_with_default_piper_http(mut self) -> EngineResult<Self> {
        let config = PiperHttpConfig::default();
        
        // ä¿å­˜æœåŠ¡ URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
        // ä» endpoint ä¸­æå–åŸºç¡€ URLï¼ˆå»æ‰ /tts åç¼€ï¼‰
        let base_url = config.endpoint
            .strip_suffix("/tts")
            .unwrap_or(&config.endpoint)
            .to_string();
        self.tts_service_url = Some(base_url);
        
        let tts_impl = PiperHttpTts::new(config)
            .map_err(|e| EngineError::new(format!("Failed to create PiperHttpTts: {}", e)))?;
        
        self.tts = Some(Arc::new(tts_impl));
        Ok(self)
    }

    /// ä½¿ç”¨è‡ªå®šä¹‰é…ç½®çš„ Piper HTTP TTS æœåŠ¡åˆå§‹åŒ– TTS æ¨¡å—
    /// 
    /// # Arguments
    /// * `config` - Piper HTTP é…ç½®
    pub fn tts_with_piper_http(mut self, config: PiperHttpConfig) -> EngineResult<Self> {
        // ä¿å­˜æœåŠ¡ URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
        let base_url = config.endpoint
            .strip_suffix("/tts")
            .unwrap_or(&config.endpoint)
            .to_string();
        self.tts_service_url = Some(base_url);
        
        let tts_impl = PiperHttpTts::new(config)
            .map_err(|e| EngineError::new(format!("Failed to create PiperHttpTts: {}", e)))?;
        
        self.tts = Some(Arc::new(tts_impl));
        Ok(self)
    }

    /// ä½¿ç”¨ YourTTS HTTP æœåŠ¡åˆå§‹åŒ– TTS æ¨¡å—ï¼ˆæ”¯æŒé›¶æ ·æœ¬éŸ³è‰²å…‹éš†ï¼‰
    /// 
    /// # Arguments
    /// * `config` - YourTTS HTTP é…ç½®
    pub fn tts_with_yourtts_http(mut self, config: YourTtsHttpConfig) -> EngineResult<Self> {
        // ä¿å­˜æœåŠ¡ URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
        self.tts_service_url = Some(config.endpoint.clone());
        
        let tts_impl = YourTtsHttp::new(config)
            .map_err(|e| EngineError::new(format!("Failed to create YourTtsHttp: {}", e)))?;
        
        self.tts = Some(Arc::new(tts_impl));
        Ok(self)
    }
    
    /// è®¾ç½®å›é€€ TTS æœåŠ¡ï¼ˆå½“ä¸» TTS ä¸æ”¯æŒæŸäº›è¯­è¨€æ—¶ä½¿ç”¨ï¼‰
    /// 
    /// # Arguments
    /// * `fallback_tts` - å›é€€ TTS æœåŠ¡å®ä¾‹
    pub fn with_fallback_tts(mut self, fallback_tts: Arc<dyn TtsStreaming>) -> Self {
        self.fallback_tts = Some(fallback_tts);
        self
    }
    
    /// å¯ç”¨æ–‡æœ¬åå¤„ç†
    /// 
    /// # Arguments
    /// * `terms_file` - æœ¯è¯­è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    /// * `enabled` - æ˜¯å¦å¯ç”¨åå¤„ç†
    pub fn with_post_processing(mut self, terms_file: Option<&Path>, enabled: bool) -> Self {
        let processor = TextPostProcessor::new(terms_file, enabled);
        self.post_processor = Some(Arc::new(processor));
        self
    }
    
    /// å¯ç”¨æ€§èƒ½æ—¥å¿—
    /// 
    /// # Arguments
    /// * `enabled` - æ˜¯å¦å¯ç”¨æ€§èƒ½æ—¥å¿—
    /// * `log_suspect` - æ˜¯å¦è®°å½•å¯ç–‘ç¿»è¯‘
    pub fn with_performance_logging(mut self, enabled: bool, log_suspect: bool) -> Self {
        let logger = PerformanceLogger::new(enabled, log_suspect);
        self.perf_logger = Some(Arc::new(logger));
        self
    }
    
    /// å¯ç”¨ TTS å¢é‡æ’­æ”¾
    /// 
    /// # Arguments
    /// * `enabled` - æ˜¯å¦å¯ç”¨å¢é‡æ’­æ”¾
    /// * `buffer_sentences` - ç¼“å†²çš„çŸ­å¥æ•°é‡ï¼ˆ0 = ç«‹å³æ’­æ”¾ï¼Œ> 0 = ç¼“å†²æ¨¡å¼ï¼‰
    /// * `max_sentence_length` - æœ€å¤§å¥å­é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
    pub fn with_tts_incremental_playback(
        mut self,
        enabled: bool,
        buffer_sentences: usize,
        max_sentence_length: usize,
    ) -> Self {
        if enabled {
            // ä½¿ç”¨æ”¯æŒé€—å·åˆ†å‰²çš„åˆ†æ®µå™¨ï¼ˆç”¨äºåœ¨é€—å·å¤„æ·»åŠ åœé¡¿ï¼‰
            let segmenter = TextSegmenter::new_with_comma_splitting(max_sentence_length);
            self.text_segmenter = Some(Arc::new(segmenter));
            self.tts_incremental_enabled = true;
            self.tts_buffer_sentences = buffer_sentences;
        }
        self
    }
    
    /// å¯ç”¨ TTS éŸ³é¢‘å¢å¼ºï¼ˆfade in/outã€åœé¡¿ï¼‰
    /// 
    /// # Arguments
    /// * `config` - éŸ³é¢‘å¢å¼ºé…ç½®
    pub fn with_audio_enhancement(mut self, config: AudioEnhancementConfig) -> Self {
        let enhancer = AudioEnhancer::new(config);
        self.audio_enhancer = Some(Arc::new(enhancer));
        self
    }
    
    /// å¯ç”¨ç¿»è¯‘è´¨é‡æ£€æŸ¥
    /// 
    /// # Arguments
    /// * `enabled` - æ˜¯å¦å¯ç”¨è´¨é‡æ£€æŸ¥
    pub fn with_translation_quality_check(mut self, enabled: bool) -> Self {
        if enabled {
            let checker = TranslationQualityChecker::new(true);
            self.quality_checker = Some(Arc::new(checker));
        }
        self
    }
    
    /// å¯ç”¨è¿ç»­è¾“å…¥è¾“å‡ºæ¨¡å¼
    /// 
    /// åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œç³»ç»Ÿä¼šï¼š
    /// - ä½¿ç”¨éŸ³é¢‘ç¼“å†²ç®¡ç†å™¨ç´¯ç§¯éŸ³é¢‘å¸§
    /// - å½“ VAD æ£€æµ‹åˆ°è¾¹ç•Œæ—¶ï¼Œå¼‚æ­¥å¤„ç†å½“å‰ç‰‡æ®µ
    /// - åœ¨å¤„ç†å½“å‰ç‰‡æ®µçš„åŒæ—¶ï¼Œç»§ç»­æ¥æ”¶æ–°çš„éŸ³é¢‘è¾“å…¥
    /// 
    /// # Arguments
    /// * `enabled` - æ˜¯å¦å¯ç”¨è¿ç»­æ¨¡å¼
    /// * `max_buffer_duration_ms` - æœ€å¤§ç¼“å†²æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œé˜²æ­¢ç¼“å†²åŒºæº¢å‡º
    /// * `min_segment_duration_ms` - æœ€å°ç‰‡æ®µæ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œé˜²æ­¢è¿‡çŸ­ç‰‡æ®µ
    pub fn with_continuous_mode(
        mut self,
        enabled: bool,
        max_buffer_duration_ms: u64,
        min_segment_duration_ms: u64,
    ) -> Self {
        if enabled {
            let buffer = AudioBufferManager::with_config(
                max_buffer_duration_ms,
                min_segment_duration_ms,
            );
            self.audio_buffer = Some(Arc::new(buffer));
            self.continuous_mode = true;
        }
        self
    }
    
    /// å¯ç”¨ TTS å¤šè¯´è¯è€…éŸ³è‰²åŒºåˆ†
    /// 
    /// åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œç³»ç»Ÿä¼šä¸ºæ¯ä¸ªè¯´è¯è€…åˆ†é…ä¸åŒçš„ TTS éŸ³è‰²ï¼ˆvoiceï¼‰
    /// å®ç°ç¬¬äºŒé˜¶æ®µç›®æ ‡ï¼šTTS å¤šè¯´è¯è€…éŸ³è‰²åŒºåˆ†
    /// 
    /// # Arguments
    /// * `available_voices` - å¯ç”¨çš„ voice åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š["zh_CN-huayan-medium", "zh_CN-xiaoyan-medium"]ï¼‰
    pub fn with_speaker_voice_mapping(
        mut self,
        available_voices: Vec<String>,
    ) -> Self {
        if !available_voices.is_empty() {
            let mapper = SpeakerVoiceMapper::new(available_voices);
            self.speaker_voice_mapper = Some(Arc::new(mapper));
        }
        self
    }
    
    /// å¯ç”¨è¯´è¯è€…è¯†åˆ«
    /// 
    /// æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    /// - VadBased: åŸºäº VAD è¾¹ç•Œçš„ç®€å•æ¨¡å¼ï¼ˆå…è´¹ç”¨æˆ·ï¼‰
    /// - EmbeddingBased: åŸºäº Speaker Embedding çš„å‡†ç¡®æ¨¡å¼ï¼ˆä»˜è´¹ç”¨æˆ·ï¼‰
    /// 
    /// # Arguments
    /// * `mode` - è¯´è¯è€…è¯†åˆ«æ¨¡å¼
    pub fn with_speaker_identification(
        mut self,
        mode: SpeakerIdentifierMode,
    ) -> EngineResult<Self> {
        let identifier: Arc<dyn SpeakerIdentifier> = match mode {
            SpeakerIdentifierMode::VadBased { min_switch_interval_ms, max_same_speaker_interval_ms } => {
                Arc::new(VadBasedSpeakerIdentifier::new(
                    min_switch_interval_ms,
                    max_same_speaker_interval_ms,
                ))
            }
            SpeakerIdentifierMode::EmbeddingBased { service_url, similarity_threshold } => {
                Arc::new(EmbeddingBasedSpeakerIdentifier::new(
                    service_url,
                    similarity_threshold,
                )?)
            }
        };
        
        self.speaker_identifier = Some(identifier);
        Ok(self)
    }

    pub fn build(self) -> EngineResult<CoreEngine> {
        Ok(CoreEngine {
            event_bus: self.event_bus.ok_or_else(|| EngineError::new("event_bus is missing"))?,
            vad: self.vad.ok_or_else(|| EngineError::new("vad is missing"))?,
            asr: self.asr.ok_or_else(|| EngineError::new("asr is missing"))?,
            nmt: self.nmt.ok_or_else(|| EngineError::new("nmt is missing"))?,
            emotion: self.emotion.ok_or_else(|| EngineError::new("emotion is missing"))?,
            persona: self.persona.ok_or_else(|| EngineError::new("persona is missing"))?,
            tts: self.tts.ok_or_else(|| EngineError::new("tts is missing"))?,
            fallback_tts: self.fallback_tts,
            config: self.config.ok_or_else(|| EngineError::new("config is missing"))?,
            cache: self.cache.ok_or_else(|| EngineError::new("cache is missing"))?,
            telemetry: self.telemetry.ok_or_else(|| EngineError::new("telemetry is missing"))?,
            post_processor: self.post_processor,
            perf_logger: self.perf_logger,
            text_segmenter: self.text_segmenter,
            audio_enhancer: self.audio_enhancer,
            quality_checker: self.quality_checker,
            nmt_service_url: self.nmt_service_url,
            tts_service_url: self.tts_service_url,
            tts_incremental_enabled: self.tts_incremental_enabled,
            tts_buffer_sentences: self.tts_buffer_sentences,
            audio_buffer: self.audio_buffer,
            continuous_mode: self.continuous_mode,
            speaker_voice_mapper: self.speaker_voice_mapper,
            speaker_identifier: self.speaker_identifier,
        })
    }
}

impl CoreEngine {
    pub async fn boot(&self) -> EngineResult<()> {
        self.event_bus.start().await?;
        let config = self.config.load().await?;
        self.cache.warm_up().await?;
        self.asr.initialize().await?;
        self.nmt.initialize().await?;
        
        // å¥åº·æ£€æŸ¥ï¼šæ£€æŸ¥ NMT å’Œ TTS æœåŠ¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼Œç­‰å¾…æœåŠ¡å°±ç»ªï¼‰
        if let (Some(nmt_url), Some(tts_url)) = (&self.nmt_service_url, &self.tts_service_url) {
            let checker = HealthChecker::new();
            
            // ç­‰å¾…æœåŠ¡å°±ç»ªï¼Œæœ€å¤šé‡è¯• 15 æ¬¡ï¼Œæ¯æ¬¡é—´éš” 1 ç§’ï¼ˆæ€»å…±æœ€å¤š 15 ç§’ï¼‰
            const MAX_RETRIES: u32 = 15;
            const RETRY_DELAY_MS: u64 = 1000;
            
            let mut nmt_healthy = false;
            let mut tts_healthy = false;
            let mut final_attempt = 0;
            
            eprintln!("[INFO] Waiting for NMT and TTS services to be ready...");
            
            for attempt in 1..=MAX_RETRIES {
                final_attempt = attempt;
                let (nmt_health, tts_health) = checker.check_all_services(nmt_url, tts_url).await;
                
                nmt_healthy = nmt_health.is_healthy;
                tts_healthy = tts_health.is_healthy;
                
                if nmt_healthy && tts_healthy {
                    // æ‰€æœ‰æœåŠ¡éƒ½å¥åº·ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                    break;
                }
                
                if attempt < MAX_RETRIES {
                    // ç­‰å¾…åé‡è¯•ï¼ˆä¸æ‰“å°ä¸­é—´ç»“æœï¼Œé¿å…æ—¥å¿—æ··ä¹±ï¼‰
                    tokio::time::sleep(tokio::time::Duration::from_millis(RETRY_DELAY_MS)).await;
                }
            }
            
            // æŠ¥å‘Šæœ€ç»ˆçŠ¶æ€
            if nmt_healthy {
                eprintln!("[INFO] NMT service health check passed: {} (attempt {}/{})", nmt_url, final_attempt, MAX_RETRIES);
            } else {
                eprintln!("[WARN] NMT service is not healthy after {} attempts: {} - Please ensure the service is running", final_attempt, nmt_url);
                // ä¸é˜»æ­¢å¯åŠ¨ï¼Œä½†è®°å½•è­¦å‘Š
            }
            
            if tts_healthy {
                eprintln!("[INFO] TTS service health check passed: {} (attempt {}/{})", tts_url, final_attempt, MAX_RETRIES);
            } else {
                eprintln!("[WARN] TTS service is not healthy after {} attempts: {} - Please ensure the service is running", final_attempt, tts_url);
                // ä¸é˜»æ­¢å¯åŠ¨ï¼Œä½†è®°å½•è­¦å‘Š
            }
        }
        
        self.telemetry
            .record(TelemetryDatum {
                name: "core_engine.boot".to_string(),
                value: 1.0,
                unit: "count".to_string(),
            })
            .await?;
        self.telemetry
            .record(TelemetryDatum {
                name: format!("core_engine.mode.{}", config.mode),
                value: 1.0,
                unit: "count".to_string(),
            })
            .await?;
        Ok(())
    }

    pub async fn shutdown(&self) -> EngineResult<()> {
        self.asr.finalize().await?;
        self.nmt.finalize().await?;
        self.tts.close().await?;
        self.cache.purge().await?;
        self.event_bus.stop().await?;
        self.telemetry
            .record(TelemetryDatum {
                name: "core_engine.shutdown".to_string(),
                value: 1.0,
                unit: "count".to_string(),
            })
            .await?;
        Ok(())
    }

    /// å¤„ç†éŸ³é¢‘å¸§ï¼ˆå®Œæ•´ä¸šåŠ¡æµç¨‹ï¼šVAD â†’ ASR â†’ NMT â†’ äº‹ä»¶å‘å¸ƒï¼‰
    /// 
    /// æµç¨‹ï¼š
    /// 1. é€šè¿‡ VAD æ£€æµ‹è¯­éŸ³æ´»åŠ¨
    /// 2. å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³ï¼Œç´¯ç§¯åˆ° ASR ç¼“å†²åŒº
    /// 3. å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³è¾¹ç•Œï¼ˆis_boundaryï¼‰ï¼Œè§¦å‘ ASR æ¨ç†
    /// 4. å¦‚æœ ASR è¿”å›æœ€ç»ˆç»“æœï¼Œè‡ªåŠ¨è§¦å‘ NMT ç¿»è¯‘
    /// 5. å‘å¸ƒäº‹ä»¶åˆ° EventBusï¼ˆASR éƒ¨åˆ†ç»“æœã€ASR æœ€ç»ˆç»“æœã€ç¿»è¯‘ç»“æœï¼‰
    /// 
    /// # Arguments
    /// * `frame` - éŸ³é¢‘å¸§
    /// * `language_hint` - è¯­è¨€æç¤ºï¼ˆå¯é€‰ï¼‰
    /// 
    /// # Returns
    /// è¿”å›å¤„ç†ç»“æœï¼ˆåŒ…å« ASR å’Œ NMT ç»“æœï¼‰
    pub async fn process_audio_frame(
        &self,
        frame: crate::types::AudioFrame,
        language_hint: Option<String>,
    ) -> EngineResult<Option<ProcessResult>> {
        // å¦‚æœå¯ç”¨äº†è¿ç»­æ¨¡å¼ï¼Œä½¿ç”¨è¿ç»­å¤„ç†é€»è¾‘
        if self.continuous_mode {
            return self.process_audio_frame_continuous(frame, language_hint).await;
        }
        
        // åŸæœ‰çš„å¤„ç†é€»è¾‘ï¼ˆéè¿ç»­æ¨¡å¼ï¼‰
        // æ€§èƒ½æ—¥å¿—ï¼šè®°å½•æ€»è€—æ—¶
        let total_start = Instant::now();
        let request_id = Uuid::new_v4().to_string();
        
        // 1. é€šè¿‡ VAD æ£€æµ‹è¯­éŸ³æ´»åŠ¨
        let vad_result = self.vad.detect(frame).await?;

        // 2. ç´¯ç§¯éŸ³é¢‘å¸§åˆ° ASR ç¼“å†²åŒº
        // å°è¯•å°† ASR è½¬æ¢ä¸º WhisperAsrStreaming
        let asr_ptr = Arc::as_ptr(&self.asr);
        let whisper_asr_ptr = asr_ptr as *const WhisperAsrStreaming;
        
        unsafe {
            let whisper_asr_ref = whisper_asr_ptr.as_ref();
            if let Some(whisper_asr) = whisper_asr_ref {
                // 2.1. å¦‚æœæä¾›äº†è¯­è¨€æç¤ºï¼Œè®¾ç½® ASR è¯­è¨€
                // åªåœ¨ç¬¬ä¸€æ¬¡æˆ–è¯­è¨€æ”¹å˜æ—¶è®¾ç½®è¯­è¨€ï¼Œé¿å…æ¯å¸§éƒ½è®¾ç½®ï¼ˆå‡å°‘æ—¥å¿—ï¼‰
                static LAST_LANGUAGE: std::sync::Mutex<Option<String>> = std::sync::Mutex::new(None);
                if let Some(ref lang_hint) = language_hint {
                    // å°†è¯­è¨€ä»£ç æ ‡å‡†åŒ–ï¼ˆä¾‹å¦‚ "zh-CN" -> "zh"ï¼‰
                    let normalized_lang = if lang_hint.starts_with("zh") {
                        Some("zh".to_string())
                    } else if lang_hint.starts_with("en") {
                        Some("en".to_string())
                    } else {
                        Some(lang_hint.clone())
                    };
                    
                    // æ£€æŸ¥è¯­è¨€æ˜¯å¦æ”¹å˜
                    let mut last_lang = LAST_LANGUAGE.lock().unwrap();
                    let should_set = last_lang.as_ref() != normalized_lang.as_ref();
                    if should_set {
                        if let Err(e) = whisper_asr.set_language(normalized_lang.clone()) {
                        eprintln!("[ASR] Warning: Failed to set language: {}", e);
                        } else {
                            *last_lang = normalized_lang;
                        }
                    }
                }
                
                // ç´¯ç§¯å¸§
                whisper_asr.accumulate_frame(vad_result.frame.clone())?;
                
                // 3. å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³è¾¹ç•Œï¼Œè§¦å‘ ASR æ¨ç†ï¼ˆè¿”å›æœ€ç»ˆç»“æœï¼‰
                if vad_result.is_boundary {
                    // 3.1. è¯†åˆ«è¯´è¯è€…ï¼ˆå¦‚æœå¯ç”¨äº†è¯´è¯è€…è¯†åˆ«ï¼‰
                    // åœ¨éè¿ç»­æ¨¡å¼ä¸‹ï¼Œä» ASR ç¼“å†²åŒºè·å–ç´¯ç§¯çš„éŸ³é¢‘ç‰‡æ®µ
                    let (speaker_result, speaker_embedding_ms) = if let Some(ref identifier) = self.speaker_identifier {
                        let speaker_start = Instant::now();
                        eprintln!("[SPEAKER] ===== Speaker Identification Started =====");
                        eprintln!("[SPEAKER] Boundary detected at timestamp: {}ms (confidence: {:.3})", 
                                 vad_result.frame.timestamp_ms, vad_result.confidence);
                        
                        // ä» ASR ç¼“å†²åŒºè·å–ç´¯ç§¯çš„éŸ³é¢‘å¸§ï¼ˆç”¨äºè¯´è¯è€…è¯†åˆ«ï¼‰
                        // è¿‡æ»¤æ‰é™éŸ³å¸§ï¼Œåªä½¿ç”¨åŒ…å«è¯­éŸ³çš„å¸§
                        let all_frames = whisper_asr.get_accumulated_frames()
                            .unwrap_or_else(|e| {
                                eprintln!("[SPEAKER] âš  Warning: Failed to get accumulated frames: {}, using current frame only", e);
                                vec![vad_result.frame.clone()]
                            });
                        
                        // å°è¯•ä» VAD è·å–ä¸Šä¸€ä¸ªè¯­éŸ³å¸§çš„æ—¶é—´æˆ³ï¼Œç”¨äºè¿‡æ»¤é™éŸ³å¸§
                        let last_speech_ts = {
                            let vad_ptr = Arc::as_ptr(&self.vad);
                            let silero_vad_ptr = vad_ptr as *const crate::vad::SileroVad;
                            unsafe {
                                if let Some(silero_vad) = silero_vad_ptr.as_ref() {
                                    silero_vad.get_last_speech_timestamp()
                                } else {
                                    None
                                }
                            }
                        };
                        
                        // è¿‡æ»¤éŸ³é¢‘å¸§ï¼šåªä¿ç•™åŒ…å«è¯­éŸ³çš„å¸§ï¼ˆåœ¨æœ€åä¸€ä¸ªè¯­éŸ³å¸§ä¹‹å‰çš„å¸§ï¼‰
                        // å¦‚æœæ— æ³•ç¡®å®šï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰å¸§ï¼ˆé™¤äº†æ˜æ˜¾çš„é™éŸ³å¸§ï¼‰
                        let audio_frames: Vec<_> = if let Some(speech_ts) = last_speech_ts {
                            // åªä½¿ç”¨æœ€åä¸€ä¸ªè¯­éŸ³å¸§ä¹‹å‰çš„å¸§ï¼ˆæ’é™¤é™éŸ³å¸§ï¼‰
                            all_frames.iter()
                                .filter(|f| f.timestamp_ms <= speech_ts)
                                .cloned()
                                .collect()
                        } else {
                            // å¦‚æœæ— æ³•ç¡®å®šï¼Œä½¿ç”¨æ‰€æœ‰å¸§ï¼ˆä½†æ’é™¤å½“å‰è¾¹ç•Œå¸§ï¼Œå› ä¸ºå®ƒå¯èƒ½æ˜¯é™éŸ³ï¼‰
                            // ä¿ç•™é™¤äº†æœ€åä¸€ä¸ªè¾¹ç•Œå¸§ä¹‹å¤–çš„æ‰€æœ‰å¸§
                            if all_frames.len() > 1 {
                                all_frames[..all_frames.len() - 1].to_vec()
                            } else {
                                all_frames.clone()
                            }
                        };
                        
                        // è®¡ç®—è¿‡æ»¤åçš„éŸ³é¢‘æ—¶é•¿
                        let filtered_duration_ms = if !audio_frames.is_empty() {
                            let total_samples: usize = audio_frames.iter().map(|f| f.data.len()).sum();
                            let sample_rate = audio_frames[0].sample_rate;
                            (total_samples as f32 / sample_rate as f32 * 1000.0) as u64
                        } else {
                            0
                        };
                        
                        // å¦‚æœè¿‡æ»¤åçš„éŸ³é¢‘å¤ªçŸ­ï¼ˆ< 1000msï¼‰ï¼Œä½¿ç”¨æ‰€æœ‰å¸§ï¼ˆåŒ…æ‹¬é™éŸ³å¸§ï¼‰
                        // è¿™æ ·å¯ä»¥ç¡®ä¿è¯´è¯è€…è¯†åˆ«èƒ½è·å–åˆ°è¶³å¤Ÿé•¿çš„éŸ³é¢‘
                        let final_audio_frames = if filtered_duration_ms < 1000 && !all_frames.is_empty() {
                            eprintln!("[SPEAKER] âš  Filtered audio too short ({}ms < 1000ms), using all frames (including silence) to ensure sufficient length", filtered_duration_ms);
                            // ä½¿ç”¨æ‰€æœ‰å¸§ï¼Œä½†æ’é™¤æœ€åä¸€ä¸ªè¾¹ç•Œå¸§ï¼ˆå› ä¸ºå®ƒå¯èƒ½æ˜¯çº¯é™éŸ³ï¼‰
                            if all_frames.len() > 1 {
                                all_frames[..all_frames.len() - 1].to_vec()
                            } else {
                                all_frames.clone()
                            }
                        } else if !audio_frames.is_empty() {
                            audio_frames
                        } else {
                            eprintln!("[SPEAKER] âš  Warning: No speech frames found after filtering, using all frames as fallback");
                            all_frames.clone()
                        };
                        
                        // è®¡ç®—è¾“å…¥éŸ³é¢‘çš„æ€»æ—¶é•¿
                        if !final_audio_frames.is_empty() {
                            let total_samples: usize = final_audio_frames.iter().map(|f| f.data.len()).sum();
                            let sample_rate = final_audio_frames[0].sample_rate;
                            let total_duration_ms = (total_samples as f32 / sample_rate as f32 * 1000.0) as u64;
                            let total_duration_sec = total_samples as f32 / sample_rate as f32;
                            eprintln!("[SPEAKER] Input audio: {} frames (filtered from {} total), {} samples, {:.2}s ({:.0}ms) at {}Hz", 
                                     final_audio_frames.len(), all_frames.len(), total_samples, total_duration_sec, total_duration_ms, sample_rate);
                            
                            // å¦‚æœè¿‡æ»¤åçš„éŸ³é¢‘ä»ç„¶å¤ªçŸ­ï¼Œè®°å½•è­¦å‘Š
                            if total_duration_ms < 1000 {
                                eprintln!("[SPEAKER] âš  Warning: Filtered audio is still too short ({}ms < 1000ms required for speaker embedding). History buffer may need more time to accumulate.", total_duration_ms);
                            }
                        } else {
                            eprintln!("[SPEAKER] âš  Warning: No frames available for speaker identification");
                        }
                        
                        let audio_frames = final_audio_frames;
                        
                        let result = identifier.identify_speaker(&audio_frames, vad_result.frame.timestamp_ms).await;
                        let speaker_ms = speaker_start.elapsed().as_millis() as u64;
                        
                        match result {
                            Ok(speaker_result) => {
                                eprintln!("[SPEAKER] âœ… Identified speaker: {} (is_new: {}, confidence: {:.2})", 
                                    speaker_result.speaker_id, speaker_result.is_new_speaker, speaker_result.confidence);
                                eprintln!("[SPEAKER] Voice embedding: {} (dim: {})", 
                                    if speaker_result.voice_embedding.is_some() { "Yes" } else { "No" },
                                    speaker_result.voice_embedding.as_ref().map(|v| v.len()).unwrap_or(0));
                                eprintln!("[SPEAKER] Reference audio: {} (samples: {})", 
                                    if speaker_result.reference_audio.is_some() { "Yes" } else { "No" },
                                    speaker_result.reference_audio.as_ref().map(|a| a.len()).unwrap_or(0));
                                
                                // è®¾ç½®å½“å‰è¯´è¯è€…IDï¼ˆç”¨äºVADè‡ªé€‚åº”è°ƒæ•´ï¼‰
                                self.set_vad_current_speaker(Some(&speaker_result.speaker_id));
                                eprintln!("[SPEAKER] â±ï¸  Speaker identification completed in {}ms", speaker_ms);
                                eprintln!("[SPEAKER] ==============================================");
                                (Some(speaker_result), Some(speaker_ms))
                            }
                            Err(e) => {
                                eprintln!("[SPEAKER] âŒ Identification failed after {}ms: {}", speaker_ms, e);
                                eprintln!("[SPEAKER] ==============================================");
                                (None, Some(speaker_ms))
                            }
                        }
                    } else {
                        (None, None)
                    };
                    
                    let speaker_id = speaker_result.as_ref().map(|r| r.speaker_id.clone());
                    let _voice_embedding = speaker_result.as_ref().and_then(|r| r.voice_embedding.clone());
                    let reference_audio = speaker_result.as_ref().and_then(|r| r.reference_audio.clone());
                    
                    // ç«‹å³å¼€å§‹ ASR è½¬å½•ï¼ˆä½¿ç”¨å·²ç´¯ç§¯çš„éŸ³é¢‘ï¼‰
                    // æ³¨æ„ï¼šåœ¨æ£€æµ‹åˆ°è¾¹ç•Œåç«‹å³å¼€å§‹å¤„ç†ï¼Œä¸ç­‰å¾…åç»­éŸ³é¢‘è¾“å…¥
                    // è¿™æ ·å¯ä»¥å®ç°æµå¼å¤„ç†ï¼šç”¨æˆ·è¯´å®Œè¯åç«‹å³å¼€å§‹ç¿»è¯‘ï¼Œæ— éœ€ç­‰å¾…å®Œæ•´éŸ³é¢‘
                    // å¯¹äºæ‰‹æœºç«¯ AECï¼ˆå£°å­¦å›å“æ¶ˆé™¤ï¼‰åœºæ™¯ï¼Œè¿™å¯ä»¥æ˜¾è‘—å‡å°‘å»¶è¿Ÿ
                    let asr_start = Instant::now();
                    eprintln!("[ASR] ğŸš€ Starting transcription immediately after boundary detection...");
                    let asr_result = whisper_asr.infer_on_boundary().await?;
                    let asr_ms = asr_start.elapsed().as_millis() as u64;
                    eprintln!("[ASR] âœ… Transcription completed in {}ms", asr_ms);
                    
                    // æ‰“å° ASR ç»“æœ
                    if let Some(ref partial) = asr_result.partial {
                        eprintln!("[ASR] ğŸ“ Partial transcript: \"{}\" (confidence: {:.2})", partial.text, partial.confidence);
                    }
                    if let Some(ref final_transcript) = asr_result.final_transcript {
                        eprintln!("[ASR] âœ… Final transcript: \"{}\" (language: {}, speaker_id: {:?})", 
                                 final_transcript.text, final_transcript.language, final_transcript.speaker_id);
                    }
                    
                    // 3.5. è¿‡æ»¤æ— æ„ä¹‰çš„ ASR ç»“æœï¼ˆåœ¨è¿›å…¥ç¿»è¯‘/TTS ä¹‹å‰ï¼‰
                    if let Some(ref final_transcript) = asr_result.final_transcript {
                        if Self::is_meaningless_transcript(&final_transcript.text) {
                            eprintln!("[ASR] â›” Filtered meaningless transcript: \"{}\" (skipping translation/TTS)", final_transcript.text);
                            // ç›´æ¥è¿”å›ï¼Œä¸è¿›å…¥åç»­å¤„ç†
                            return Ok(Some(ProcessResult {
                                asr: asr_result,
                                emotion: None,
                                translation: None,
                                tts: None,
                            }));
                        }
                    }
                    
                    // 4. å‘å¸ƒ ASR æœ€ç»ˆç»“æœäº‹ä»¶ï¼ˆåŒ…å« speaker_idï¼‰
                    // 4.1. æ›´æ–°è¯­é€Ÿï¼ˆå¦‚æœå¯ç”¨äº†è‡ªé€‚åº”VADï¼‰
                    if let Some(ref final_transcript) = asr_result.final_transcript {
                        if let Some(ref sid) = speaker_id {
                            // è®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆä»ASRç¼“å†²åŒºè·å–ï¼‰
                            let audio_frames = whisper_asr.get_accumulated_frames()
                                .unwrap_or_else(|_| vec![]);
                            if !audio_frames.is_empty() {
                                let total_samples: usize = audio_frames.iter().map(|f| f.data.len()).sum();
                                let sample_rate = audio_frames[0].sample_rate;
                                let audio_duration_ms = (total_samples as f32 / sample_rate as f32 * 1000.0) as u64;
                                
                                // æ›´æ–°VADä¸­çš„è¯´è¯è€…è¯­é€Ÿ
                                self.update_vad_speaker_speech_rate(sid, &final_transcript.text, audio_duration_ms);
                            }
                        }
                        
                        // æ›´æ–° final_transcript çš„ speaker_id
                        let mut transcript_with_speaker = final_transcript.clone();
                        transcript_with_speaker.speaker_id = speaker_id.clone();
                        self.publish_asr_final_event(&transcript_with_speaker, vad_result.frame.timestamp_ms).await?;
                    }
                    
                    // 5. å¦‚æœ ASR è¿”å›æœ€ç»ˆç»“æœï¼Œè¿›è¡Œ Emotion åˆ†æã€Persona ä¸ªæ€§åŒ–ï¼Œç„¶åè§¦å‘ NMT ç¿»è¯‘
                    let (emotion_result, translation_result, tts_result, _nmt_ms, _tts_ms) = if let Some(ref final_transcript) = asr_result.final_transcript {
                        // 5.1. Emotion æƒ…æ„Ÿåˆ†æ
                        let emotion_result = self.analyze_emotion(final_transcript, vad_result.frame.timestamp_ms).await.ok();
                        
                        // 5.2. åº”ç”¨ Persona ä¸ªæ€§åŒ–
                        let personalized_transcript = self.personalize_transcript(final_transcript).await?;
                        
                        // 5.3. ç«‹å³å¼€å§‹ç¿»è¯‘ï¼ˆæµå¼å¤„ç†ï¼šASR å®Œæˆåç«‹å³ç¿»è¯‘ï¼Œä¸ç­‰å¾…ï¼‰
                        // è¿™æ ·å¯ä»¥å®ç°ï¼šç”¨æˆ·è¯´å®Œè¯ â†’ ç«‹å³å¼€å§‹ç¿»è¯‘ â†’ ç«‹å³å¼€å§‹ TTS â†’ ç«‹å³å¬åˆ°ç¬¬ä¸€æ®µè¯
                        // å¯¹äºæ‰‹æœºç«¯ AEC åœºæ™¯ï¼Œè¿™å¯ä»¥æ˜¾è‘—å‡å°‘ç«¯åˆ°ç«¯å»¶è¿Ÿ
                        let mut personalized_with_speaker = personalized_transcript;
                        personalized_with_speaker.speaker_id = speaker_id.clone();
                        // è®¡ç®—åŸå§‹éŸ³é¢‘æ—¶é•¿ï¼ˆç”¨äºåç»­è®¡ç®—æ¯ä¸ª segment çš„è¯­é€Ÿï¼‰
                        let source_audio_duration_ms = if let Some(ref _final_transcript) = asr_result.final_transcript {
                            let audio_frames = whisper_asr.get_accumulated_frames()
                                .unwrap_or_else(|_| vec![]);
                            if !audio_frames.is_empty() {
                                let total_samples: usize = audio_frames.iter().map(|f| f.data.len()).sum();
                                let sample_rate = audio_frames[0].sample_rate;
                                Some((total_samples as f32 / sample_rate as f32 * 1000.0) as u64)
                            } else {
                                None
                            }
                        } else {
                            None
                        };
                        
                        let nmt_start = Instant::now();
                        eprintln!("[NMT] ğŸš€ Starting translation immediately after ASR...");
                        let mut translation_result = self.translate_and_publish(&personalized_with_speaker, vad_result.frame.timestamp_ms).await.ok();
                        
                        // å°†åŸå§‹éŸ³é¢‘ä¿¡æ¯æ·»åŠ åˆ°ç¿»è¯‘ç»“æœä¸­ï¼ˆç”¨äºè®¡ç®—æ¯ä¸ª segment çš„è¯­é€Ÿï¼‰
                        if let Some(ref mut translation) = translation_result {
                            if let Some(ref final_transcript) = asr_result.final_transcript {
                                translation.source_audio_duration_ms = source_audio_duration_ms;
                                translation.source_text = Some(final_transcript.text.clone());
                            }
                        }
                        
                        let nmt_ms = nmt_start.elapsed().as_millis() as u64;
                        eprintln!("[NMT] âœ… Translation completed in {}ms", nmt_ms);
                        
                        // 5.4. ç«‹å³å¼€å§‹ TTS åˆæˆï¼ˆæµå¼å¤„ç†ï¼šç¿»è¯‘å®Œæˆåç«‹å³åˆæˆï¼‰
                        // è¿™æ ·ç”¨æˆ·å¯ä»¥åœ¨è¯´å®Œè¯åç«‹å³å¬åˆ°ç¬¬ä¸€æ®µç¿»è¯‘ç»“æœ
                        let (tts_result, tts_ms, yourtts_ms) = if let Some(ref translation) = translation_result {
                            let tts_start = Instant::now();
                            eprintln!("[TTS] ğŸš€ Starting synthesis immediately after translation...");
                            match self.synthesize_and_publish(translation, vad_result.frame.timestamp_ms, reference_audio.clone()).await {
                                Ok((result, yt_ms)) => {
                                    let tts_ms = tts_start.elapsed().as_millis() as u64;
                                    eprintln!("[TTS] Synthesis completed in {}ms (audio size: {} bytes)", tts_ms, result.audio.len());
                                    (Some(result), tts_ms, yt_ms)
                                }
                                Err(e) => {
                                    let tts_ms = tts_start.elapsed().as_millis() as u64;
                                    eprintln!("[TTS] Synthesis failed in {}ms: {}", tts_ms, e);
                                    (None, tts_ms, None)
                                }
                            }
                        } else {
                            eprintln!("[TTS] Skipped (no translation result)");
                            (None, 0, None)
                        };
                        
                        // æ€§èƒ½æ—¥å¿—è®°å½•
                        let total_ms = total_start.elapsed().as_millis() as u64;
                        eprintln!("[PERF] ===== Pipeline timing summary =====");
                        if let Some(se_ms) = speaker_embedding_ms {
                            eprintln!("[PERF] Speaker Embedding: {}ms", se_ms);
                        }
                        eprintln!("[PERF] ASR:                {}ms", asr_ms);
                        eprintln!("[PERF] NMT:                {}ms", nmt_ms);
                        eprintln!("[PERF] TTS:                {}ms", tts_ms);
                        eprintln!("[PERF] Total:              {}ms", total_ms);
                        eprintln!("[PERF] Note: Adaptive VAD overhead < 0.2ms (not shown separately)");
                        if let Some(yt_ms) = yourtts_ms {
                            eprintln!("[PERF] YourTTS:            {}ms", yt_ms);
                        }
                        eprintln!("[PERF] Total:              {}ms", total_ms);
                        eprintln!("[PERF] =====================================");
                        
                        if let Some(ref logger) = self.perf_logger {
                            let config = self.config.current().await.ok();
                            let src_lang = final_transcript.language.clone();
                            let tgt_lang = config.as_ref().map(|c| c.target_language.clone()).unwrap_or_else(|| "zh".to_string());
                            
                            let mut perf_log = PerformanceLog::new(
                                request_id.clone(),
                                src_lang,
                                tgt_lang,
                                asr_ms,
                                nmt_ms,
                                tts_ms,
                                total_ms,
                                translation_result.is_some(),
                            );
                            
                            if let Some(ref translation) = translation_result {
                                perf_log.check_suspect_translation(&final_transcript.text, &translation.translated_text);
                            }
                            
                            logger.log(&perf_log);
                        }
                        
                        (emotion_result, translation_result, tts_result, nmt_ms, tts_ms)
                    } else {
                        (None, None, None, 0, 0)
                    };
                    
                    return Ok(Some(ProcessResult {
                        asr: asr_result,
                        emotion: emotion_result,
                        translation: translation_result,
                        tts: tts_result,
                    }));
                } else {
                    // æœªæ£€æµ‹åˆ°è¾¹ç•Œï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºéƒ¨åˆ†ç»“æœï¼ˆå¦‚æœå¯ç”¨æµå¼æ¨ç†ï¼‰
                    if whisper_asr.is_streaming_enabled() {
                        if let Some(partial) = whisper_asr.infer_partial(vad_result.frame.timestamp_ms).await? {
                            // å‘å¸ƒ ASR éƒ¨åˆ†ç»“æœäº‹ä»¶
                            self.publish_asr_partial_event(&partial, vad_result.frame.timestamp_ms).await?;
                            
                            return Ok(Some(ProcessResult {
                                asr: AsrResult {
                                    partial: Some(partial),
                                    final_transcript: None,
                                },
                                emotion: None,
                                translation: None,
                                tts: None,
                            }));
                        }
                    }
                    // ä¸éœ€è¦è¾“å‡ºéƒ¨åˆ†ç»“æœï¼Œè¿”å› None
                    return Ok(None);
                }
            } else {
                // å¦‚æœä¸æ˜¯ WhisperAsrStreamingï¼Œä½¿ç”¨åŸæ¥çš„ infer æ–¹æ³•
                // åœ¨ç§»åŠ¨ frame ä¹‹å‰ä¿å­˜éœ€è¦çš„ä¿¡æ¯
                let frame_timestamp = vad_result.frame.timestamp_ms;
                let frame_data_len = vad_result.frame.data.len();
                let frame_sample_rate = vad_result.frame.sample_rate;
                let asr_result = self.asr.infer(crate::asr_streaming::AsrRequest {
                    frame: vad_result.frame,
                    language_hint: language_hint.clone(),
                }).await?;
                
                // æ‰“å° ASR ç»“æœ
                if let Some(ref partial) = asr_result.partial {
                    eprintln!("[ASR] ğŸ“ Partial transcript: \"{}\" (confidence: {:.2})", partial.text, partial.confidence);
                }
                if let Some(ref final_transcript) = asr_result.final_transcript {
                    eprintln!("[ASR] âœ… Final transcript: \"{}\" (language: {}, speaker_id: {:?})", 
                             final_transcript.text, final_transcript.language, final_transcript.speaker_id);
                }
                
                // è¿‡æ»¤æ— æ„ä¹‰çš„ ASR ç»“æœï¼ˆåœ¨è¿›å…¥ç¿»è¯‘/TTS ä¹‹å‰ï¼‰
                if let Some(ref final_transcript) = asr_result.final_transcript {
                    if Self::is_meaningless_transcript(&final_transcript.text) {
                        eprintln!("[ASR] â›” Filtered meaningless transcript: \"{}\" (skipping translation/TTS)", final_transcript.text);
                        // ç›´æ¥è¿”å›ï¼Œä¸è¿›å…¥åç»­å¤„ç†
                        return Ok(Some(ProcessResult {
                            asr: asr_result,
                            emotion: None,
                            translation: None,
                            tts: None,
                        }));
                    }
                }
                
                // å¦‚æœæ£€æµ‹åˆ°è¾¹ç•Œä¸”æœ‰æœ€ç»ˆç»“æœï¼Œè¿›è¡Œ Emotion åˆ†æã€Persona ä¸ªæ€§åŒ–ï¼Œç„¶åè§¦å‘ç¿»è¯‘
                if vad_result.is_boundary {
                    if let Some(ref final_transcript) = asr_result.final_transcript {
                        self.publish_asr_final_event(final_transcript, frame_timestamp).await?;
                        
                        // Emotion æƒ…æ„Ÿåˆ†æ
                        let emotion_result = self.analyze_emotion(final_transcript, frame_timestamp).await.ok();
                        
                        // åº”ç”¨ Persona ä¸ªæ€§åŒ–
                        let personalized_transcript = self.personalize_transcript(final_transcript).await?;
                        
                        // è®¡ç®—åŸå§‹éŸ³é¢‘æ—¶é•¿ï¼ˆç”¨äºåç»­è®¡ç®—æ¯ä¸ª segment çš„è¯­é€Ÿï¼‰
                        let source_audio_duration_ms = {
                            Some((frame_data_len as f32 / frame_sample_rate as f32 * 1000.0) as u64)
                        };
                        
                        // ä½¿ç”¨ä¸ªæ€§åŒ–åçš„ transcript è¿›è¡Œç¿»è¯‘
                        let mut translation_result = self.translate_and_publish(&personalized_transcript, frame_timestamp).await.ok();
                        
                        // å°†åŸå§‹éŸ³é¢‘ä¿¡æ¯æ·»åŠ åˆ°ç¿»è¯‘ç»“æœä¸­ï¼ˆç”¨äºè®¡ç®—æ¯ä¸ª segment çš„è¯­é€Ÿï¼‰
                        if let Some(ref mut translation) = translation_result {
                            if let Some(ref final_transcript) = asr_result.final_transcript {
                                translation.source_audio_duration_ms = source_audio_duration_ms;
                                translation.source_text = Some(final_transcript.text.clone());
                            }
                        }
                        
                        // å¦‚æœç¿»è¯‘æˆåŠŸï¼Œè¿›è¡Œ TTS åˆæˆ
                        let tts_result = if let Some(ref translation) = translation_result {
                            self.synthesize_and_publish(translation, frame_timestamp, None).await.ok().map(|(chunk, _)| chunk)
                        } else {
                            None
                        };
                        
                        return Ok(Some(ProcessResult {
                            asr: asr_result,
                            emotion: emotion_result,
                            translation: translation_result,
                            tts: tts_result,
                        }));
                    }
                }
                
                // å¦‚æœæœ‰éƒ¨åˆ†ç»“æœï¼Œå‘å¸ƒäº‹ä»¶
                if let Some(ref partial) = asr_result.partial {
                    self.publish_asr_partial_event(partial, frame_timestamp).await?;
                }
                
                return Ok(Some(ProcessResult {
                    asr: asr_result,
                    emotion: None,
                    translation: None,
                    tts: None,
                }));
            }
        }
    }

    /// å¤„ç†éŸ³é¢‘å¸§ï¼ˆè¿ç»­è¾“å…¥è¾“å‡ºæ¨¡å¼ï¼‰
    /// 
    /// åœ¨æ­¤æ¨¡å¼ä¸‹ï¼š
    /// 1. å°†éŸ³é¢‘å¸§æ·»åŠ åˆ°ç¼“å†²åŒº
    /// 2. é€šè¿‡ VAD æ£€æµ‹è¾¹ç•Œ
    /// 3. å¦‚æœæ£€æµ‹åˆ°è¾¹ç•Œï¼Œå¼‚æ­¥å¤„ç†å½“å‰ç‰‡æ®µï¼ˆä¸é˜»å¡éŸ³é¢‘æ¥æ”¶ï¼‰
    /// 4. ç»§ç»­æ¥æ”¶æ–°çš„éŸ³é¢‘è¾“å…¥
    async fn process_audio_frame_continuous(
        &self,
        frame: crate::types::AudioFrame,
        language_hint: Option<String>,
    ) -> EngineResult<Option<ProcessResult>> {
        // è·å–éŸ³é¢‘ç¼“å†²ç®¡ç†å™¨
        let buffer = self.audio_buffer.as_ref()
            .ok_or_else(|| EngineError::new("Audio buffer not initialized in continuous mode"))?;
        
        // ä¿å­˜ frame çš„ timestampï¼ˆåœ¨ç§»åŠ¨ä¹‹å‰ï¼‰
        let current_frame_timestamp = frame.timestamp_ms;
        
        // 1. å°†å¸§æ·»åŠ åˆ°ç¼“å†²åŒº
        match buffer.push_frame(frame.clone()).await {
            Ok(()) => {
                // æ­£å¸¸æ·»åŠ 
            }
            Err(e) => {
                // ç¼“å†²åŒºæº¢å‡ºï¼Œå¼ºåˆ¶æˆªæ–­
                eprintln!("[VAD] Buffer overflow detected, forcing boundary: {}", e);
                // ç»§ç»­æ‰§è¡Œä¸‹é¢çš„è¾¹ç•Œæ£€æµ‹é€»è¾‘
            }
        }
        
        // 2. VAD æ£€æµ‹
        let vad_result = self.vad.detect(frame).await?;
        
        // 3. å¦‚æœæ£€æµ‹åˆ°è¾¹ç•Œï¼Œæäº¤å½“å‰ç¼“å†²åŒº
        if vad_result.is_boundary {
            // æ£€æŸ¥æœ€å°ç‰‡æ®µæ—¶é•¿
            if !buffer.check_min_duration().await {
                // ç‰‡æ®µå¤ªçŸ­ï¼Œç»§ç»­ç´¯ç§¯
                eprintln!("[VAD] Segment too short, continuing accumulation");
                return Ok(None);
            }
            
            // è·å–å½“å‰ç¼“å†²åŒºçš„æ‰€æœ‰å¸§
            let frames = buffer.take_current_buffer().await;
            
            if frames.is_empty() {
                return Ok(None);
            }
            
            // åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªç¼“å†²åŒºï¼ˆç»§ç»­æ¥æ”¶æ–°éŸ³é¢‘ï¼‰
            buffer.swap_buffers().await;
            
            // åˆå¹¶æ‰€æœ‰å¸§ä¸ºå•ä¸ªéŸ³é¢‘æ•°æ®
            let merged_audio = merge_frames(&frames);
            
            // åˆ›å»ºåˆå¹¶åçš„ AudioFrame
            let merged_frame = crate::types::AudioFrame {
                sample_rate: frames[0].sample_rate,
                channels: frames[0].channels,
                data: merged_audio,
                timestamp_ms: frames[0].timestamp_ms,
            };
            
            // è¯†åˆ«è¯´è¯è€…ï¼ˆå¦‚æœå¯ç”¨äº†è¯´è¯è€…è¯†åˆ«ï¼‰
            let boundary_timestamp = frames.last().map(|f| f.timestamp_ms).unwrap_or(current_frame_timestamp);
            let (speaker_result, speaker_embedding_ms) = if let Some(ref identifier) = self.speaker_identifier {
                let speaker_start = Instant::now();
                eprintln!("[SPEAKER] ===== Speaker Identification Started =====");
                eprintln!("[SPEAKER] Boundary timestamp: {}ms", boundary_timestamp);
                
                // è®¡ç®—è¾“å…¥éŸ³é¢‘çš„æ€»æ—¶é•¿
                let total_samples: usize = frames.iter().map(|f| f.data.len()).sum();
                let sample_rate = frames.first().map(|f| f.sample_rate).unwrap_or(16000);
                let total_duration_ms = (total_samples as f32 / sample_rate as f32 * 1000.0) as u64;
                let total_duration_sec = total_samples as f32 / sample_rate as f32;
                eprintln!("[SPEAKER] Input audio: {} frames, {} samples, {:.2}s ({:.0}ms) at {}Hz", 
                         frames.len(), total_samples, total_duration_sec, total_duration_ms, sample_rate);
                
                let result = identifier.identify_speaker(&frames, boundary_timestamp).await;
                let speaker_ms = speaker_start.elapsed().as_millis() as u64;
                
                match result {
                    Ok(speaker_result) => {
                        eprintln!("[SPEAKER] âœ… Identified speaker: {} (is_new: {}, confidence: {:.2})", 
                            speaker_result.speaker_id, speaker_result.is_new_speaker, speaker_result.confidence);
                        eprintln!("[SPEAKER] Voice embedding: {} (dim: {})", 
                            if speaker_result.voice_embedding.is_some() { "Yes" } else { "No" },
                            speaker_result.voice_embedding.as_ref().map(|v| v.len()).unwrap_or(0));
                        eprintln!("[SPEAKER] Reference audio: {} (samples: {})", 
                            if speaker_result.reference_audio.is_some() { "Yes" } else { "No" },
                            speaker_result.reference_audio.as_ref().map(|a| a.len()).unwrap_or(0));
                        eprintln!("[SPEAKER] â±ï¸  Speaker identification completed in {}ms", speaker_ms);
                        eprintln!("[SPEAKER] ==============================================");
                        
                        // è®¾ç½®å½“å‰è¯´è¯è€…IDï¼ˆç”¨äºVADè‡ªé€‚åº”è°ƒæ•´ï¼‰
                        self.set_vad_current_speaker(Some(&speaker_result.speaker_id));
                        
                        (Some(speaker_result), Some(speaker_ms))
                    }
                    Err(e) => {
                        eprintln!("[SPEAKER] âŒ Identification failed after {}ms: {}", speaker_ms, e);
                        eprintln!("[SPEAKER] ==============================================");
                        (None, Some(speaker_ms))
                    }
                }
            } else {
                eprintln!("[SPEAKER] Speaker identification disabled");
                (None, None)
            };
            
            let speaker_id = speaker_result.as_ref().map(|r| r.speaker_id.clone());
            let voice_embedding = speaker_result.as_ref().and_then(|r| r.voice_embedding.clone());
            let reference_audio = speaker_result.as_ref().and_then(|r| r.reference_audio.clone());
            
            // å¼‚æ­¥å¤„ç†å½“å‰ç‰‡æ®µï¼ˆä¸é˜»å¡éŸ³é¢‘æ¥æ”¶ï¼‰
            // å…‹éš†æ‰€æœ‰éœ€è¦çš„ç»„ä»¶ç”¨äºå¼‚æ­¥ä»»åŠ¡
            let engine_clone = self.clone();
            let language_hint_clone = language_hint.clone();
            
            eprintln!("[CONTINUOUS] Processing segment asynchronously (frame count: {}, duration: {}ms, speaker_id: {:?})...", 
                frames.len(), 
                frames.last().map(|f| f.timestamp_ms - frames[0].timestamp_ms).unwrap_or(0),
                speaker_id
            );
            
            tokio::spawn(async move {
                let _result = engine_clone.process_audio_segment(
                    merged_frame,
                    language_hint_clone,
                    speaker_id,
                    voice_embedding,
                    reference_audio,
                    speaker_embedding_ms,
                ).await;
            });
            
            // ç«‹å³è¿”å›ï¼Œç»§ç»­æ¥æ”¶æ–°éŸ³é¢‘
            return Ok(None);
        }
        
        // 4. æœªæ£€æµ‹åˆ°è¾¹ç•Œï¼Œç»§ç»­ç´¯ç§¯
        Ok(None)
    }
    
    /// å¤„ç†éŸ³é¢‘ç‰‡æ®µï¼ˆASR â†’ NMT â†’ TTSï¼‰
    /// 
    /// è¿™æ˜¯ä»è¿ç»­å¤„ç†æ¨¡å¼ä¸­åˆ†ç¦»å‡ºæ¥çš„æ–¹æ³•ï¼Œç”¨äºå¼‚æ­¥å¤„ç†éŸ³é¢‘ç‰‡æ®µ
    async fn process_audio_segment(
        &self,
        frame: crate::types::AudioFrame,
        language_hint: Option<String>,
        speaker_id: Option<String>,
        _voice_embedding: Option<Vec<f32>>,
        reference_audio: Option<Vec<f32>>,
        speaker_embedding_ms: Option<u64>,
    ) -> EngineResult<Option<ProcessResult>> {
        // ä½¿ç”¨åŸæœ‰çš„ process_audio_frame é€»è¾‘ï¼Œä½†éœ€è¦å…ˆé€šè¿‡ VAD æ£€æµ‹
        // ç”±äºæˆ‘ä»¬å·²ç»æœ‰äº†å®Œæ•´çš„éŸ³é¢‘ç‰‡æ®µï¼Œè¿™é‡Œéœ€è¦ç‰¹æ®Šå¤„ç†
        
        // æ€§èƒ½æ—¥å¿—ï¼šè®°å½•æ€»è€—æ—¶
        let total_start = Instant::now();
        let request_id = Uuid::new_v4().to_string();
        
        // å¯¹äºè¿ç»­æ¨¡å¼ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•´ä¸ªç‰‡æ®µä¼ é€’ç»™ ASR
        // è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå°†æ•´ä¸ªç‰‡æ®µä½œä¸ºä¸€ä¸ªè¾¹ç•Œå¸§å¤„ç†
        let asr_ptr = Arc::as_ptr(&self.asr);
        let whisper_asr_ptr = asr_ptr as *const WhisperAsrStreaming;
        
        unsafe {
            let whisper_asr_ref = whisper_asr_ptr.as_ref();
            if let Some(whisper_asr) = whisper_asr_ref {
                // è®¾ç½®è¯­è¨€
                if let Some(ref lang_hint) = language_hint {
                    let normalized_lang = if lang_hint.starts_with("zh") {
                        Some("zh".to_string())
                    } else if lang_hint.starts_with("en") {
                        Some("en".to_string())
                    } else {
                        Some(lang_hint.clone())
                    };
                    if let Err(e) = whisper_asr.set_language(normalized_lang) {
                        eprintln!("[ASR] Warning: Failed to set language: {}", e);
                    }
                }
                
                // å°†æ•´ä¸ªç‰‡æ®µæ·»åŠ åˆ° ASRï¼ˆè¿™é‡Œéœ€è¦å°†ç‰‡æ®µåˆ†å‰²æˆå¤šä¸ªå¸§ï¼‰
                // ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨ infer æ–¹æ³•å¤„ç†æ•´ä¸ªç‰‡æ®µ
                let asr_start = Instant::now();
                eprintln!("[ASR] Starting transcription (continuous mode)...");
                
                // ä½¿ç”¨ infer æ–¹æ³•å¤„ç†æ•´ä¸ªç‰‡æ®µ
                let frame_clone = frame.clone();
                let asr_result = self.asr.infer(crate::asr_streaming::AsrRequest {
                    frame: frame_clone,
                    language_hint: language_hint.clone(),
                }).await?;
                
                let asr_ms = asr_start.elapsed().as_millis() as u64;
                eprintln!("[ASR] Transcription completed in {}ms", asr_ms);
                
                // æ‰“å° ASR ç»“æœ
                if let Some(ref partial) = asr_result.partial {
                    eprintln!("[ASR] ğŸ“ Partial transcript: \"{}\" (confidence: {:.2})", partial.text, partial.confidence);
                }
                if let Some(ref final_transcript) = asr_result.final_transcript {
                    eprintln!("[ASR] âœ… Final transcript: \"{}\" (language: {}, speaker_id: {:?})", 
                             final_transcript.text, final_transcript.language, final_transcript.speaker_id);
                }
                
                // å‘å¸ƒ ASR æœ€ç»ˆç»“æœäº‹ä»¶
                if let Some(mut final_transcript) = asr_result.final_transcript.clone() {
                    // è®¾ç½®è¯´è¯è€… IDï¼ˆå¦‚æœå·²è¯†åˆ«ï¼‰
                    if final_transcript.speaker_id.is_none() {
                        final_transcript.speaker_id = speaker_id.clone();
                    }
                    
                    // æ›´æ–°è¯­é€Ÿï¼ˆå¦‚æœå¯ç”¨äº†è‡ªé€‚åº”VADï¼‰
                    if let Some(ref sid) = speaker_id {
                        // è®¡ç®—éŸ³é¢‘æ—¶é•¿
                        let total_samples = frame.data.len();
                        let sample_rate = frame.sample_rate;
                        let audio_duration_ms = (total_samples as f32 / sample_rate as f32 * 1000.0) as u64;
                        
                        // æ›´æ–°VADä¸­çš„è¯´è¯è€…è¯­é€Ÿ
                        self.update_vad_speaker_speech_rate(sid, &final_transcript.text, audio_duration_ms);
                    }
                    
                    let timestamp = frame.timestamp_ms;
                    self.publish_asr_final_event(&final_transcript, timestamp).await?;
                    
                    // ç»§ç»­å¤„ç†ï¼šEmotion â†’ Persona â†’ NMT â†’ TTS
                    let emotion_result = self.analyze_emotion(&final_transcript, timestamp).await.ok();
                    let personalized_transcript = self.personalize_transcript(&final_transcript).await?;
                    
                    // è®¡ç®—åŸå§‹éŸ³é¢‘æ—¶é•¿ï¼ˆç”¨äºåç»­è®¡ç®—æ¯ä¸ª segment çš„è¯­é€Ÿï¼‰
                    let source_audio_duration_ms = {
                        let total_samples = frame.data.len();
                        let sample_rate = frame.sample_rate;
                        Some((total_samples as f32 / sample_rate as f32 * 1000.0) as u64)
                    };
                    
                    let nmt_start = Instant::now();
                    eprintln!("[NMT] Starting translation (continuous mode, speaker_id: {:?})...", personalized_transcript.speaker_id);
                    let mut translation_result = self.translate_and_publish(&personalized_transcript, timestamp).await.ok();
                    
                    // å°†åŸå§‹éŸ³é¢‘ä¿¡æ¯æ·»åŠ åˆ°ç¿»è¯‘ç»“æœä¸­ï¼ˆç”¨äºè®¡ç®—æ¯ä¸ª segment çš„è¯­é€Ÿï¼‰
                    if let Some(ref mut translation) = translation_result {
                        if let Some(ref final_transcript) = asr_result.final_transcript {
                            translation.source_audio_duration_ms = source_audio_duration_ms;
                            translation.source_text = Some(final_transcript.text.clone());
                        }
                    }
                    
                    let nmt_ms = nmt_start.elapsed().as_millis() as u64;
                    eprintln!("[NMT] Translation completed in {}ms", nmt_ms);
                    
                    let (tts_result, tts_ms, yourtts_ms) = if let Some(ref translation) = translation_result {
                        let tts_start = Instant::now();
                        eprintln!("[TTS] ===== TTS Synthesis Started =====");
                        eprintln!("[TTS] Text: '{}'", translation.translated_text);
                        eprintln!("[TTS] Speaker ID: {:?}", translation.speaker_id);
                        eprintln!("[TTS] Reference audio: {} (samples: {})", 
                            if reference_audio.is_some() { "Yes" } else { "No" },
                            reference_audio.as_ref().map(|a| a.len()).unwrap_or(0));
                        match self.synthesize_and_publish(translation, timestamp, reference_audio.clone()).await {
                            Ok((result, yourtts_time)) => {
                                let tts_ms = tts_start.elapsed().as_millis() as u64;
                                eprintln!("[TTS] âœ… Synthesis completed in {}ms (audio size: {} bytes)", tts_ms, result.audio.len());
                                eprintln!("[TTS] ==========================================");
                                (Some(result), tts_ms, yourtts_time)
                            }
                            Err(e) => {
                                let tts_ms = tts_start.elapsed().as_millis() as u64;
                                eprintln!("[TTS] âŒ Synthesis failed in {}ms: {}", tts_ms, e);
                                eprintln!("[TTS] ==========================================");
                                (None, tts_ms, None)
                            }
                        }
                    } else {
                        eprintln!("[TTS] Skipped (no translation result)");
                        (None, 0, None)
                    };
                    
                    // æ€§èƒ½æ—¥å¿—
                    let total_ms = total_start.elapsed().as_millis() as u64;
                    eprintln!("[PERF] ===== Continuous mode timing summary =====");
                    eprintln!("[PERF] ASR:                {}ms", asr_ms);
                    if let Some(se_ms) = speaker_embedding_ms {
                        eprintln!("[PERF] Speaker Embedding: {}ms", se_ms);
                    }
                    eprintln!("[PERF] NMT:                {}ms", nmt_ms);
                    eprintln!("[PERF] TTS:                {}ms", tts_ms);
                    if let Some(yt_ms) = yourtts_ms {
                        eprintln!("[PERF] YourTTS:            {}ms", yt_ms);
                    }
                    eprintln!("[PERF] Total:              {}ms", total_ms);
                    eprintln!("[PERF] ===========================================");
                    
                    if let Some(ref logger) = self.perf_logger {
                        let config = self.config.current().await.ok();
                        let src_lang = final_transcript.language.clone();
                        let tgt_lang = config.as_ref().map(|c| c.target_language.clone()).unwrap_or_else(|| "zh".to_string());
                        
                        let mut perf_log = PerformanceLog::new(
                            request_id.clone(),
                            src_lang,
                            tgt_lang,
                            asr_ms,
                            nmt_ms,
                            tts_ms,
                            total_ms,
                            translation_result.is_some(),
                        );
                        
                        if let Some(ref translation) = translation_result {
                            perf_log.check_suspect_translation(&final_transcript.text, &translation.translated_text);
                        }
                        
                        logger.log(&perf_log);
                    }
                    
                    // æ›´æ–° ASR ç»“æœä¸­çš„ speaker_id
                    let mut asr_result_with_speaker = asr_result;
                    if let Some(ref mut final_t) = asr_result_with_speaker.final_transcript {
                        final_t.speaker_id = speaker_id.clone();
                    }
                    
                    return Ok(Some(ProcessResult {
                        asr: asr_result_with_speaker,
                        emotion: emotion_result,
                        translation: translation_result,
                        tts: tts_result,
                    }));
                }
            }
        }
        
        Ok(None)
    }

    /// åˆ†ææƒ…æ„Ÿ
    async fn analyze_emotion(
        &self,
        transcript: &StableTranscript,
        timestamp_ms: u64,
    ) -> EngineResult<EmotionResponse> {
        // æ„é€  Emotion è¯·æ±‚ï¼ˆæ ¹æ® Emotion_Adapter_Spec.mdï¼‰
        let request = EmotionRequest {
            text: transcript.text.clone(),
            lang: transcript.language.clone(),
        };
        
        // æ‰§è¡Œæƒ…æ„Ÿåˆ†æ
        let response = self.emotion.analyze(request).await?;
        
        // å‘å¸ƒ Emotion äº‹ä»¶
        self.publish_emotion_event(&response, timestamp_ms).await?;
        
        Ok(response)
    }

    /// åº”ç”¨ Persona ä¸ªæ€§åŒ–
    async fn personalize_transcript(
        &self,
        transcript: &StableTranscript,
    ) -> EngineResult<StableTranscript> {
        // ä»é…ç½®ä¸­è·å– PersonaContextï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨é»˜è®¤å€¼ï¼‰
        // TODO: åç»­å¯ä»¥ä»ç”¨æˆ·é…ç½®æˆ–æ•°æ®åº“è·å–çœŸå®çš„ PersonaContext
        let context = PersonaContext {
            user_id: "default_user".to_string(),
            tone: "formal".to_string(),  // é»˜è®¤ä½¿ç”¨æ­£å¼è¯­è°ƒ
            culture: transcript.language.clone(),
        };
        
        // åº”ç”¨ä¸ªæ€§åŒ–
        self.persona.personalize(transcript.clone(), context).await
    }

    /// ç¿»è¯‘å¹¶å‘å¸ƒäº‹ä»¶
    async fn translate_and_publish(
        &self,
        transcript: &StableTranscript,
        timestamp_ms: u64,
    ) -> EngineResult<TranslationResponse> {
        // 1. è·å–ç›®æ ‡è¯­è¨€ï¼ˆä»é…ç½®ä¸­ï¼Œä½¿ç”¨ .ok() é¿å…é˜»å¡ï¼‰
        // âš ï¸ ä¼˜åŒ–ï¼šå¦‚æœé…ç½®è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼è€Œä¸æ˜¯é˜»å¡æ•´ä¸ªæµç¨‹
        let target_language = self.config.current().await
            .map(|c| c.target_language)
            .unwrap_or_else(|_| {
                eprintln!("[NMT] âš ï¸  Failed to get config, using default target_language: zh");
                "zh".to_string()
            });
        
        // 2. æ„é€ ç¿»è¯‘è¯·æ±‚ï¼ˆä¼ é€’ speaker_idï¼‰
        let translation_request = TranslationRequest {
            transcript: PartialTranscript {
                text: transcript.text.clone(),
                confidence: 1.0,  // æœ€ç»ˆè½¬å½•çš„ç½®ä¿¡åº¦
                is_final: true,
            },
            target_language: target_language.clone(),
            wait_k: None,
            speaker_id: transcript.speaker_id.clone(),  // ä¼ é€’ speaker_id
        };
        
        // 3. æ‰§è¡Œç¿»è¯‘
        let mut translation_response = self.nmt.translate(translation_request).await?;
        
        // 3.1. ç¡®ä¿ speaker_id è¢«ä¼ é€’åˆ° TranslationResponse
        if translation_response.speaker_id.is_none() {
            translation_response.speaker_id = transcript.speaker_id.clone();
        }
        eprintln!("[NMT] Raw translation result: '{}'", translation_response.translated_text);
        
        // 4. åº”ç”¨ç¿»è¯‘è´¨é‡æ£€æŸ¥
        if let Some(ref checker) = self.quality_checker {
            let before_check = translation_response.translated_text.clone();
            let checked_text = checker.check_and_fix(
                &transcript.text,
                &translation_response.translated_text,
                &target_language,
            );
            if before_check != checked_text {
                eprintln!("[NMT] After quality check: '{}' (was: '{}')", checked_text, before_check);
            }
            translation_response.translated_text = checked_text;
        }
        
        // 5. åº”ç”¨æ–‡æœ¬åå¤„ç†
        if let Some(ref processor) = self.post_processor {
            let before_process = translation_response.translated_text.clone();
            let processed_text = processor.process(&translation_response.translated_text, &target_language);
            if before_process != processed_text {
                eprintln!("[NMT] After post-processing: '{}' (was: '{}')", processed_text, before_process);
            }
            translation_response.translated_text = processed_text;
        }
        
        eprintln!("[NMT] Final translation: '{}'", translation_response.translated_text);
        
        // 6. å‘å¸ƒç¿»è¯‘äº‹ä»¶
        self.publish_translation_event(&translation_response, timestamp_ms).await?;
        
        Ok(translation_response)
    }

    /// å‘å¸ƒ ASR éƒ¨åˆ†ç»“æœäº‹ä»¶
    async fn publish_asr_partial_event(
        &self,
        partial: &PartialTranscript,
        timestamp_ms: u64,
    ) -> EngineResult<()> {
        let event = CoreEvent {
            topic: EventTopic("AsrPartial".to_string()),
            payload: json!({
                "text": partial.text,
                "confidence": partial.confidence,
                "is_final": partial.is_final,
            }),
            timestamp_ms,
        };
        self.event_bus.publish(event).await?;
        Ok(())
    }

    /// å‘å¸ƒ ASR æœ€ç»ˆç»“æœäº‹ä»¶
    async fn publish_asr_final_event(
        &self,
        final_transcript: &StableTranscript,
        timestamp_ms: u64,
    ) -> EngineResult<()> {
        let event = CoreEvent {
            topic: EventTopic("AsrFinal".to_string()),
            payload: json!({
                "text": final_transcript.text,
                "speaker_id": final_transcript.speaker_id,
                "language": final_transcript.language,
            }),
            timestamp_ms,
        };
        self.event_bus.publish(event).await?;
        Ok(())
    }

    /// å‘å¸ƒ Emotion äº‹ä»¶
    async fn publish_emotion_event(
        &self,
        emotion: &EmotionResponse,
        timestamp_ms: u64,
    ) -> EngineResult<()> {
        let event = CoreEvent {
            topic: EventTopic("Emotion".to_string()),
            payload: json!({
                "primary": emotion.primary,
                "intensity": emotion.intensity,
                "confidence": emotion.confidence,
            }),
            timestamp_ms,
        };
        self.event_bus.publish(event).await?;
        Ok(())
    }

    /// å‘å¸ƒç¿»è¯‘äº‹ä»¶
    async fn publish_translation_event(
        &self,
        translation: &TranslationResponse,
        timestamp_ms: u64,
    ) -> EngineResult<()> {
        let event = CoreEvent {
            topic: EventTopic("Translation".to_string()),
            payload: json!({
                "translated_text": translation.translated_text,
                "is_stable": translation.is_stable,
            }),
            timestamp_ms,
        };
        self.event_bus.publish(event).await?;
        Ok(())
    }

    /// TTS åˆæˆå¹¶å‘å¸ƒäº‹ä»¶
    /// 
    /// è¿”å› (TtsStreamChunk, YourTTSè€—æ—¶)
    async fn synthesize_and_publish(
        &self,
        translation: &TranslationResponse,
        timestamp_ms: u64,
        reference_audio: Option<Vec<f32>>,
    ) -> EngineResult<(TtsStreamChunk, Option<u64>)> {
        // å¦‚æœå¯ç”¨å¢é‡æ’­æ”¾ï¼Œä½¿ç”¨å¢é‡åˆæˆæ–¹æ³•
        if self.tts_incremental_enabled {
            return self.synthesize_and_publish_incremental(translation, timestamp_ms, reference_audio).await;
        }

        // åŸæœ‰çš„ä¸€æ¬¡æ€§åˆæˆé€»è¾‘
        // 1. è·å–ç›®æ ‡è¯­è¨€ï¼ˆç”¨äº TTS localeï¼‰
        // âš ï¸ ä¼˜åŒ–ï¼šå¦‚æœé…ç½®è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼è€Œä¸æ˜¯é˜»å¡æ•´ä¸ªæµç¨‹
        let target_language = self.config.current().await
            .map(|c| c.target_language)
            .unwrap_or_else(|_| {
                eprintln!("[TTS] âš ï¸  Failed to get config, using default target_language: zh");
                "zh".to_string()
            });
        
        // 2. å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼šå°†å°æ•°è½¬æ¢ä¸ºä¸­æ–‡è¯»æ³•
        let processed_text = if target_language.starts_with("zh") {
            Self::convert_decimals_to_chinese(&translation.translated_text)
        } else {
            translation.translated_text.clone()
        };
        
        // 3. ä½¿ç”¨ä¼ å…¥çš„ reference_audioï¼ˆç”¨äº zero-shot TTSï¼‰
        eprintln!("[TTS] Reference audio: {} (samples: {})", 
            if reference_audio.is_some() { "Yes" } else { "No" },
            reference_audio.as_ref().map(|a| a.len()).unwrap_or(0));
        
        // 4. é€‰æ‹© voiceï¼ˆå¦‚æœå¯ç”¨äº†è¯´è¯è€…éŸ³è‰²æ˜ å°„ï¼‰
        // å¦‚æœæœ‰ reference_audioï¼Œä¼˜å…ˆä½¿ç”¨ zero-shot TTSï¼ˆvoice å¯ä»¥ä¸ºç©ºï¼‰
        // å¦åˆ™ä½¿ç”¨é¢„å®šä¹‰çš„ voice
        let voice = if reference_audio.is_none() {
            if let Some(ref speaker_id) = translation.speaker_id {
                if let Some(ref mapper) = self.speaker_voice_mapper {
                    let assigned_voice = mapper.get_or_assign_voice(speaker_id).await;
                    eprintln!("[TTS] Assigned voice: '{}' for speaker: {}", assigned_voice, speaker_id);
                    assigned_voice
                } else {
                    eprintln!("[TTS] No voice mapper, using empty voice");
                    String::new()
                }
            } else {
                eprintln!("[TTS] No speaker_id, using empty voice");
                String::new()
            }
        } else {
            // ä½¿ç”¨ zero-shot TTSï¼Œvoice å¯ä»¥ä¸ºç©º
            eprintln!("[TTS] Using zero-shot TTS mode (reference audio provided)");
            String::new()
        };
        
        // 5. æ„é€  TTS è¯·æ±‚
        eprintln!("[TTS] Constructing TTS request: text='{}', voice='{}', locale='{}'", 
            if processed_text.len() > 50 { &processed_text[..50] } else { &processed_text },
            voice, target_language);
        
        // 5.1. è·å–è¯´è¯è€…çš„è¯­é€Ÿï¼ˆå¦‚æœå¯ç”¨äº†è‡ªé€‚åº”VADï¼‰
        let speech_rate = if let Some(ref speaker_id) = translation.speaker_id {
            self.get_vad_speaker_speech_rate(speaker_id)
        } else {
            None
        };
        
        if let Some(rate) = speech_rate {
            eprintln!("[TTS] Using speaker speech rate: {:.2} chars/s for speaker: {}", 
                     rate, translation.speaker_id.as_ref().unwrap_or(&"unknown".to_string()));
        }
        
        let tts_request = TtsRequest {
            text: processed_text,
            voice,
            locale: target_language.clone(),
            reference_audio,
            speaker: translation.speaker_id.clone(),
            speech_rate,
        };
        
        // 6. æ‰§è¡Œ TTS åˆæˆ
        let tts_synth_start = Instant::now();
        eprintln!("[TTS] Calling TTS service...");
        eprintln!("[TTS] Request details: reference_audio={}, speaker={:?}, voice='{}'", 
                 if tts_request.reference_audio.is_some() { 
                     format!("Yes ({} samples)", tts_request.reference_audio.as_ref().map(|a| a.len()).unwrap_or(0))
                 } else { 
                     "No".to_string() 
                 },
                 tts_request.speaker,
                 tts_request.voice);
        
        // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ YourTTS
        let is_yourtts = self.tts_service_url.as_ref()
            .map(|url| url.contains("5004") || url.contains("yourtts"))
            .unwrap_or(false);
        if is_yourtts {
            eprintln!("[TTS] âœ… Using YourTTS service (zero-shot TTS with reference audio support)");
        } else {
            eprintln!("[TTS] âš ï¸  Using non-YourTTS service (Piper or other), reference audio will NOT be used!");
        }
        
        let tts_chunk = self.tts.synthesize(tts_request).await?;
        let tts_synth_ms = tts_synth_start.elapsed().as_millis() as u64;
        eprintln!("[TTS] TTS service call completed in {}ms", tts_synth_ms);
        
        // 6. å‘å¸ƒ TTS äº‹ä»¶ï¼ˆåŒ…å« speaker_id ä¿¡æ¯ï¼‰
        self.publish_tts_event(&tts_chunk, timestamp_ms).await?;
        
        // å¦‚æœä½¿ç”¨ YourTTSï¼Œè®°å½• YourTTS çš„è€—æ—¶ï¼ˆä»æ—¥å¿—ä¸­æå–æˆ–ä½¿ç”¨æ€»è€—æ—¶ï¼‰
        // æ³¨æ„ï¼šYourTTS çš„è€—æ—¶å·²ç»åœ¨ yourtts_http.rs ä¸­è®°å½•ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ€»è€—æ—¶ä½œä¸ºè¿‘ä¼¼å€¼
        // å¦‚æœ TTS æœåŠ¡æ˜¯ YourTTSï¼Œåˆ™è¿”å›è€—æ—¶ï¼›å¦åˆ™è¿”å› None
        let yourtts_ms = if self.tts_service_url.as_ref()
            .map(|url| url.contains("5004") || url.contains("yourtts"))
            .unwrap_or(false) {
            Some(tts_synth_ms)
        } else {
            None
        };
        
        Ok((tts_chunk, yourtts_ms))
    }

    /// TTS å¢é‡åˆæˆå¹¶å‘å¸ƒäº‹ä»¶
    /// 
    /// å°†æ–‡æœ¬åˆ†å‰²æˆçŸ­å¥ï¼Œæ¯ä¸ªçŸ­å¥åˆæˆå®Œæˆåç«‹å³å‘å¸ƒï¼ˆæˆ–ç¼“å†²åå‘å¸ƒï¼‰
    /// 
    /// è¿”å› (TtsStreamChunk, YourTTSè€—æ—¶)
    async fn synthesize_and_publish_incremental(
        &self,
        translation: &TranslationResponse,
        timestamp_ms: u64,
        reference_audio: Option<Vec<f32>>,
    ) -> EngineResult<(TtsStreamChunk, Option<u64>)> {
        // 1. è·å–ç›®æ ‡è¯­è¨€ï¼ˆç”¨äº TTS localeï¼‰
        // âš ï¸ ä¼˜åŒ–ï¼šå¦‚æœé…ç½®è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼è€Œä¸æ˜¯é˜»å¡æ•´ä¸ªæµç¨‹
        let target_language = self.config.current().await
            .map(|c| c.target_language)
            .unwrap_or_else(|_| {
                eprintln!("[TTS] âš ï¸  Failed to get config, using default target_language: zh");
                "zh".to_string()
            });
        
        // 2. åˆ†å‰²æ–‡æœ¬ä¸ºçŸ­å¥ï¼ˆä½¿ç”¨å¸¦åœé¡¿ç±»å‹çš„åˆ†æ®µï¼‰
        let segmenter = self.text_segmenter.as_ref()
            .ok_or_else(|| EngineError::new("Text segmenter not initialized".to_string()))?;
        
        eprintln!("[TTS] Input text for segmentation: '{}'", translation.translated_text);
        
        // å°è¯•ä½¿ç”¨å¸¦åœé¡¿ç±»å‹çš„åˆ†æ®µï¼ˆå¦‚æœæ”¯æŒï¼‰
        let segments_with_pause = if segmenter.split_on_comma {
            let segments = segmenter.segment_with_pause_type(&translation.translated_text);
            eprintln!("[TTS] Segmented into {} parts:", segments.len());
            for (i, seg) in segments.iter().enumerate() {
                eprintln!("[TTS]   Segment {}: '{}' (pause_type: {:?})", i + 1, seg.text, seg.pause_type);
            }
            segments
        } else {
            // å‘åå…¼å®¹ï¼šä½¿ç”¨æ—§çš„åˆ†æ®µæ–¹æ³•
            let segments = segmenter.segment(&translation.translated_text);
            eprintln!("[TTS] Segmented into {} parts (legacy method):", segments.len());
            for (i, text) in segments.iter().enumerate() {
                eprintln!("[TTS]   Segment {}: '{}'", i + 1, text);
            }
            segments
                .into_iter()
                .map(|text| {
                    let pause_type = if text.ends_with('.') 
                        || text.ends_with('!') 
                        || text.ends_with('?')
                        || text.ends_with('ã€‚')
                        || text.ends_with('ï¼')
                        || text.ends_with('ï¼Ÿ') {
                        crate::text_segmentation::PauseType::SentenceEnd
                    } else {
                        crate::text_segmentation::PauseType::None
                    };
                    crate::text_segmentation::TextSegment { text, pause_type }
                })
                .collect()
        };
        
        if segments_with_pause.is_empty() {
            return Err(EngineError::new("No segments to synthesize".to_string()));
        }

        // 3. å‡†å¤‡é˜¶æ®µï¼šé¢„å…ˆå‡†å¤‡æ‰€æœ‰ TTS è¯·æ±‚å‚æ•°ï¼ˆåŒ…æ‹¬å¼‚æ­¥çš„ voice è·å–ï¼‰
        let tts_incremental_start = Instant::now();
        
        // 3.1. é¢„å…ˆè·å– voiceï¼ˆå¦‚æœéœ€è¦ï¼Œä¸”åªè·å–ä¸€æ¬¡ï¼‰
        let use_reference_audio = reference_audio.clone();
        let common_voice = if use_reference_audio.is_none() {
            if let Some(ref speaker_id) = translation.speaker_id {
                if let Some(ref mapper) = self.speaker_voice_mapper {
                    mapper.get_or_assign_voice(speaker_id).await
                } else {
                    String::new()
                }
            } else {
                String::new()
            }
        } else {
            String::new()
        };
        
        // 3.2. è®¡ç®—æ¯ä¸ª segment çš„ç‹¬ç«‹è¯­é€Ÿ
        let segment_speech_rates: Vec<Option<f32>> = if let (Some(source_duration_ms), Some(source_text)) = 
            (translation.source_audio_duration_ms, translation.source_text.clone()) {
            let source_text_len = source_text.chars().count() as f32;
            let source_duration_sec = source_duration_ms as f32 / 1000.0;
            let overall_speech_rate = if source_duration_sec > 0.0 {
                source_text_len / source_duration_sec
            } else {
                0.0
            };
            
            // âš ï¸ é‡è¦ï¼šè¯­é€Ÿåº”è¯¥åŸºäºåŸå§‹è¾“å…¥æ–‡æœ¬å’ŒéŸ³é¢‘æ—¶é•¿ï¼Œè€Œä¸æ˜¯ç¿»è¯‘åçš„æ–‡æœ¬
            // ç¿»è¯‘åçš„æ–‡æœ¬é•¿åº¦å¯èƒ½å®Œå…¨ä¸åŒï¼Œç›´æ¥ç”¨ç¿»è¯‘æ–‡æœ¬è®¡ç®—è¯­é€Ÿä¼šå¯¼è‡´é”™è¯¯
            // æ–¹æ³•ï¼šæ ¹æ®ç¿»è¯‘æ–‡æœ¬çš„ segment é•¿åº¦æ¯”ä¾‹ï¼Œä¼°ç®—å…¶åœ¨åŸå§‹è¾“å…¥ä¸­çš„å¯¹åº”æ—¶é•¿
            let total_translated_chars = translation.translated_text.chars().count() as f32;
            segments_with_pause.iter().map(|seg| {
                let segment_chars = seg.text.chars().count() as f32;
                if total_translated_chars > 0.0 && source_duration_sec > 0.0 {
                    // è®¡ç®—è¿™ä¸ª segment åœ¨ç¿»è¯‘æ–‡æœ¬ä¸­çš„æ¯”ä¾‹
                    let segment_ratio = segment_chars / total_translated_chars;
                    // å‡è®¾ç¿»è¯‘æ–‡æœ¬çš„ segment æ¯”ä¾‹ä¸åŸå§‹æ–‡æœ¬çš„ segment æ¯”ä¾‹ç›¸ä¼¼
                    // å› æ­¤è¿™ä¸ª segment å¯¹åº”çš„åŸå§‹éŸ³é¢‘æ—¶é•¿ = æ€»æ—¶é•¿ * æ¯”ä¾‹
                    let estimated_segment_duration_sec = source_duration_sec * segment_ratio;
                    // âš ï¸ å…³é”®ä¿®æ­£ï¼šä½¿ç”¨ç¿»è¯‘æ–‡æœ¬çš„å­—ç¬¦æ•°å’ŒåŸå§‹éŸ³é¢‘æ—¶é•¿è®¡ç®—è¯­é€Ÿ
                    // è¿™æ · TTS è¾“å‡ºä¼šåŒ¹é…åŸå§‹è¾“å…¥çš„è¯­é€Ÿ
                    let segment_speech_rate = if estimated_segment_duration_sec > 0.0 {
                        segment_chars / estimated_segment_duration_sec
                    } else {
                        overall_speech_rate
                    };
                    Some(segment_speech_rate)
                } else {
                    None
                }
            }).collect()
        } else {
            segments_with_pause.iter().map(|_| {
                if let Some(ref speaker_id) = translation.speaker_id {
                    self.get_vad_speaker_speech_rate(speaker_id)
                } else {
                    None
                }
            }).collect()
        };
        
        // 3.3. åˆ›å»ºæ‰€æœ‰ segment çš„å¹¶è¡Œå¤„ç†ä»»åŠ¡
        eprintln!("[TTS] âš¡ Starting parallel synthesis of {} segments...", segments_with_pause.len());
        let segment_futures: Vec<_> = segments_with_pause.iter().enumerate().map(|(idx, segment)| {
            let is_last = idx == segments_with_pause.len() - 1;
            let segment_text = segment.text.clone();
            let segment_pause_type = segment.pause_type;
            
            // é¢„å¤„ç†æ–‡æœ¬
            let processed_text = if target_language.starts_with("zh") {
                Self::convert_decimals_to_chinese(&segment_text)
            } else {
                segment_text.clone()
            };
            
            // è·å–è¯­é€Ÿ
            let speech_rate = segment_speech_rates.get(idx)
                .and_then(|rate| *rate)
                .or_else(|| {
                    if let Some(ref speaker_id) = translation.speaker_id {
                        self.get_vad_speaker_speech_rate(speaker_id)
                    } else {
                        None
                    }
                });
            
            // æ„é€  TTS è¯·æ±‚
            let tts_request = TtsRequest {
                text: processed_text.clone(),
                voice: common_voice.clone(),
                locale: target_language.clone(),
                reference_audio: use_reference_audio.clone(),
                speaker: translation.speaker_id.clone(),
                speech_rate,
            };
            
            // è®°å½•æ—¥å¿—
            if let Some(rate) = speech_rate {
                eprintln!("[TTS] âš¡ Queueing segment {:2} for parallel synthesis: '{}' (speech_rate: {:.2} chars/s)", 
                    idx + 1, segment_text, rate);
            } else {
                eprintln!("[TTS] âš¡ Queueing segment {:2} for parallel synthesis: '{}'", idx + 1, segment_text);
            }
            
            // åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ï¼šåˆæˆ + å¢å¼º
            let tts_clone = Arc::clone(&self.tts);
            let fallback_tts_clone = self.fallback_tts.as_ref().map(Arc::clone);
            let enhancer_clone = self.audio_enhancer.as_ref().map(Arc::clone);
            
            async move {
                let segment_tts_start = Instant::now();
                
                // åˆæˆéŸ³é¢‘ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰
                let mut chunk = match tts_clone.synthesize(tts_request.clone()).await {
                    Ok(chunk) => chunk,
                    Err(e) => {
                        // æ£€æŸ¥æ˜¯å¦æ˜¯è¯­è¨€ä¸æ”¯æŒçš„é”™è¯¯
                        let error_msg = e.to_string().to_lowercase();
                        let is_language_error = error_msg.contains("language") || 
                                                error_msg.contains("not in the available languages") ||
                                                error_msg.contains("ä¸æ”¯æŒ") ||
                                                error_msg.contains("dict_keys");
                        
                        if is_language_error {
                            eprintln!("[TTS] âš ï¸  Segment {:2} failed due to unsupported language, trying fallback TTS...", idx + 1);
                            // å¦‚æœæœ‰å›é€€ TTSï¼Œå°è¯•ä½¿ç”¨å®ƒï¼ˆä¸ä¼ é€’ reference_audioï¼Œå› ä¸º Piper ä¸æ”¯æŒï¼‰
                            // ä½†ä¿ç•™è¯­é€Ÿä¿¡æ¯ï¼Œç¡®ä¿è¾“å‡ºè¯­é€ŸåŒ¹é…è¾“å…¥
                            if let Some(ref fallback) = fallback_tts_clone {
                                let mut fallback_request = tts_request.clone();
                                fallback_request.reference_audio = None;  // Piper ä¸æ”¯æŒ reference_audio
                                // è¯­é€Ÿä¿¡æ¯å·²ä¿ç•™åœ¨ tts_request ä¸­ï¼Œä¼šä¼ é€’ç»™ Piper
                                
                                eprintln!("[TTS] âš¡ Fallback: Using Piper TTS with speech_rate={:?}", fallback_request.speech_rate);
                                
                                match fallback.synthesize(fallback_request).await {
                                    Ok(fallback_chunk) => {
                                        eprintln!("[TTS] âœ… Segment {:2} fallback TTS succeeded", idx + 1);
                                        fallback_chunk
                                    }
                                    Err(fallback_err) => {
                                        eprintln!("[TTS] âŒ Segment {:2} fallback TTS also failed: {}", idx + 1, fallback_err);
                                        return Err(fallback_err);
                                    }
                                }
                            } else {
                                eprintln!("[TTS] âŒ No fallback TTS available, segment {:2} failed", idx + 1);
                                return Err(e);
                            }
                        } else {
                            eprintln!("[TTS] âŒ Segment {:2} synthesis failed: {}", idx + 1, e);
                            return Err(e);
                        }
                    }
                };
                
                let segment_tts_ms = segment_tts_start.elapsed().as_millis() as u64;
                
                // åº”ç”¨éŸ³é¢‘å¢å¼º
                if let Some(ref enhancer) = enhancer_clone {
                    let pause_type = if segment_pause_type != crate::text_segmentation::PauseType::None {
                        Some(segment_pause_type)
                    } else {
                        None
                    };
                    
                    match enhancer.enhance_audio_with_pause_type(
                        &chunk.audio,
                        idx == 0,  // is_first
                        is_last,   // is_last
                        pause_type,
                    ).await {
                        Ok(enhanced_audio) => {
                            chunk.audio = enhanced_audio;
                            eprintln!("[TTS] âœ… Segment {:2} completed in {}ms: '{}' (audio_size: {} bytes)", 
                                idx + 1, segment_tts_ms, segment_text, chunk.audio.len());
                        }
                        Err(e) => {
                            eprintln!("[TTS] âš ï¸  Segment {:2} enhancement failed: {}, using original audio", idx + 1, e);
                        }
                    }
                } else {
                    eprintln!("[TTS] âœ… Segment {:2} completed in {}ms: '{}' (audio_size: {} bytes)", 
                        idx + 1, segment_tts_ms, segment_text, chunk.audio.len());
                }
                
                Ok((idx, chunk, segment_tts_ms, is_last))
            }
        }).collect();
        
        // 3.4. å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆ
        eprintln!("[TTS] âš¡ Executing {} segments in parallel...", segment_futures.len());
        let segment_results = join_all(segment_futures).await;
        
        // 3.5. æŒ‰é¡ºåºå¤„ç†ç»“æœï¼ˆä¿æŒæ’­æ”¾é¡ºåºï¼‰
        let mut ordered_chunks = Vec::new();
        let mut total_yourtts_ms = 0u64;
        let mut yourtts_call_count = 0u64;
        
        // æ”¶é›†æ‰€æœ‰ç»“æœï¼ˆåŒ…å«ç´¢å¼•ï¼Œç”¨äºæ’åºï¼‰
        let mut results_with_idx: Vec<_> = Vec::new();
        for result in segment_results {
            results_with_idx.push(result?);
        }
        
        // æŒ‰ç´¢å¼•æ’åºä»¥ç¡®ä¿é¡ºåº
        results_with_idx.sort_by_key(|(idx, _, _, _)| *idx);
        
        // æŒ‰é¡ºåºå¤„ç†æ¯ä¸ªç»“æœ
        let mut current_timestamp = timestamp_ms;
        for (idx, mut chunk, segment_tts_ms, is_last) in results_with_idx {
            // ç´¯è®¡ YourTTS è€—æ—¶
            if self.tts_service_url.as_ref()
                .map(|url| url.contains("5004") || url.contains("yourtts"))
                .unwrap_or(false) {
                total_yourtts_ms += segment_tts_ms;
                yourtts_call_count += 1;
            }
            
            // è®¾ç½®æ—¶é—´æˆ³å’Œ is_last æ ‡å¿—
            chunk.timestamp_ms = current_timestamp;
            chunk.is_last = is_last;
            
            // ç«‹å³å‘å¸ƒï¼ˆbuffer_sentences == 0ï¼‰
            if self.tts_buffer_sentences == 0 {
                self.publish_tts_event(&chunk, current_timestamp).await?;
                eprintln!("[TTS] ğŸ“¤ Published segment {:2} immediately (timestamp: {}ms)", idx + 1, current_timestamp);
            }
            
            // ä¿å­˜åˆ° ordered_chunksï¼ˆç”¨äºåˆå¹¶å’Œç¼“å†²æ¨¡å¼ï¼‰
            ordered_chunks.push(chunk);
            
            // æ›´æ–°æ—¶é—´æˆ³
            current_timestamp += 100; // æ¯ä¸ªçŸ­å¥é—´éš” 100ms
        }
        
        // 3.6. ç¼“å†²æ¨¡å¼ï¼šå‘å¸ƒå‰©ä½™çš„çŸ­å¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.tts_buffer_sentences > 0 {
            for (idx, chunk) in ordered_chunks.iter().enumerate() {
                self.publish_tts_event(chunk, chunk.timestamp_ms).await?;
                eprintln!("[TTS] ğŸ“¤ Published segment {:2} from buffer (timestamp: {}ms)", idx + 1, chunk.timestamp_ms);
            }
        }
        
        // 3.7. åˆå¹¶æ‰€æœ‰ chunks çš„éŸ³é¢‘æ•°æ®ï¼Œè¿”å›å®Œæ•´çš„éŸ³é¢‘
        let mut merged_audio = Vec::new();
        for chunk in &ordered_chunks {
            merged_audio.extend_from_slice(&chunk.audio);
        }
        
        let tts_incremental_total_ms = tts_incremental_start.elapsed().as_millis() as u64;
        eprintln!("[TTS] âš¡ Parallel synthesis completed: {} segments in {}ms (avg: {:.1}ms/segment)", 
            ordered_chunks.len(), 
            tts_incremental_total_ms,
            if ordered_chunks.len() > 0 { tts_incremental_total_ms as f32 / ordered_chunks.len() as f32 } else { 0.0 });
        eprintln!("[TTS] Total audio size: {} bytes", merged_audio.len());
        
        // åˆ›å»ºåˆå¹¶åçš„ chunk ç”¨äºè¿”å›
        let merged_chunk = if let Some(first_chunk) = ordered_chunks.first() {
            TtsStreamChunk {
                audio: merged_audio,
                timestamp_ms: first_chunk.timestamp_ms,
                is_last: true,
            }
        } else {
            return Err(EngineError::new("No chunks to merge".to_string()));
        };
        
        // å¦‚æœä½¿ç”¨ YourTTSï¼Œè®°å½• YourTTS çš„è€—æ—¶
        let yourtts_ms = if self.tts_service_url.as_ref()
            .map(|url| url.contains("5004") || url.contains("yourtts"))
            .unwrap_or(false) {
            if yourtts_call_count > 0 {
                Some(total_yourtts_ms)
            } else {
                None
            }
        } else {
            None
        };
        
        if let Some(yt_ms) = yourtts_ms {
            eprintln!("[TTS] YourTTS total time: {}ms ({} calls, avg: {:.1}ms per call)", 
                yt_ms, yourtts_call_count, if yourtts_call_count > 0 { yt_ms as f64 / yourtts_call_count as f64 } else { 0.0 });
        }
        
        // è¿”å›åˆå¹¶åçš„ chunk
        Ok((merged_chunk, yourtts_ms))
    }

    /// æ£€æŸ¥ ASR è½¬å½•æ–‡æœ¬æ˜¯å¦ä¸ºæ— æ„ä¹‰çš„æ–‡æœ¬ï¼ˆåº”è¯¥è¢«è¿‡æ»¤ï¼Œä¸è¿›å…¥ç¿»è¯‘/TTSï¼‰
    /// 
    /// è¿‡æ»¤è§„åˆ™ï¼š
    /// - åŒ…å« "(Titled by ...)"ã€"å­—å¹•:..." ç­‰å­—å¹•ä¿¡æ¯
    /// - åŒ…å«è§†é¢‘ç»“å°¾å¸¸è§å­—å¹•ï¼Œå¦‚ "è¬è¬å¤§å®¶æ”¶çœ‹"ã€"è°¢è°¢å¤§å®¶è§‚çœ‹" ç­‰
    /// - åŒ…å«å…¶ä»–æ— æ„ä¹‰çš„æ¨¡å¼
    /// 
    /// è¿”å› true è¡¨ç¤ºåº”è¯¥è¿‡æ»¤æ‰ï¼Œä¸è¿›å…¥åç»­å¤„ç†
    fn is_meaningless_transcript(text: &str) -> bool {
        let text_lower = text.to_lowercase();
        let text_trimmed = text.trim();
        
        // å®Œæ•´çš„æ— æ„ä¹‰æ–‡æœ¬åˆ—è¡¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        let exact_matches = vec![
            "è¬è¬å¤§å®¶æ”¶çœ‹",
            "è°¢è°¢å¤§å®¶æ”¶çœ‹",
            "è¬è¬å¤§å®¶è§€çœ‹",
            "è°¢è°¢å¤§å®¶è§‚çœ‹",
            "è¬è¬è§€çœ‹",
            "è°¢è°¢è§‚çœ‹",
            "æ„Ÿè¬æ”¶çœ‹",
            "æ„Ÿè°¢æ”¶çœ‹",
            "thank you for watching",
            "thanks for watching",
        ];
        
        // æ£€æŸ¥ç²¾ç¡®åŒ¹é…ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        for pattern in &exact_matches {
            if text_trimmed.eq_ignore_ascii_case(pattern) {
                return true;
            }
        }
        
        // æ— æ„ä¹‰æ¨¡å¼åˆ—è¡¨ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼‰
        let meaningless_patterns = vec![
            "titled by",
            "title:",
            "å­—å¹•:",
            "subtitle:",
            "source:",
            "author:",
            "translated by",
        ];
        
        // æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•æ— æ„ä¹‰æ¨¡å¼
        for pattern in meaningless_patterns {
            if text_lower.contains(pattern) {
                // è¿›ä¸€æ­¥æ£€æŸ¥ï¼šç¡®è®¤æ˜¯åœ¨æ‹¬å·æˆ–æ–¹æ‹¬å·å†…çš„æ¨¡å¼
                // è¿™æ ·å¯ä»¥é¿å…è¯¯åˆ¤æ­£å¸¸æ–‡æœ¬
                let pattern_pos = text_lower.find(pattern);
                if let Some(pos) = pattern_pos {
                    // æ£€æŸ¥å‰åæ˜¯å¦æœ‰æ‹¬å·ï¼ˆç®€å•æ£€æŸ¥ï¼Œé¿å…å¤æ‚è§£æï¼‰
                    let before = if pos > 0 { &text_lower[..pos] } else { "" };
                    let after = if pos + pattern.len() < text_lower.len() { &text_lower[pos + pattern.len()..] } else { "" };
                    
                    // æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰å¼€æ‹¬å·ï¼Œåé¢æ˜¯å¦æœ‰é—­æ‹¬å·
                    let has_open_bracket = before.chars().rev().take(10).any(|c| matches!(c, '(' | '[' | 'ï¼ˆ'));
                    let has_close_bracket = after.chars().take(50).any(|c| matches!(c, ')' | ']' | 'ï¼‰'));
                    
                    // å¦‚æœæ¨¡å¼åœ¨æ‹¬å·å†…ï¼Œåˆ¤å®šä¸ºæ— æ„ä¹‰
                    if has_open_bracket || has_close_bracket {
                        return true;
                    }
                }
            }
        }
        
        false
    }

    /// å°†ä¸­æ–‡æ–‡æœ¬ä¸­çš„å°æ•°è½¬æ¢ä¸ºä¸­æ–‡è¯»æ³•
    /// 
    /// ä¾‹å¦‚ï¼š"ç‰ˆæœ¬1.4" -> "ç‰ˆæœ¬ä¸€ç‚¹å››"
    ///      "ä»·æ ¼3.14" -> "ä»·æ ¼ä¸‰ç‚¹ä¸€å››"
    ///      "ç‰ˆæœ¬1.0" -> "ç‰ˆæœ¬ä¸€ç‚¹é›¶"
    fn convert_decimals_to_chinese(text: &str) -> String {
        let mut result = String::new();
        let mut chars = text.chars().peekable();
        
        while let Some(ch) = chars.next() {
            if ch.is_ascii_digit() {
                // å¼€å§‹è¯»å–æ•°å­—
                let mut num_str = String::new();
                num_str.push(ch);
                
                // æ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯å°æ•°ç‚¹
                let mut has_decimal = false;
                if let Some(&'.') = chars.peek() {
                    chars.next(); // è·³è¿‡å°æ•°ç‚¹
                    has_decimal = true;
                    num_str.push('.');
                    
                    // è¯»å–å°æ•°ç‚¹åçš„æ•°å­—
                    while let Some(&next_ch) = chars.peek() {
                        if next_ch.is_ascii_digit() {
                            num_str.push(chars.next().unwrap());
                        } else {
                            break;
                        }
                    }
                } else {
                    // ç»§ç»­è¯»å–æ•´æ•°éƒ¨åˆ†
                    while let Some(&next_ch) = chars.peek() {
                        if next_ch.is_ascii_digit() {
                            num_str.push(chars.next().unwrap());
                        } else {
                            break;
                        }
                    }
                }
                
                // å¦‚æœæ˜¯å°æ•°ï¼Œè½¬æ¢ä¸ºä¸­æ–‡è¯»æ³•
                if has_decimal && num_str.contains('.') {
                    let parts: Vec<&str> = num_str.split('.').collect();
                    if parts.len() == 2 {
                        // è½¬æ¢æ•´æ•°éƒ¨åˆ†
                        let int_part = parts[0];
                        let mut chinese_num = String::new();
                        for digit_char in int_part.chars() {
                            if let Some(digit) = digit_char.to_digit(10) {
                                chinese_num.push_str(match digit {
                                    0 => "é›¶",
                                    1 => "ä¸€",
                                    2 => "äºŒ",
                                    3 => "ä¸‰",
                                    4 => "å››",
                                    5 => "äº”",
                                    6 => "å…­",
                                    7 => "ä¸ƒ",
                                    8 => "å…«",
                                    9 => "ä¹",
                                    _ => "",
                                });
                            }
                        }
                        
                        // æ·»åŠ "ç‚¹"
                        chinese_num.push_str("ç‚¹");
                        
                        // è½¬æ¢å°æ•°éƒ¨åˆ†
                        for digit_char in parts[1].chars() {
                            if let Some(digit) = digit_char.to_digit(10) {
                                chinese_num.push_str(match digit {
                                    0 => "é›¶",
                                    1 => "ä¸€",
                                    2 => "äºŒ",
                                    3 => "ä¸‰",
                                    4 => "å››",
                                    5 => "äº”",
                                    6 => "å…­",
                                    7 => "ä¸ƒ",
                                    8 => "å…«",
                                    9 => "ä¹",
                                    _ => "",
                                });
                            }
                        }
                        
                        result.push_str(&chinese_num);
                        continue;
                    }
                }
                
                // å¦‚æœä¸æ˜¯å°æ•°ï¼Œä¿æŒåŸæ ·
                result.push_str(&num_str);
            } else {
                result.push(ch);
            }
        }
        
        result
    }

    /// æ›´æ–°VADä¸­çš„è¯´è¯è€…è¯­é€Ÿï¼ˆç”¨äºè‡ªé€‚åº”è°ƒæ•´ï¼‰
    fn update_vad_speaker_speech_rate(&self, speaker_id: &str, text: &str, audio_duration_ms: u64) {
        // å°è¯•å°† VAD è½¬æ¢ä¸º SileroVad
        let vad_ptr = Arc::as_ptr(&self.vad);
        let silero_vad_ptr = vad_ptr as *const crate::vad::SileroVad;
        
        unsafe {
            if let Some(silero_vad) = silero_vad_ptr.as_ref() {
                silero_vad.update_speaker_speech_rate(speaker_id, text, audio_duration_ms);
            }
        }
    }
    
    /// è·å–è¯´è¯è€…çš„è¯­é€Ÿï¼ˆç”¨äºä¼ é€’ç»™TTSï¼‰
    fn get_vad_speaker_speech_rate(&self, speaker_id: &str) -> Option<f32> {
        // å°è¯•å°† VAD è½¬æ¢ä¸º SileroVad
        let vad_ptr = Arc::as_ptr(&self.vad);
        let silero_vad_ptr = vad_ptr as *const crate::vad::SileroVad;
        
        unsafe {
            if let Some(silero_vad) = silero_vad_ptr.as_ref() {
                silero_vad.get_speaker_speech_rate(speaker_id)
            } else {
                None
            }
        }
    }
    
    /// è®¾ç½®VADçš„å½“å‰è¯´è¯è€…IDï¼ˆç”¨äºè‡ªé€‚åº”è°ƒæ•´ï¼‰
    fn set_vad_current_speaker(&self, speaker_id: Option<&str>) {
        // å°è¯•å°† VAD è½¬æ¢ä¸º SileroVad
        let vad_ptr = Arc::as_ptr(&self.vad);
        let silero_vad_ptr = vad_ptr as *const crate::vad::SileroVad;
        
        unsafe {
            if let Some(silero_vad) = silero_vad_ptr.as_ref() {
                silero_vad.set_current_speaker(speaker_id);
            }
        }
    }
    
    /// å‘å¸ƒ TTS äº‹ä»¶
    async fn publish_tts_event(
        &self,
        tts_chunk: &TtsStreamChunk,
        timestamp_ms: u64,
    ) -> EngineResult<()> {
        let event = CoreEvent {
            topic: EventTopic("Tts".to_string()),
            payload: json!({
                "audio_length": tts_chunk.audio.len(),
                "timestamp_ms": tts_chunk.timestamp_ms,
                "is_last": tts_chunk.is_last,
            }),
            timestamp_ms,
        };
        self.event_bus.publish(event).await?;
        Ok(())
    }
}

/// å¤„ç†ç»“æœï¼ˆåŒ…å« ASRã€Emotionã€NMT å’Œ TTS ç»“æœï¼‰
#[derive(Debug, Clone)]
pub struct ProcessResult {
    pub asr: AsrResult,
    pub emotion: Option<EmotionResponse>,
    pub translation: Option<TranslationResponse>,
    pub tts: Option<TtsStreamChunk>,
}
