use std::path::PathBuf;
use std::sync::Arc;
use std::io::Cursor;
use std::time::Instant;
use axum::{
    extract::{ws::{WebSocketUpgrade, WebSocket, Message}, State},
    http::StatusCode,
    response::Response,
    routing::{get, post},
    Json, Router,
};
use futures_util::{SinkExt, StreamExt};
use serde::{Deserialize, Serialize};
use tokio::net::TcpListener;
use tower_http::cors::CorsLayer;
use base64::{Engine as _, engine::general_purpose};

use core_engine::bootstrap::{CoreEngine, CoreEngineBuilder, ProcessResult};
use core_engine::config_manager::{ConfigManager, EngineConfig};
use core_engine::error::EngineResult;
use core_engine::types::AudioFrame;
use core_engine::health_check::HealthChecker;
use core_engine::emotion_adapter::EmotionStub;
use core_engine::persona_adapter::PersonaStub;
use core_engine::event_bus::{EventBus, CoreEvent, EventTopic, EventSubscription, ChannelEventBus};
use core_engine::vad::{VoiceActivityDetector, DetectionOutcome, SileroVad};
use core_engine::cache_manager::CacheManager;
use core_engine::telemetry::{TelemetrySink, TelemetryDatum};
use core_engine::speaker_identifier::{SpeakerIdentifierMode, EmbeddingBasedMode, EmbeddingBasedSpeakerIdentifier};
use core_engine::tts_streaming::YourTtsHttpConfig;
use async_trait::async_trait;

/// è¿è¡Œæ—¶é…ç½®ï¼ˆä» TOML æ–‡ä»¶åŠ è½½ï¼‰
#[derive(Debug, Clone, Deserialize)]
struct RuntimeConfig {
    nmt: NmtConfig,
    tts: TtsConfig,
    #[serde(default)]
    asr: Option<AsrConfig>,
    #[serde(default)]
    speaker_embedding: Option<SpeakerEmbeddingConfig>,
    #[serde(default)]
    yourtts: Option<YourTtsConfig>,
    engine: EngineRuntimeConfig,
}

#[derive(Debug, Clone, Deserialize)]
struct NmtConfig {
    url: String,
}

#[derive(Debug, Clone, Deserialize)]
struct TtsConfig {
    url: String,
}

#[derive(Debug, Clone, Deserialize)]
struct AsrConfig {
    url: String,
}

#[derive(Debug, Clone, Deserialize)]
struct SpeakerEmbeddingConfig {
    url: String,
}

#[derive(Debug, Clone, Deserialize)]
struct YourTtsConfig {
    url: String,
}

#[derive(Debug, Clone, Deserialize)]
struct EngineRuntimeConfig {
    port: u16,
    whisper_model_path: Option<String>,
    silero_vad_model_path: Option<String>,
}

/// S2S è¯·æ±‚ï¼ˆæ•´å¥ç¿»è¯‘ï¼‰
#[derive(Debug, Deserialize)]
struct S2SRequest {
    audio: String, // base64 ç¼–ç çš„éŸ³é¢‘æ•°æ®
    src_lang: String,
    tgt_lang: String,
}

/// S2S å“åº”
#[derive(Debug, Serialize)]
struct S2SResponse {
    audio: String, // base64 ç¼–ç çš„éŸ³é¢‘æ•°æ®
    transcript: String,
    translation: String,
}

/// å¥åº·æ£€æŸ¥å“åº”
#[derive(Debug, Serialize)]
struct HealthResponse {
    status: String,
    services: ServiceHealth,
}

#[derive(Debug, Serialize)]
struct ServiceHealth {
    nmt: bool,
    tts: bool,
    engine: bool,
}

/// åº”ç”¨çŠ¶æ€
#[derive(Clone)]
struct AppState {
    engine: Arc<CoreEngine>,
    config: RuntimeConfig,
    simple_config: Arc<SimpleConfig>,  // ç”¨äºåŠ¨æ€æ›´æ–°è¯­è¨€é…ç½®
    event_bus: Arc<ChannelEventBus>,  // äº‹ä»¶æ€»çº¿ï¼ˆç”¨äº WebSocket è®¢é˜…ï¼‰
    speaker_mode: Arc<RwLock<EmbeddingBasedMode>>,  // å½“å‰è¯´è¯è€…è¯†åˆ«æ¨¡å¼
    speaker_identifier: Option<Arc<EmbeddingBasedSpeakerIdentifier>>,  // è¯´è¯è€…è¯†åˆ«å™¨å¼•ç”¨ï¼ˆç”¨äºåŠ¨æ€åˆ‡æ¢æ¨¡å¼ï¼‰
}

// ç®€å•çš„é»˜è®¤å®ç°
struct SimpleEventBus;

#[async_trait]
impl EventBus for SimpleEventBus {
    async fn start(&self) -> EngineResult<()> {
        Ok(())
    }

    async fn stop(&self) -> EngineResult<()> {
        Ok(())
    }

    async fn publish(&self, _event: CoreEvent) -> EngineResult<()> {
        Ok(())
    }

    async fn subscribe(&self, topic: EventTopic) -> EngineResult<EventSubscription> {
        Ok(EventSubscription { topic })
    }
}

const FINAL_FRAME_FLAG: u64 = 1u64 << 63;

struct SimpleVad;

#[async_trait]
impl VoiceActivityDetector for SimpleVad {
    async fn detect(&self, frame: AudioFrame) -> EngineResult<DetectionOutcome> {
        let is_final = (frame.timestamp_ms & FINAL_FRAME_FLAG) != 0;
        let cleaned_timestamp = frame.timestamp_ms & !FINAL_FRAME_FLAG;
        let mut cleaned_frame = frame.clone();
        cleaned_frame.timestamp_ms = cleaned_timestamp;
        Ok(DetectionOutcome {
            boundary_type: None,
            is_boundary: is_final,
            confidence: 1.0,
            frame: cleaned_frame,
        })
    }
}

use tokio::sync::RwLock;

struct SimpleConfig {
    source_lang: Arc<RwLock<String>>,
    target_lang: Arc<RwLock<String>>,
}

impl SimpleConfig {
    fn new(source_lang: String, target_lang: String) -> Self {
        Self {
            source_lang: Arc::new(RwLock::new(source_lang)),
            target_lang: Arc::new(RwLock::new(target_lang)),
        }
    }

    async fn set_target_language(&self, lang: String) {
        *self.target_lang.write().await = lang;
    }

    async fn set_source_language(&self, lang: String) {
        *self.source_lang.write().await = lang;
    }
}

#[async_trait]
impl ConfigManager for SimpleConfig {
    async fn load(&self) -> EngineResult<EngineConfig> {
        let source_lang = self.source_lang.read().await.clone();
        let target_lang = self.target_lang.read().await.clone();
        Ok(EngineConfig {
            mode: "balanced".to_string(),
            source_language: source_lang,
            target_language: target_lang,
        })
    }

    async fn current(&self) -> EngineResult<EngineConfig> {
        self.load().await
    }
}

struct SimpleCache;

#[async_trait]
impl CacheManager for SimpleCache {
    async fn warm_up(&self) -> EngineResult<()> {
        Ok(())
    }

    async fn purge(&self) -> EngineResult<()> {
        Ok(())
    }
}

struct SimpleTelemetry;

#[async_trait]
impl TelemetrySink for SimpleTelemetry {
    async fn record(&self, _datum: TelemetryDatum) -> EngineResult<()> {
        Ok(())
    }
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    let args: Vec<String> = std::env::args().collect();
    let config_path = args
        .iter()
        .position(|a| a == "--config")
        .and_then(|i| args.get(i + 1))
        .map(|s| PathBuf::from(s))
        .unwrap_or_else(|| PathBuf::from("lingua_core_config.toml"));

    eprintln!("[INFO] Loading config from: {}", config_path.display());

    // 2. åŠ è½½é…ç½®æ–‡ä»¶
    let config_content = std::fs::read_to_string(&config_path)
        .map_err(|e| anyhow::anyhow!("Failed to read config file: {}", e))?;
    let runtime_config: RuntimeConfig = toml::from_str(&config_content)
        .map_err(|e| anyhow::anyhow!("Failed to parse config file: {}", e))?;

    eprintln!("[INFO] Config loaded:");
    eprintln!("[INFO]   NMT URL: {}", runtime_config.nmt.url);
    eprintln!("[INFO]   TTS URL: {}", runtime_config.tts.url);
    eprintln!("[INFO]   Engine Port: {}", runtime_config.engine.port);

    // 2.5. åˆå§‹åŒ– ASR è¿‡æ»¤å™¨é…ç½®ï¼ˆå¿…é¡»åœ¨åˆ›å»º CoreEngine ä¹‹å‰ï¼‰
    let _ = core_engine::asr_filters::config::init_config_from_file();
    eprintln!("[INFO] ASR filter config initialized");

    // 3. åˆ›å»º SimpleConfigï¼ˆç”¨äºåŠ¨æ€æ›´æ–°è¯­è¨€ï¼‰
    let simple_config = Arc::new(SimpleConfig::new("en".to_string(), "zh".to_string()));
    
    // 4. åˆå§‹åŒ–äº‹ä»¶æ€»çº¿ï¼ˆä½¿ç”¨ ChannelEventBus ä»¥æ”¯æŒçœŸæ­£çš„å‘å¸ƒ/è®¢é˜…ï¼‰
    let event_bus = Arc::new(ChannelEventBus::new());
    event_bus.start().await
        .map_err(|e| anyhow::anyhow!("Failed to start event bus: {}", e))?;
    
    // 5. åˆå§‹åŒ– CoreEngine å’Œ Speaker Identifier
    let (engine, speaker_identifier) = initialize_engine(&runtime_config, simple_config.clone(), event_bus.clone()).await?;
    eprintln!("[INFO] CoreEngine initialized successfully");

    // 6. å¯åŠ¨ HTTP æœåŠ¡å™¨
    let app_state = AppState {
        engine: Arc::new(engine),
        config: runtime_config.clone(),
        simple_config: simple_config.clone(),
        event_bus: event_bus.clone(),
        speaker_mode: Arc::new(RwLock::new(EmbeddingBasedMode::SingleUser)),  // é»˜è®¤å•äººæ¨¡å¼
        speaker_identifier,  // è¯´è¯è€…è¯†åˆ«å™¨å¼•ç”¨ï¼ˆç”¨äºåŠ¨æ€åˆ‡æ¢æ¨¡å¼ï¼‰
    };

    let app = Router::new()
        .route("/health", get(health_check))
        .route("/s2s", post(s2s_handler))
        .route("/stream", get(stream_handler))
        .route("/config/speaker-mode", get(get_speaker_mode))
        .route("/config/speaker-mode", post(set_speaker_mode))
        .layer(CorsLayer::permissive())
        .with_state(app_state);

    let addr = format!("0.0.0.0:{}", runtime_config.engine.port);
    eprintln!("[INFO] Starting HTTP server on {}", addr);

    let listener = TcpListener::bind(&addr).await?;
    axum::serve(listener, app).await?;

    Ok(())
}

/// åˆå§‹åŒ– CoreEngine
/// è¿”å› (CoreEngine, Option<Arc<EmbeddingBasedSpeakerIdentifier>>)
async fn initialize_engine(
    config: &RuntimeConfig, 
    simple_config: Arc<SimpleConfig>,
    event_bus: Arc<ChannelEventBus>,
) -> EngineResult<(CoreEngine, Option<Arc<EmbeddingBasedSpeakerIdentifier>>)> {
    let crate_root = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
    
    // 1. åˆå§‹åŒ– SileroVad
    // æ³¨æ„ï¼šé…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„å¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„
    // - ç»å¯¹è·¯å¾„ï¼šç›´æ¥ä½¿ç”¨ï¼ˆä¾‹å¦‚ï¼šD:\Programs\github\lingua\core\engine\models\vad\silero\silero_vad_official.onnxï¼‰
    // - ç›¸å¯¹è·¯å¾„ï¼šä» crate_root è§£æï¼ˆä¾‹å¦‚ï¼šmodels/vad/silero/silero_vad_official.onnxï¼‰
    let silero_vad_model_path = config.engine.silero_vad_model_path.clone()
        .map(|p| {
            let path = PathBuf::from(&p);
            // å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ä» crate_root è§£æ
            if path.is_absolute() {
                path
            } else {
                // ç›¸å¯¹è·¯å¾„ï¼šä» crate_root è§£æ
                // æ³¨æ„ï¼šRust çš„ PathBuf::join() ä¼šè‡ªåŠ¨å¤„ç†è·¯å¾„åˆ†éš”ç¬¦ï¼ˆ/ å’Œ \ï¼‰
                crate_root.join(&p)
            }
        })
        .unwrap_or_else(|| crate_root.join("models/vad/silero/silero_vad_official.onnx"));
    
    eprintln!("[INFO] Crate root: {}", crate_root.display());
    eprintln!("[INFO] SileroVad model path from config: {:?}", config.engine.silero_vad_model_path);
    eprintln!("[INFO] Resolved SileroVad model path: {} (exists: {})", 
              silero_vad_model_path.display(), 
              silero_vad_model_path.exists());
    
    let vad: Arc<dyn VoiceActivityDetector> = if silero_vad_model_path.exists() {
        eprintln!("[INFO] Initializing SileroVad from: {}", silero_vad_model_path.display());
        Arc::new(SileroVad::new(&silero_vad_model_path)
            .map_err(|e| core_engine::error::EngineError::new(format!("Failed to initialize SileroVad: {}", e)))?) as Arc<dyn VoiceActivityDetector>
    } else {
        eprintln!("[WARN] SileroVad model not found at: {}, using SimpleVad", silero_vad_model_path.display());
        eprintln!("[WARN] Crate root: {}", crate_root.display());
        Arc::new(SimpleVad) as Arc<dyn VoiceActivityDetector>
    };

    // 2. åˆå§‹åŒ– ASRï¼ˆä¼˜å…ˆä½¿ç”¨ faster-whisperï¼Œå¦åˆ™ä½¿ç”¨æœ¬åœ° whisper-rsï¼‰
    let mut builder = CoreEngineBuilder::new()
        .event_bus(event_bus.clone() as Arc<dyn EventBus>)
        .vad(vad);
    
    if let Some(ref asr_config) = config.asr {
        eprintln!("[INFO] Initializing Faster-Whisper ASR: {}", asr_config.url);
        builder = builder.asr_with_faster_whisper(asr_config.url.clone(), 30)
            .map_err(|e| core_engine::error::EngineError::new(format!("Failed to initialize Faster-Whisper ASR: {}", e)))?;
    } else {
        eprintln!("[WARN] ASR config not found, using default Whisper");
        builder = builder.asr_with_default_whisper()
            .map_err(|e| core_engine::error::EngineError::new(format!("Failed to initialize ASR: {}", e)))?;
    }

    // 3. åˆå§‹åŒ– NMT
    builder = builder.nmt_with_m2m100_http_client(Some(&config.nmt.url))
        .map_err(|e| core_engine::error::EngineError::new(format!("Failed to initialize NMT: {}", e)))?;

    // 4. åˆå§‹åŒ– TTSï¼ˆä¼˜å…ˆä½¿ç”¨ YourTTSï¼Œå¦åˆ™ä½¿ç”¨ Piper TTSï¼‰
    if let Some(ref yourtts_config) = config.yourtts {
        eprintln!("[INFO] Initializing YourTTS: {}", yourtts_config.url);
        builder = builder.tts_with_yourtts_http(YourTtsHttpConfig {
            endpoint: yourtts_config.url.clone(),
            timeout_ms: 30000,
        })
        .map_err(|e| core_engine::error::EngineError::new(format!("Failed to initialize YourTTS: {}", e)))?;
    } else {
        eprintln!("[WARN] YourTTS config not found, using Piper TTS");
        builder = builder.tts_with_piper_http(core_engine::tts_streaming::PiperHttpConfig {
            endpoint: config.tts.url.clone(),
            default_voice: "zh_CN-huayan-medium".to_string(),
            timeout_ms: 8000,
        })
        .map_err(|e| core_engine::error::EngineError::new(format!("Failed to initialize TTS: {}", e)))?;
    }

    // 5. åˆå§‹åŒ–è¯´è¯è€…è¯†åˆ«ï¼ˆå¦‚æœé…ç½®äº† Speaker Embedding æœåŠ¡ï¼‰
    // åˆ›å»º identifier å¹¶ä¿å­˜å¼•ç”¨ï¼Œç„¶åè®© builder ä½¿ç”¨åŒä¸€ä¸ªå®ä¾‹
    let speaker_identifier_ref: Option<Arc<EmbeddingBasedSpeakerIdentifier>> = if let Some(ref speaker_config) = config.speaker_embedding {
        eprintln!("[INFO] Initializing Speaker Identification: {}", speaker_config.url);
        // åˆ›å»º identifier å¹¶ä¿å­˜å¼•ç”¨
        let identifier = EmbeddingBasedSpeakerIdentifier::new(
            Some(speaker_config.url.clone()),
            0.4,
            core_engine::speaker_identifier::EmbeddingBasedMode::SingleUser,
        )?;
        let identifier_arc = Arc::new(identifier);
        // å°† identifier è½¬æ¢ä¸º trait å¯¹è±¡ç”¨äº builder
        let identifier_for_builder: Arc<dyn core_engine::speaker_identifier::SpeakerIdentifier> = identifier_arc.clone();
        // ç›´æ¥è®¾ç½®åˆ° builderï¼Œä½¿ç”¨åŒä¸€ä¸ªå®ä¾‹ï¼ˆè¿™æ ·æ¨¡å¼åˆ‡æ¢æ‰èƒ½ç”Ÿæ•ˆï¼‰
        builder = builder.with_speaker_identifier_custom(identifier_for_builder);
        Some(identifier_arc)
    } else {
        eprintln!("[WARN] Speaker Embedding config not found, speaker identification disabled");
        None
    };

    // 6. æ„å»º CoreEngine
    let engine = builder
        .emotion(Arc::new(EmotionStub))
        .persona(Arc::new(PersonaStub))
        .config(simple_config.clone() as Arc<dyn ConfigManager>)
        .cache(Arc::new(SimpleCache))
        .telemetry(Arc::new(SimpleTelemetry))
        .with_post_processing(None, true)
        .with_tts_incremental_playback(true, 0, 50)
        .with_audio_enhancement(core_engine::tts_audio_enhancement::AudioEnhancementConfig::default())
        .with_continuous_mode(true, 5000, 200)  // å¯ç”¨è¿ç»­æ¨¡å¼ä»¥æ”¯æŒ WebSocket æµå¼å¤„ç† (max_buffer=5s, min_segment=200ms)
        .build()
        .map_err(|e| core_engine::error::EngineError::new(format!("Failed to build engine: {}", e)))?;

    // å¯åŠ¨å¼•æ“
    engine.boot().await
        .map_err(|e| core_engine::error::EngineError::new(format!("Failed to boot engine: {}", e)))?;

    // ä» engine ä¸­è·å– speaker_identifierï¼ˆå¦‚æœæ˜¯ EmbeddingBasedSpeakerIdentifierï¼‰
    // ç”±äºæ— æ³•ä» trait å¯¹è±¡ç›´æ¥è·å–å…·ä½“ç±»å‹ï¼Œæˆ‘ä»¬éœ€è¦åœ¨åˆ›å»ºæ—¶å°±ä¿å­˜å¼•ç”¨
    // è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„ identifier_arcï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    Ok((engine, speaker_identifier_ref))
}

/// å¥åº·æ£€æŸ¥ç«¯ç‚¹
async fn health_check(State(state): State<AppState>) -> Json<HealthResponse> {
    let checker = HealthChecker::new();
    let nmt_health = checker.check_nmt_service(&state.config.nmt.url).await;
    let tts_health = checker.check_tts_service(&state.config.tts.url).await;

    Json(HealthResponse {
        status: "ok".to_string(),
        services: ServiceHealth {
            nmt: nmt_health.is_healthy,
            tts: tts_health.is_healthy,
            engine: true,
        },
    })
}

/// S2S æ•´å¥ç¿»è¯‘ç«¯ç‚¹
async fn s2s_handler(
    State(state): State<AppState>,
    Json(request): Json<S2SRequest>,
) -> Result<Json<S2SResponse>, StatusCode> {
    let s2s_start = Instant::now();
    eprintln!("[S2S] ===== Request started =====");
    
    // 1. è§£ç  base64 éŸ³é¢‘
    let audio_data = general_purpose::STANDARD
        .decode(&request.audio)
        .map_err(|e| {
            eprintln!("[ERROR] Failed to decode base64 audio: {}", e);
            StatusCode::BAD_REQUEST
        })?;

    // 2. è§£æ WAV éŸ³é¢‘å¹¶è½¬æ¢ä¸º AudioFrame åˆ—è¡¨
    let audio_frames = parse_wav_to_frames(&audio_data)
        .map_err(|e| {
            eprintln!("[ERROR] Failed to parse WAV audio: {}", e);
            StatusCode::BAD_REQUEST
        })?;

    if audio_frames.is_empty() {
        return Err(StatusCode::BAD_REQUEST);
    }
    let frame_info = audio_frames.first().map(|frame| {
        format!(
            "{}Hz {}ch {} samples",
            frame.sample_rate,
            frame.channels,
            frame.data.len()
        )
    }).unwrap_or_else(|| "unknown format".into());
    eprintln!(
        "[S2S] Received audio: {} bytes -> {} frames (first frame: {}) [src={}, tgt={}]",
        audio_data.len(),
        audio_frames.len(),
        frame_info,
        request.src_lang,
        request.tgt_lang
    );

    // 3. æ ¹æ®è¯·æ±‚æ›´æ–°ç›®æ ‡è¯­è¨€é…ç½®
    state.simple_config.set_target_language(request.tgt_lang.clone()).await;
    state.simple_config.set_source_language(request.src_lang.clone()).await;
    eprintln!("[S2S] Updated language config: src={}, tgt={}", request.src_lang, request.tgt_lang);

    // 4. å¤„ç†æ‰€æœ‰éŸ³é¢‘å¸§ï¼Œç´¯ç§¯åˆ° ASR ç¼“å†²åŒº
    // å¯¹äºæ•´å¥ç¿»è¯‘ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†æ‰€æœ‰å¸§ï¼Œæœ€åä¸€å¸§åº”è¯¥è§¦å‘è¾¹ç•Œæ£€æµ‹
    let mut final_result: Option<ProcessResult> = None;
    
    // å¤„ç†æ‰€æœ‰å¸§ï¼Œé™¤äº†æœ€åä¸€å¸§
    for frame in audio_frames.iter().take(audio_frames.len().saturating_sub(1)) {
        match state.engine.process_audio_frame(frame.clone(), Some(request.src_lang.clone())).await {
            Ok(Some(result)) => {
                // è®°å½•æœ€æ–°ç»“æœï¼Œä½†ç»§ç»­å¤„ç†å‰©ä½™å¸§ï¼Œç¡®ä¿éŸ³é¢‘è¢«å®Œæ•´æ¶ˆè€—
                final_result = Some(result);
            }
            Ok(None) => {
                // ç»§ç»­å¤„ç†ä¸‹ä¸€å¸§ï¼ˆå¸§è¢«ç´¯ç§¯åˆ°ç¼“å†²åŒºï¼‰
                continue;
            }
            Err(e) => {
                eprintln!("[ERROR] Error processing audio frame: {}", e);
                return Err(StatusCode::INTERNAL_SERVER_ERROR);
            }
        }
    }
    
    // 5. å¤„ç†æœ€åä¸€å¸§ï¼Œåº”è¯¥è§¦å‘è¾¹ç•Œæ£€æµ‹å’Œå®Œæ•´æ¨ç†
    if final_result.is_none() {
        if let Some(last_frame) = audio_frames.last() {
            // åˆ›å»ºä¸€ä¸ªæ ‡è®°ä¸ºè¾¹ç•Œçš„å¸§ï¼ˆé€šè¿‡ä¿®æ”¹ timestamp æˆ–ä½¿ç”¨ç‰¹æ®Šå¤„ç†ï¼‰
            // å®é™…ä¸Šï¼ŒSimpleVad æ€»æ˜¯è¿”å› is_boundary=trueï¼Œæ‰€ä»¥æœ€åä¸€å¸§åº”è¯¥è§¦å‘æ¨ç†
            match state.engine.process_audio_frame(last_frame.clone(), Some(request.src_lang.clone())).await {
                Ok(Some(result)) => {
                    final_result = Some(result);
                }
                Ok(None) => {
                    // å¦‚æœæ²¡æœ‰ç»“æœï¼Œå¯èƒ½æ˜¯éŸ³é¢‘å¤ªçŸ­æˆ–æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³
                    // è¿”å›é”™è¯¯
                    return Err(StatusCode::BAD_REQUEST);
                }
                Err(e) => {
                    eprintln!("[ERROR] Error processing final audio frame: {}", e);
                    return Err(StatusCode::INTERNAL_SERVER_ERROR);
                }
            }
        }
    }

    let result = final_result.ok_or(StatusCode::BAD_REQUEST)?;

    // 6. æå–ç»“æœ
    let transcript = result
        .asr
        .final_transcript
        .as_ref()
        .map(|t| t.text.clone())
        .unwrap_or_default();

    let translation = result
        .translation
        .as_ref()
        .map(|t| t.translated_text.clone())
        .unwrap_or_default();

    if !transcript.trim().is_empty() {
        eprintln!("[S2S] Transcript: {}", transcript);
    } else {
        eprintln!("[S2S] Transcript: <empty>");
    }
    if !translation.trim().is_empty() {
        eprintln!("[S2S] Translation: {}", translation);
    } else {
        eprintln!("[S2S] Translation: <empty>");
    }

    // 7. è·å– TTS éŸ³é¢‘ï¼ˆbase64 ç¼–ç ï¼‰
    let audio_base64 = if let Some(tts_chunk) = result.tts {
        let audio_size = tts_chunk.audio.len();
        eprintln!("[S2S] TTS audio size: {} bytes", audio_size);
        if audio_size > 0 {
            general_purpose::STANDARD.encode(&tts_chunk.audio)
        } else {
            eprintln!("[S2S] WARNING: TTS audio is empty!");
            String::new()
        }
    } else {
        eprintln!("[S2S] WARNING: TTS result is None!");
        String::new()
    };

    // 8. è®¡ç®—æ€»æ—¶é•¿å¹¶è¿”å›ç»“æœ
    let s2s_total_ms = s2s_start.elapsed().as_millis() as u64;
    eprintln!("[S2S] ===== Request completed in {}ms =====", s2s_total_ms);
    
    // è¾“å‡ºè¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡ï¼ˆå¦‚æœä¹‹å‰è®°å½•äº†å„æ­¥éª¤æ—¶é—´ï¼‰
    // æ³¨æ„ï¼šè¿™é‡Œåªè¾“å‡ºæ€»æ—¶é•¿ï¼Œå„æ­¥éª¤çš„è¯¦ç»†æ—¶é—´éœ€è¦åœ¨ process_audio_frame ä¸­è®°å½•
    
    Ok(Json(S2SResponse {
        audio: audio_base64,
        transcript,
        translation,
    }))
}

/// è§£æ WAV éŸ³é¢‘æ•°æ®ä¸º AudioFrame åˆ—è¡¨
fn parse_wav_to_frames(wav_data: &[u8]) -> anyhow::Result<Vec<AudioFrame>> {
    use hound::WavReader;
    
    let cursor = Cursor::new(wav_data);
    let mut reader = WavReader::new(cursor)
        .map_err(|e| anyhow::anyhow!("Failed to create WAV reader: {}", e))?;
    
    let spec = reader.spec();
    
    // è¯»å–æ‰€æœ‰æ ·æœ¬
    let mut samples = Vec::new();
    match spec.sample_format {
        hound::SampleFormat::Float => {
            for sample in reader.samples::<f32>() {
                samples.push(sample?);
            }
        }
        hound::SampleFormat::Int => {
            let max_val = (1i32 << (spec.bits_per_sample - 1)) as f32;
            for sample in reader.samples::<i32>() {
                samples.push(sample? as f32 / max_val);
            }
        }
    }

    // å¦‚æœéŸ³é¢‘æ˜¯ç«‹ä½“å£°ï¼Œè½¬æ¢ä¸ºå•å£°é“ï¼ˆå–å¹³å‡å€¼ï¼‰
    let mono_samples = if spec.channels == 2 {
        samples
            .chunks(2)
            .map(|chunk| (chunk[0] + chunk[1]) / 2.0)
            .collect()
    } else {
        samples
    };

    // å¦‚æœé‡‡æ ·ç‡ä¸æ˜¯ 16kHzï¼Œéœ€è¦é‡é‡‡æ ·
    // ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œå‡è®¾è¾“å…¥éŸ³é¢‘å·²ç»æ˜¯ 16kHz
    // å®é™…åº”ç”¨ä¸­åº”è¯¥æ·»åŠ é‡é‡‡æ ·é€»è¾‘
    
    // æŒ‰ 10ms ä¸€å¸§æ‹†åˆ†ï¼ˆWhisper æœŸæœ›çš„æ ¼å¼ï¼‰
    let frame_size = (spec.sample_rate / 100) as usize;
    let mut frames = Vec::new();
    
    for (idx, chunk) in mono_samples.chunks(frame_size).enumerate() {
        frames.push(AudioFrame {
            sample_rate: spec.sample_rate,
            channels: 1, // è½¬æ¢ä¸ºå•å£°é“
            data: chunk.to_vec(),
            timestamp_ms: (idx * 10) as u64,
        });
    }

    if let Some(last) = frames.last_mut() {
        last.timestamp_ms |= FINAL_FRAME_FLAG;
    }

    Ok(frames)
}

/// WebSocket æµå¼ç¿»è¯‘ç«¯ç‚¹
async fn stream_handler(
    ws: WebSocketUpgrade,
    State(state): State<AppState>,
) -> Response {
    ws.on_upgrade(move |socket| async move {
        handle_socket(socket, state).await;
    })
}

async fn handle_socket(socket: WebSocket, state: AppState) {
    eprintln!("[WebSocket] âœ… Client connected");

    // åˆ†ç¦» WebSocket çš„å‘é€ç«¯å’Œæ¥æ”¶ç«¯
    let (sender, mut receiver) = socket.split();
    
    // ä½¿ç”¨ Arc<Mutex<>> åŒ…è£… senderï¼Œä»¥ä¾¿åœ¨å¤šä¸ªä»»åŠ¡ä¸­å…±äº«
    let sender = Arc::new(tokio::sync::Mutex::new(sender));
    
    let mut src_lang = "en".to_string(); // é»˜è®¤æºè¯­è¨€
    let mut tgt_lang = "zh".to_string(); // é»˜è®¤ç›®æ ‡è¯­è¨€
    let mut frame_count = 0u64;
    
    // è®¢é˜… TTS äº‹ä»¶ï¼Œç”¨äºæ¥æ”¶å¢é‡éŸ³é¢‘è¾“å‡º
    let mut tts_receiver_from_bus = state.event_bus.subscribe_receiver(EventTopic("Tts".to_string()));
    eprintln!("[WebSocket] ğŸ“¡ Subscribed to TTS events");
    
    // å¯åŠ¨ä»»åŠ¡ï¼šä»äº‹ä»¶æ€»çº¿æ¥æ”¶ TTS äº‹ä»¶ï¼ŒæŒ‰ timestamp_ms æ’åºåå‘é€åˆ° WebSocket
    let sender_for_tts = Arc::clone(&sender);
    tokio::spawn(async move {
        let mut pending_events: Vec<CoreEvent> = Vec::new();
        let mut next_expected_timestamp = 0u64;
        
        while let Some(event) = tts_receiver_from_bus.recv().await {
            pending_events.push(event);
            
            // æŒ‰ timestamp_ms æ’åº
            pending_events.sort_by_key(|e| e.timestamp_ms);
            
            // å‘é€æ‰€æœ‰å¯ä»¥å‘é€çš„äº‹ä»¶ï¼ˆæŒ‰é¡ºåºï¼‰
            while let Some(pos) = pending_events.iter().position(|e| e.timestamp_ms >= next_expected_timestamp) {
                let event = pending_events.remove(pos);
                next_expected_timestamp = event.timestamp_ms + 1;  // æ›´æ–°æœŸæœ›çš„æ—¶é—´æˆ³
                
                // è§£æäº‹ä»¶ payload
                if let Some(audio_base64) = event.payload.get("audio").and_then(|v| v.as_str()) {
                    let response_json = serde_json::json!({
                        "type": "tts_chunk",
                        "audio": audio_base64,
                        "timestamp_ms": event.timestamp_ms,
                        "is_last": event.payload.get("is_last").and_then(|v| v.as_bool()).unwrap_or(false),
                    });
                    
                    let mut sender_guard = sender_for_tts.lock().await;
                    if let Err(e) = sender_guard.send(Message::Text(response_json.to_string())).await {
                        eprintln!("[WebSocket] âŒ Failed to send TTS event: {}", e);
                        return;
                    }
                    drop(sender_guard); // æ˜¾å¼é‡Šæ”¾é”
                    
                    eprintln!("[WebSocket] ğŸ“¤ Sent TTS chunk (timestamp: {}ms, is_last: {}, audio_size: {} chars)", 
                        event.timestamp_ms,
                        event.payload.get("is_last").and_then(|v| v.as_bool()).unwrap_or(false),
                        audio_base64.len());
                }
            }
        }
    });

    while let Some(msg) = receiver.next().await {
        let msg = match msg {
            Ok(msg) => msg,
                            Err(e) => {
                eprintln!("[WebSocket] âŒ Error receiving message: {}", e);
                return;
            }
        };

        match msg {
            Message::Text(text) => {
                // å°è¯•è§£æä¸º JSONï¼ˆé…ç½®æˆ–éŸ³é¢‘å¸§ï¼‰
                if let Ok(json_msg) = serde_json::from_str::<serde_json::Value>(&text) {
                    if json_msg["type"] == "config" {
                        // å¤„ç†é…ç½®æ¶ˆæ¯
                        if let Some(lang) = json_msg["src_lang"].as_str() {
                            src_lang = lang.to_string();
                        }
                        if let Some(lang) = json_msg["tgt_lang"].as_str() {
                            tgt_lang = lang.to_string();
                        }
                        state.simple_config.set_source_language(src_lang.clone()).await;
                        state.simple_config.set_target_language(tgt_lang.clone()).await;
                        eprintln!("[WebSocket] âš™ï¸ Config updated: src={}, tgt={}", src_lang, tgt_lang);
                    } else if json_msg["type"] == "audio_frame" {
                        // å¤„ç†éŸ³é¢‘å¸§
                        if let (Some(base64_audio), Some(timestamp_ms), Some(sample_rate), Some(channels)) = (
                            json_msg["data"].as_str(),
                            json_msg["timestamp_ms"].as_u64(),
                            json_msg["sample_rate"].as_u64(),
                            json_msg["channels"].as_u64(),
                        ) {
                            frame_count += 1;
                            
                            // è§£ç  base64 éŸ³é¢‘æ•°æ®
                            let audio_data = match general_purpose::STANDARD.decode(base64_audio) {
                                Ok(data) => data,
                    Err(e) => {
                                    eprintln!("[WebSocket] âŒ Failed to decode base64 audio (frame #{}): {}", frame_count, e);
                                    continue;
                                }
                            };

                            // å°† 16-bit PCM è½¬æ¢ä¸º f32
                            let pcm_data: Vec<i16> = audio_data
                    .chunks_exact(2)
                                .map(|chunk| i16::from_le_bytes([chunk[0], chunk[1]]))
                    .collect();
                            let float_data: Vec<f32> = pcm_data.into_iter().map(|s| s as f32 / 32768.0).collect();
                
                // è®¡ç®—éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯
                            let max_amplitude = float_data.iter().fold(0.0f32, |a, &b| a.max(b.abs()));
                            let rms = (float_data.iter().map(|x| x * x).sum::<f32>() / float_data.len() as f32).sqrt();

                            let audio_frame = AudioFrame {
                                sample_rate: sample_rate as u32,
                                channels: channels as u8,
                                data: float_data,
                                timestamp_ms,
                            };

                            // æ¯ 50 å¸§è¾“å‡ºä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                            if frame_count % 50 == 0 {
                                eprintln!("[WebSocket] ğŸ“¥ Received audio frame #{}: {}Hz {}ch, {} samples ({}ms), max={:.4}, rms={:.4}", 
                                    frame_count, sample_rate, channels, audio_frame.data.len(), 
                                    timestamp_ms, max_amplitude, rms);
                            }

                            // å¤„ç†éŸ³é¢‘å¸§ï¼ˆå¦‚æœå¯ç”¨äº†è¿ç»­æ¨¡å¼ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨è¿ç»­å¤„ç†é€»è¾‘ï¼‰
                            match state.engine.process_audio_frame(audio_frame, Some(src_lang.clone())).await {
                    Ok(Some(result)) => {
                                    // å‘é€ ASR è½¬å½•ã€NMT ç¿»è¯‘å’Œ TTS éŸ³é¢‘
                                    let tts_audio_base64 = result.tts.as_ref().and_then(|t| {
                                        if t.audio.is_empty() {
                                            eprintln!("[WebSocket] âš ï¸ TTS audio is empty!");
                                            None
                                        } else {
                                            eprintln!("[WebSocket] ğŸ“¤ Sending TTS audio: {} bytes (base64: {} chars)", 
                                                t.audio.len(), 
                                                general_purpose::STANDARD.encode(&t.audio).len());
                                            Some(general_purpose::STANDARD.encode(&t.audio))
                                        }
                                    });
                                    
                                    let response_json = serde_json::json!({
                                        "transcript": result.asr.final_transcript.as_ref().map(|t| t.text.clone()),
                                        "translation": result.translation.as_ref().map(|t| t.translated_text.clone()),
                                        "audio": tts_audio_base64,
                                    });
                                    
                                    eprintln!("[WebSocket] ğŸ“¤ Sending response: transcript={:?}, translation={:?}, audio={}", 
                                        result.asr.final_transcript.as_ref().map(|t| t.text.as_str()),
                                        result.translation.as_ref().map(|t| t.translated_text.as_str()),
                                        if tts_audio_base64.is_some() { "Yes" } else { "No" });
                                    
                                    let mut sender_guard = sender.lock().await;
                                    if let Err(e) = sender_guard.send(Message::Text(response_json.to_string())).await {
                                        eprintln!("[WebSocket] âŒ Failed to send response: {}", e);
                                        drop(sender_guard);
                                        break;
                                    }
                                    drop(sender_guard); // æ˜¾å¼é‡Šæ”¾é”
                                    }
                                    Ok(None) => {
                                    // æ²¡æœ‰æœ€ç»ˆç»“æœï¼Œç»§ç»­å¤„ç†
                                    eprintln!("[WebSocket] â³ å¤„ç†ä¸­ï¼Œæš‚æ— æœ€ç»ˆç»“æœ");
                                    }
                                    Err(e) => {
                                    eprintln!("[WebSocket] âŒ Error processing audio frame #{}: {}", frame_count, e);
                                }
                            }
                        } else {
                            eprintln!("[WebSocket] âš ï¸ Invalid audio_frame message format (frame #{})", frame_count);
                        }
                    } else {
                        eprintln!("[WebSocket] âš ï¸ Unknown message type: {}", json_msg["type"]);
                    }
                } else {
                    eprintln!("[WebSocket] âš ï¸ Failed to parse JSON message");
                }
            }
            Message::Binary(data) => {
                eprintln!("[WebSocket] ğŸ“¦ Received binary message: {} bytes", data.len());
            }
            Message::Ping(payload) => {
                let mut sender_guard = sender.lock().await;
                if let Err(e) = sender_guard.send(Message::Pong(payload)).await {
                    eprintln!("[WebSocket] âŒ Failed to send Pong: {}", e);
                    drop(sender_guard);
                    break;
                }
                drop(sender_guard); // æ˜¾å¼é‡Šæ”¾é”
            }
            Message::Pong(_) => {
                // ä¸åšå¤„ç†
            }
            Message::Close(close_frame) => {
                eprintln!("[WebSocket] ğŸ”Œ Client disconnected (frames received: {})", frame_count);
                if let Some(frame) = close_frame {
                    eprintln!("[WebSocket] Close frame: code={:?}, reason={:?}", frame.code, frame.reason);
                }
                break;
            }
        }
    }
    eprintln!("[WebSocket] ğŸ‘‹ Connection closed (total frames: {})", frame_count);
}

/// è·å–å½“å‰è¯´è¯è€…è¯†åˆ«æ¨¡å¼
#[derive(Debug, Serialize)]
struct SpeakerModeResponse {
    mode: String,  // "single_user" æˆ– "multi_user"
}

/// è®¾ç½®è¯´è¯è€…è¯†åˆ«æ¨¡å¼è¯·æ±‚
#[derive(Debug, Deserialize)]
struct SetSpeakerModeRequest {
    mode: String,  // "single_user" æˆ– "multi_user"
}

/// è®¾ç½®è¯´è¯è€…è¯†åˆ«æ¨¡å¼å“åº”
#[derive(Debug, Serialize)]
struct SetSpeakerModeResponse {
    success: bool,
    message: String,
    current_mode: String,
}

/// è·å–å½“å‰è¯´è¯è€…è¯†åˆ«æ¨¡å¼
async fn get_speaker_mode(State(state): State<AppState>) -> Json<SpeakerModeResponse> {
    let mode = state.speaker_mode.read().await;
    let mode_str = match *mode {
        EmbeddingBasedMode::SingleUser => "single_user",
        EmbeddingBasedMode::MultiUser => "multi_user",
    };
    Json(SpeakerModeResponse {
        mode: mode_str.to_string(),
    })
}

/// è®¾ç½®è¯´è¯è€…è¯†åˆ«æ¨¡å¼
async fn set_speaker_mode(
    State(state): State<AppState>,
    Json(request): Json<SetSpeakerModeRequest>,
) -> Result<Json<SetSpeakerModeResponse>, StatusCode> {
    let new_mode = match request.mode.as_str() {
        "single_user" => EmbeddingBasedMode::SingleUser,
        "multi_user" => EmbeddingBasedMode::MultiUser,
        _ => {
            return Ok(Json(SetSpeakerModeResponse {
                success: false,
                message: format!("æ— æ•ˆçš„æ¨¡å¼: {}. æœ‰æ•ˆå€¼: single_user, multi_user", request.mode),
                current_mode: {
                    let current = state.speaker_mode.read().await;
                    match *current {
                        EmbeddingBasedMode::SingleUser => "single_user".to_string(),
                        EmbeddingBasedMode::MultiUser => "multi_user".to_string(),
                    }
                },
            }));
        }
    };
    
    {
        let mut mode = state.speaker_mode.write().await;
        *mode = new_mode;
    }
    
    let mode_str = match new_mode {
        EmbeddingBasedMode::SingleUser => "single_user",
        EmbeddingBasedMode::MultiUser => "multi_user",
    };
    
    // å¦‚æœå­˜åœ¨ speaker_identifierï¼Œç›´æ¥è°ƒç”¨å…¶ set_mode æ–¹æ³•ï¼ˆåŠ¨æ€åˆ‡æ¢ï¼Œæ•°æ®ä¿ç•™ï¼‰
    if let Some(ref identifier) = state.speaker_identifier {
        identifier.set_mode(new_mode).await;
        eprintln!("[CONFIG] è¯´è¯è€…è¯†åˆ«æ¨¡å¼å·²åŠ¨æ€æ›´æ–°ä¸º: {} (æ•°æ®å·²ä¿ç•™ï¼Œä¸ä¼šæ¸…ç©ºå¦ä¸€ç§æ¨¡å¼çš„è®°å½•)", mode_str);
    } else {
        eprintln!("[CONFIG] è¯´è¯è€…è¯†åˆ«æ¨¡å¼å·²æ›´æ–°ä¸º: {} (ä½†æœªæ‰¾åˆ° identifierï¼Œå¯èƒ½éœ€è¦é‡å¯å¼•æ“)", mode_str);
    }
    
    Ok(Json(SetSpeakerModeResponse {
        success: true,
        message: format!("æ¨¡å¼å·²æ›´æ–°ä¸º: {}. æ•°æ®å·²ä¿ç•™ï¼Œåˆ‡æ¢æ¨¡å¼ä¸ä¼šæ¸…ç©ºå¦ä¸€ç§æ¨¡å¼çš„è®°å½•", mode_str),
        current_mode: mode_str.to_string(),
    }))
}
