# ASR 上下文缓存功能

## 概述

在连续输入模式下，VAD 会将用户的表达切分成多个短语，导致每个短语之间缺乏上下文参考，从而降低识别准确度。为了解决这个问题，实现了上下文缓存功能：在识别当前短语时，将前 2 句短语的文本作为上下文参考传递给 Whisper。

## 实现方案

### 1. 上下文缓存

在 `WhisperAsrStreaming` 中维护一个上下文缓存：

```rust
context_cache: Arc<Mutex<Vec<String>>>,  // 上下文缓存（最多保留 2 句）
```

### 2. 上下文传递

在 `infer_on_boundary()` 中：
1. 从上下文缓存中获取前 2 句的文本
2. 将上下文文本作为 `context_prompt` 传递给 `transcribe_full_with_segments()`
3. 识别完成后，将当前句子的文本添加到上下文缓存中

### 3. 缓存更新

- 每次识别完成后，将识别结果添加到缓存
- 只保留最近 2 句（超过 2 句时，删除最旧的）
- 空文本不会被添加到缓存

## 工作流程

```
短句1：
├─ 识别（无上下文）
└─ 更新缓存：["短句1"]

短句2：
├─ 获取上下文：["短句1"]
├─ 识别（使用上下文："短句1"）
└─ 更新缓存：["短句1", "短句2"]

短句3：
├─ 获取上下文：["短句1", "短句2"]
├─ 识别（使用上下文："短句1 短句2"）
└─ 更新缓存：["短句2", "短句3"]（删除最旧的）

短句4：
├─ 获取上下文：["短句2", "短句3"]
├─ 识别（使用上下文："短句2 短句3"）
└─ 更新缓存：["短句3", "短句4"]
```

## 上下文利用方式

目前上下文缓存已经实现，上下文信息可以通过以下方式利用：

### 1. **直接传递给模型（如果库支持）**
如果 `whisper_rs` 支持 `initial_prompt`，可以在 `transcribe_full_with_segments()` 中启用：
```rust
if let Some(prompt) = context_prompt {
    params.set_initial_prompt(Some(prompt));  // 如果 API 支持
}
```

### 2. **后处理使用**
即使不能直接传递给模型，上下文缓存仍然有价值：
- **语言模型后处理**：使用上下文信息对识别结果进行纠错
- **语义理解**：结合上下文理解完整语义
- **日志和调试**：查看上下文帮助调试识别问题

### 3. **未来扩展**
上下文缓存为将来的功能扩展打下了基础：
- 如果库升级支持 `initial_prompt`，可以立即启用
- 可以用于其他后处理任务
- 可以用于上下文相关的功能（如对话理解）

## 优势

1. **提高识别准确度**：通过提供上下文，帮助 Whisper 更好地理解语境
2. **自动管理**：缓存自动更新，无需手动管理
3. **内存占用小**：只保留最近 2 句，内存占用很小

## 注意事项

1. **上下文长度**：上下文文本不应过长，避免影响识别性能
2. **语境变化**：如果前后句语境差异很大，上下文可能引入噪声
3. **API 支持**：需要确认 `whisper_rs` 是否支持 `initial_prompt`

