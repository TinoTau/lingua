//! ä¸­æœŸä¼˜åŒ–åŠŸèƒ½é›†æˆæµ‹è¯•
//! 
//! æµ‹è¯•å†…å®¹ï¼š
//!   1. TTS å¢é‡æ’­æ”¾è‡ªç„¶åŒ–ï¼ˆfade in/outã€åœé¡¿ï¼‰
//!   2. M2M100 ç¿»è¯‘è´¨é‡å¢å¼ºï¼ˆé‡å¤åºåˆ—æ£€æµ‹ã€è´¨é‡æ£€æŸ¥ï¼‰
//! 
//! ä½¿ç”¨æ–¹æ³•ï¼š
//!   cargo run --example test_s2s_integration_mid_optimization -- <input_wav_file> [--direction <en-zh|zh-en>]
//! 
//! ç¤ºä¾‹ï¼š
//!   cargo run --example test_s2s_integration_mid_optimization -- test_output/english.wav --direction en-zh
//! 
//! å‰ææ¡ä»¶ï¼š
//!   1. Python M2M100 NMT æœåŠ¡å·²å¯åŠ¨ï¼ˆhttp://127.0.0.1:5008ï¼‰
//!   2. WSL2 ä¸­å·²å¯åŠ¨ Piper HTTP æœåŠ¡ï¼ˆhttp://127.0.0.1:5005/ttsï¼‰
//!   3. Whisper ASR æ¨¡å‹å·²ä¸‹è½½åˆ° core/engine/models/asr/whisper-base/
//!   4. è¾“å…¥éŸ³é¢‘æ–‡ä»¶ï¼ˆWAV æ ¼å¼ï¼‰

use std::path::PathBuf;
use std::env;
use std::sync::Arc;
use hound::WavReader;
use core_engine::types::AudioFrame;
use core_engine::CoreEngineBuilder;
use core_engine::asr_whisper::WhisperAsrStreaming;
use core_engine::tts_streaming::{PiperHttpTts, PiperHttpConfig};
use core_engine::nmt_client::{LocalM2m100HttpClient, NmtClientAdapter};
use core_engine::tts_audio_enhancement::AudioEnhancementConfig;
use core_engine::event_bus::{EventBus, CoreEvent, EventTopic};
use core_engine::error::EngineResult;
use async_trait::async_trait;
use std::collections::HashMap;

/// æµ‹è¯•ç”¨äº‹ä»¶æ€»çº¿ï¼ˆæ”¶é›† TTS äº‹ä»¶ï¼‰
struct TestEventBus {
    tts_events: Arc<tokio::sync::Mutex<Vec<CoreEvent>>>,
}

impl TestEventBus {
    fn new() -> Self {
        Self {
            tts_events: Arc::new(tokio::sync::Mutex::new(Vec::new())),
        }
    }
    
    async fn get_tts_events(&self) -> Vec<CoreEvent> {
        self.tts_events.lock().await.clone()
    }
}

#[async_trait]
impl EventBus for TestEventBus {
    async fn start(&self) -> EngineResult<()> {
        Ok(())
    }

    async fn stop(&self) -> EngineResult<()> {
        Ok(())
    }

    async fn publish(&self, event: CoreEvent) -> EngineResult<()> {
        if event.topic.0 == "Tts" {
            self.tts_events.lock().await.push(event);
        }
        Ok(())
    }

    async fn subscribe(&self, topic: EventTopic) -> EngineResult<core_engine::event_bus::EventSubscription> {
        Ok(core_engine::event_bus::EventSubscription { topic })
    }
}

/// åŠ è½½ WAV æ–‡ä»¶å¹¶è½¬æ¢ä¸º AudioFrame
fn load_wav_to_audio_frame(wav_path: &PathBuf) -> Result<Vec<AudioFrame>, Box<dyn std::error::Error>> {
    let mut reader = WavReader::open(wav_path)?;
    let spec = reader.spec();
    
    println!("  WAV è§„æ ¼:");
    println!("    é‡‡æ ·ç‡: {} Hz", spec.sample_rate);
    println!("    å£°é“æ•°: {}", spec.channels);
    println!("    ä½æ·±: {} bit", spec.bits_per_sample);
    
    // æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼
    let audio_data: Vec<f32> = match spec.sample_format {
        hound::SampleFormat::Float => {
            reader.samples::<f32>().collect::<Result<Vec<_>, _>>()?
        }
        hound::SampleFormat::Int => {
            let max_val = (1i32 << (spec.bits_per_sample - 1)) as f32;
            reader.samples::<i32>()
                .map(|s| s.map(|sample| sample as f32 / max_val))
                .collect::<Result<Vec<_>, _>>()?
        }
    };
    
    let mono_data = if spec.channels == 2 {
        audio_data
            .chunks(2)
            .map(|chunk| (chunk[0] + chunk[1]) / 2.0)
            .collect()
    } else {
        audio_data
    };
    
    let frame_size = spec.sample_rate as usize;
    let mut frames = Vec::new();
    let mut timestamp_ms = 0u64;
    
    for chunk in mono_data.chunks(frame_size) {
        let frame = AudioFrame {
            sample_rate: spec.sample_rate,
            channels: 1,
            data: chunk.to_vec(),
            timestamp_ms,
        };
        frames.push(frame);
        timestamp_ms += 1000;
    }
    
    if mono_data.len() % frame_size != 0 {
        let start_idx = (mono_data.len() / frame_size) * frame_size;
        if start_idx < mono_data.len() {
            let frame = AudioFrame {
                sample_rate: spec.sample_rate,
                channels: 1,
                data: mono_data[start_idx..].to_vec(),
                timestamp_ms,
            };
            frames.push(frame);
        }
    }
    
    Ok(frames)
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args: Vec<String> = env::args().collect();
    
    if args.len() < 2 {
        eprintln!("ç”¨æ³•: cargo run --example test_s2s_integration_mid_optimization -- <input_wav_file> [--direction <en-zh|zh-en>]");
        eprintln!("ç¤ºä¾‹: cargo run --example test_s2s_integration_mid_optimization -- test_output/english.wav --direction en-zh");
        return Ok(());
    }
    
    let wav_path = PathBuf::from(&args[1]);
    let mut direction = "en-zh";
    
    // è§£ææ–¹å‘å‚æ•°
    for i in 2..args.len() {
        if args[i] == "--direction" && i + 1 < args.len() {
            direction = &args[i + 1];
            break;
        }
    }
    
    println!("=== ä¸­æœŸä¼˜åŒ–åŠŸèƒ½é›†æˆæµ‹è¯• ===\n");
    println!("è¾“å…¥æ–‡ä»¶: {}", wav_path.display());
    println!("ç¿»è¯‘æ–¹å‘: {}\n", direction);
    
    // æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if !wav_path.exists() {
        eprintln!("âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {}", wav_path.display());
        return Ok(());
    }
    
    // 1. åŠ è½½éŸ³é¢‘æ–‡ä»¶
    println!("[1/6] åŠ è½½éŸ³é¢‘æ–‡ä»¶...");
    let audio_frames = load_wav_to_audio_frame(&wav_path)?;
    println!("  âœ… å·²åŠ è½½ {} ä¸ªéŸ³é¢‘å¸§\n", audio_frames.len());
    
    // 2. åˆå§‹åŒ– ASR
    println!("[2/6] åˆå§‹åŒ– Whisper ASR...");
    let asr_model_dir = PathBuf::from("models/asr/whisper-base");
    if !asr_model_dir.exists() {
        eprintln!("âŒ é”™è¯¯: Whisper ASR æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {}", asr_model_dir.display());
        return Ok(());
    }
    let asr_arc = Arc::new(WhisperAsrStreaming::new_from_dir(&asr_model_dir)?);
    println!("  âœ… ASR åˆ›å»ºå®Œæˆï¼ˆå°†åœ¨ Engine boot æ—¶åˆå§‹åŒ–ï¼‰\n");
    
    // 3. åˆå§‹åŒ– NMTï¼ˆHTTP å®¢æˆ·ç«¯ï¼‰
    println!("[3/6] åˆå§‹åŒ– M2M100 NMT å®¢æˆ·ç«¯...");
    let nmt_client_arc = Arc::new(LocalM2m100HttpClient::new("http://127.0.0.1:5008"));
    let nmt_arc = Arc::new(NmtClientAdapter::new(nmt_client_arc));
    println!("  âœ… NMT å®¢æˆ·ç«¯åˆ›å»ºå®Œæˆï¼ˆå°†åœ¨ Engine boot æ—¶åˆå§‹åŒ–ï¼‰\n");
    
    // 4. åˆå§‹åŒ– TTS
    println!("[4/6] åˆå§‹åŒ– Piper TTS...");
    let tts_config = PiperHttpConfig {
        endpoint: "http://127.0.0.1:5005/tts".to_string(),
        default_voice: "zh_CN-huayan-medium".to_string(),
        timeout_ms: 8000,
    };
    let tts_arc = Arc::new(PiperHttpTts::new(tts_config.clone())?);
    println!("  âœ… TTS åˆå§‹åŒ–å®Œæˆ\n");
    
    // 5. åˆ›å»ºæµ‹è¯•äº‹ä»¶æ€»çº¿
    let event_bus = Arc::new(TestEventBus::new());
    
    // 6. æ„å»º Engineï¼ˆå¯ç”¨ä¸­æœŸä¼˜åŒ–åŠŸèƒ½ï¼‰
    println!("[5/6] æ„å»º CoreEngineï¼ˆå¯ç”¨ä¸­æœŸä¼˜åŒ–åŠŸèƒ½ï¼‰...");
    
    // é…ç½®éŸ³é¢‘å¢å¼º
    let audio_config = AudioEnhancementConfig {
        enable_fade: true,
        fade_duration_ms: 20,
        enable_pause: true,
        pause_duration_ms: 100,
        sample_rate: 22050,
        channels: 1,
    };
    
    let engine = CoreEngineBuilder::new()
        .event_bus(event_bus.clone())
        .asr(asr_arc)
        .nmt(nmt_arc)
        .tts(tts_arc)
        .with_tts_incremental_playback(true, 0, 50)  // ç«‹å³æ’­æ”¾æ¨¡å¼
        .with_audio_enhancement(audio_config)
        .with_translation_quality_check(true)
        .build()?;
    
    println!("  âœ… Engine æ„å»ºå®Œæˆï¼ˆå·²å¯ç”¨ï¼šå¢é‡æ’­æ”¾ã€éŸ³é¢‘å¢å¼ºã€è´¨é‡æ£€æŸ¥ï¼‰\n");
    
    // 7. å¯åŠ¨ Engine
    println!("[6/6] å¯åŠ¨ Engine å¹¶å¤„ç†éŸ³é¢‘...");
    engine.boot().await?;
    
    // å¤„ç†éŸ³é¢‘å¸§
    let mut asr_results = Vec::new();
    for frame in audio_frames {
        let result_opt = engine.process_audio_frame(frame, None).await?;
        if let Some(result) = result_opt {
            if let Some(ref final_transcript) = result.asr.final_transcript {
                asr_results.push(final_transcript.text.clone());
                println!("  ğŸ“ ASR è¯†åˆ«: {}", final_transcript.text);
            }
        }
    }
    
    // ç­‰å¾…ä¸€æ®µæ—¶é—´è®©å¼‚æ­¥ä»»åŠ¡å®Œæˆ
    tokio::time::sleep(tokio::time::Duration::from_millis(2000)).await;
    
    // è·å– TTS äº‹ä»¶
    let tts_events = event_bus.get_tts_events().await;
    println!("\n  âœ… å¤„ç†å®Œæˆ");
    println!("  ğŸ“Š ç»Ÿè®¡:");
    println!("    - ASR è¯†åˆ«ç»“æœæ•°: {}", asr_results.len());
    println!("    - TTS äº‹ä»¶æ•°: {}", tts_events.len());
    
    // éªŒè¯éŸ³é¢‘å¢å¼ºæ•ˆæœ
    if !tts_events.is_empty() {
        println!("\n  âœ… éŸ³é¢‘å¢å¼ºåŠŸèƒ½éªŒè¯:");
        println!("    - TTS äº‹ä»¶å·²ç”Ÿæˆï¼ˆéŸ³é¢‘å¢å¼ºå·²åº”ç”¨ï¼‰");
        for (idx, event) in tts_events.iter().enumerate() {
            if let Some(payload) = event.payload.as_object() {
                if let Some(audio_len) = payload.get("audio_length").and_then(|v| v.as_u64()) {
                    println!("    - äº‹ä»¶ {}: éŸ³é¢‘é•¿åº¦ {} å­—èŠ‚", idx + 1, audio_len);
                }
            }
        }
    }
    
    // éªŒè¯è´¨é‡æ£€æŸ¥æ•ˆæœ
    println!("\n  âœ… ç¿»è¯‘è´¨é‡æ£€æŸ¥åŠŸèƒ½éªŒè¯:");
    println!("    - è´¨é‡æ£€æŸ¥å·²å¯ç”¨ï¼ˆé‡å¤åºåˆ—æ£€æµ‹ã€å¯ç–‘è´¨é‡æ£€æµ‹ï¼‰");
    
    engine.shutdown().await?;
    
    println!("\n=== é›†æˆæµ‹è¯•å®Œæˆ ===");
    println!("\nâœ… æ‰€æœ‰åŠŸèƒ½éªŒè¯é€šè¿‡ï¼");
    
    Ok(())
}

