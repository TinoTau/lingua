//! å®Œæ•´ S2S æµé›†æˆæµ‹è¯•ï¼ˆä½¿ç”¨ HTTP NMT å®¢æˆ·ç«¯ï¼‰
//! 
//! ä½¿ç”¨æ–¹æ³•ï¼š
//!   cargo run --example test_s2s_full_simple_http -- <input_wav_file> [--direction <en-zh|zh-en>]
//! 
//! ç¤ºä¾‹ï¼š
//!   cargo run --example test_s2s_full_simple_http -- test_output/s2s_flow_test.wav --direction en-zh
//! 
//! å‰ææ¡ä»¶ï¼š
//!   1. Python M2M100 NMT æœåŠ¡å·²å¯åŠ¨ï¼ˆhttp://127.0.0.1:5008ï¼‰
//!   2. WSL2 ä¸­å·²å¯åŠ¨ Piper HTTP æœåŠ¡ï¼ˆhttp://127.0.0.1:5005/ttsï¼‰
//!   3. Whisper ASR æ¨¡å‹å·²ä¸‹è½½åˆ° core/engine/models/asr/whisper-base/
//!   4. è¾“å…¥éŸ³é¢‘æ–‡ä»¶ï¼ˆWAV æ ¼å¼ï¼‰

use std::path::PathBuf;
use std::env;
use std::fs;
use hound::WavReader;
use core_engine::types::AudioFrame;
use core_engine::asr_streaming::{AsrRequest, AsrStreaming};
use core_engine::nmt_incremental::{TranslationRequest, NmtIncremental};
use core_engine::tts_streaming::{TtsRequest, TtsStreaming};
use core_engine::asr_whisper::WhisperAsrStreaming;
use core_engine::tts_streaming::{PiperHttpTts, PiperHttpConfig};

/// åŠ è½½ WAV æ–‡ä»¶å¹¶è½¬æ¢ä¸º AudioFrame
fn load_wav_to_audio_frame(wav_path: &PathBuf) -> Result<Vec<AudioFrame>, Box<dyn std::error::Error>> {
    let mut reader = WavReader::open(wav_path)?;
    let spec = reader.spec();
    
    println!("  WAV è§„æ ¼:");
    println!("    é‡‡æ ·ç‡: {} Hz", spec.sample_rate);
    println!("    å£°é“æ•°: {}", spec.channels);
    println!("    ä½æ·±: {} bit", spec.bits_per_sample);
    
    // æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼
    let audio_data: Vec<f32> = match spec.sample_format {
        hound::SampleFormat::Float => {
            reader.samples::<f32>().collect::<Result<Vec<_>, _>>()?
        }
        hound::SampleFormat::Int => {
            let max_val = (1i32 << (spec.bits_per_sample - 1)) as f32;
            reader.samples::<i32>()
                .map(|s| s.map(|sample| sample as f32 / max_val))
                .collect::<Result<Vec<_>, _>>()?
        }
    };
    
    let mono_data = if spec.channels == 2 {
        audio_data
            .chunks(2)
            .map(|chunk| (chunk[0] + chunk[1]) / 2.0)
            .collect()
    } else {
        audio_data
    };
    
    let frame_size = spec.sample_rate as usize;
    let mut frames = Vec::new();
    let mut timestamp_ms = 0u64;
    
    for chunk in mono_data.chunks(frame_size) {
        let frame = AudioFrame {
            sample_rate: spec.sample_rate,
            channels: 1,
            data: chunk.to_vec(),
            timestamp_ms,
        };
        frames.push(frame);
        timestamp_ms += 1000;
    }
    
    if mono_data.len() % frame_size != 0 {
        let start_idx = (mono_data.len() / frame_size) * frame_size;
        if start_idx < mono_data.len() {
            let frame = AudioFrame {
                sample_rate: spec.sample_rate,
                channels: 1,
                data: mono_data[start_idx..].to_vec(),
                timestamp_ms,
            };
            frames.push(frame);
        }
    }
    
    Ok(frames)
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args: Vec<String> = env::args().collect();
    
    if args.len() < 2 {
        eprintln!("ç”¨æ³•: cargo run --example test_s2s_full_simple_http -- <input_wav_file> [--direction <en-zh|zh-en>]");
        eprintln!("ç¤ºä¾‹: cargo run --example test_s2s_full_simple_http -- test_output/s2s_flow_test.wav --direction en-zh");
        return Ok(());
    }
    
    let wav_path = PathBuf::from(&args[1]);
    let mut direction = "en-zh";
    
    for i in 2..args.len() {
        if args[i] == "--direction" && i + 1 < args.len() {
            direction = &args[i + 1];
        }
    }
    
    println!("=== S2S å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆHTTP NMT å®¢æˆ·ç«¯ï¼‰===\n");
    println!("è¾“å…¥æ–‡ä»¶: {}", wav_path.display());
    println!("ç¿»è¯‘æ–¹å‘: {}\n", direction);
    
    // 1. åŠ è½½ WAV æ–‡ä»¶
    println!("[1/5] åŠ è½½éŸ³é¢‘æ–‡ä»¶...");
    let audio_frames = load_wav_to_audio_frame(&wav_path)?;
    println!("  âœ… åŠ è½½æˆåŠŸï¼Œå…± {} å¸§\n", audio_frames.len());
    
    // 2. åˆå§‹åŒ– ASR
    println!("[2/5] åˆå§‹åŒ– Whisper ASR...");
    let crate_root = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
    let asr_model_dir = crate_root.join("models/asr/whisper-base");
    
    if !asr_model_dir.exists() {
        return Err(format!("Whisper ASR æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {}", asr_model_dir.display()).into());
    }
    
    let asr = WhisperAsrStreaming::new_from_dir(&asr_model_dir)
        .map_err(|e| format!("Failed to load Whisper ASR: {}", e))?;
    asr.initialize().await
        .map_err(|e| format!("Failed to initialize ASR: {}", e))?;
    println!("  âœ… ASR åˆå§‹åŒ–æˆåŠŸ\n");
    
    // 3. åˆå§‹åŒ– NMTï¼ˆä½¿ç”¨ HTTP å®¢æˆ·ç«¯ï¼‰
    println!("[3/5] åˆå§‹åŒ– M2M100 HTTP NMT å®¢æˆ·ç«¯...");
    let nmt = core_engine::CoreEngineBuilder::new()
        .nmt_with_m2m100_http_client(Some("http://127.0.0.1:5008"))
        .map_err(|e| format!("Failed to create NMT client: {}", e))?;
    
    // ä» builder ä¸­æå– nmtï¼ˆè¿™é‡Œéœ€è¦é‡æ–°è®¾è®¡ï¼Œæš‚æ—¶ç›´æ¥åˆ›å»ºï¼‰
    use std::sync::Arc;
    use core_engine::nmt_client::{LocalM2m100HttpClient, NmtClientAdapter};
    let nmt_client = Arc::new(LocalM2m100HttpClient::new("http://127.0.0.1:5008"));
    let nmt = Arc::new(NmtClientAdapter::new(nmt_client));
    
    nmt.initialize().await
        .map_err(|e| format!("Failed to initialize NMT: {}", e))?;
    println!("  âœ… NMT å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\n");
    
    // 4. åˆå§‹åŒ– TTS
    println!("[4/5] åˆå§‹åŒ– Piper HTTP TTS...");
    let tts_config = PiperHttpConfig::default();
    let tts = PiperHttpTts::new(tts_config)
        .map_err(|e| format!("Failed to create Piper TTS: {}", e))?;
    println!("  âœ… TTS åˆå§‹åŒ–æˆåŠŸ\n");
    
    // 5. å¤„ç†éŸ³é¢‘å¸§
    println!("[5/5] å¤„ç†éŸ³é¢‘å¸§...");
    let mut all_transcripts = Vec::new();
    
    for (idx, frame) in audio_frames.iter().enumerate() {
        println!("  å¤„ç†å¸§ {}/{}...", idx + 1, audio_frames.len());
        
        // ASR
        let asr_request = AsrRequest {
            frame: frame.clone(),
            language_hint: None,
        };
        let asr_result = asr.infer(asr_request).await?;
        
        if let Some(final_transcript) = asr_result.final_transcript {
            println!("    ASR è¾“å‡º: {}", final_transcript.text);
            all_transcripts.push(final_transcript.text.clone());
            
            // NMT
            let target_lang = if direction == "en-zh" { "zh" } else { "en" };
            let translation_request = TranslationRequest {
                transcript: core_engine::types::PartialTranscript {
                    text: final_transcript.text.clone(),
                    confidence: 1.0,
                    is_final: true,
                },
                target_language: target_lang.to_string(),
                wait_k: None,
            };
            
            let translation_result = nmt.translate(translation_request).await?;
            println!("    NMT è¾“å‡º: {}", translation_result.translated_text);
            
            // TTSï¼šæ ¹æ®ç›®æ ‡è¯­è¨€é€‰æ‹©åˆé€‚çš„è¯­éŸ³æ¨¡å‹
            // é—®é¢˜ä¿®å¤ï¼šä¹‹å‰æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡è¯­éŸ³ï¼Œå¯¼è‡´è‹±æ–‡æ–‡æœ¬æ— æ³•æ­£ç¡®å‘éŸ³
            let (tts_voice, tts_locale) = if target_lang == "zh" {
                ("zh_CN-huayan-medium", "zh")
            } else {
                // è‹±æ–‡ç›®æ ‡è¯­è¨€ï¼šå°è¯•ä½¿ç”¨è‹±æ–‡è¯­éŸ³æ¨¡å‹
                ("en_US-lessac-medium", "en")
            };
            
            let tts_request = TtsRequest {
                text: translation_result.translated_text.clone(),
                voice: tts_voice.to_string(),
                locale: tts_locale.to_string(),
            };
            println!("    TTS è¯·æ±‚: voice={}, locale={}, text=\"{}\"", 
                tts_request.voice, tts_request.locale, tts_request.text);
            
            // å°è¯•ä½¿ç”¨ç›®æ ‡è¯­è¨€çš„è¯­éŸ³æ¨¡å‹
            let mut tts_success = false;
            match tts.synthesize(tts_request.clone()).await {
                Ok(result) => {
                    println!("    âœ… TTS å®Œæˆï¼Œç”ŸæˆéŸ³é¢‘é•¿åº¦: {} å­—èŠ‚", result.audio.len());
                    println!("      æ—¶é—´æˆ³: {} ms, æ˜¯å¦æœ€å: {}", result.timestamp_ms, result.is_last);
                    
                    // ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                    let output_dir = PathBuf::from("test_output");
                    if !output_dir.exists() {
                        fs::create_dir_all(&output_dir).unwrap_or_else(|e| {
                            eprintln!("è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {}", e);
                        });
                    }
                    
                    let timestamp = std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap_or_default()
                        .as_secs();
                    let output_file = output_dir.join(format!("tts_output_{}_{}_{}.wav", 
                        idx, target_lang, timestamp));
                    match fs::write(&output_file, &result.audio) {
                        Ok(_) => {
                            println!("    ğŸ’¾ éŸ³é¢‘å·²ä¿å­˜: {}", output_file.display());
                        },
                        Err(e) => {
                            println!("    âš ï¸  ä¿å­˜éŸ³é¢‘å¤±è´¥: {}", e);
                        }
                    }
                    tts_success = true;
                },
                Err(e) => {
                    println!("    âŒ TTS å¤±è´¥: {}", e);
                    
                    // å¦‚æœè‹±æ–‡è¯­éŸ³æ¨¡å‹ä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¸­æ–‡è¯­éŸ³æ¨¡å‹
                    if target_lang == "en" {
                        println!("    âš ï¸  è‹±æ–‡è¯­éŸ³æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ä¸­æ–‡è¯­éŸ³æ¨¡å‹ä½œä¸ºå›é€€...");
                        
                        let fallback_request = TtsRequest {
                            text: translation_result.translated_text.clone(),
                            voice: "zh_CN-huayan-medium".to_string(),
                            locale: "zh".to_string(),
                        };
                        
                        match tts.synthesize(fallback_request).await {
                            Ok(result) => {
                                println!("    âš ï¸  ä½¿ç”¨ä¸­æ–‡è¯­éŸ³æ¨¡å‹ç”Ÿæˆï¼ˆå‘éŸ³å¯èƒ½ä¸å‡†ç¡®ï¼‰ï¼ŒéŸ³é¢‘é•¿åº¦: {} å­—èŠ‚", result.audio.len());
                                
                                // ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆæ ‡è®°ä¸ºå›é€€ï¼‰
                                let output_dir = PathBuf::from("test_output");
                                if !output_dir.exists() {
                                    fs::create_dir_all(&output_dir).unwrap_or_else(|e| {
                                        eprintln!("è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {}", e);
                                    });
                                }
                                
                                let timestamp = std::time::SystemTime::now()
                                    .duration_since(std::time::UNIX_EPOCH)
                                    .unwrap_or_default()
                                    .as_secs();
                                let output_file = output_dir.join(format!("tts_output_{}_{}_fallback_{}.wav", 
                                    idx, target_lang, timestamp));
                                match fs::write(&output_file, &result.audio) {
                                    Ok(_) => {
                                        println!("    ğŸ’¾ éŸ³é¢‘å·²ä¿å­˜ï¼ˆå›é€€ï¼‰: {}", output_file.display());
                                    },
                                    Err(e) => {
                                        println!("    âš ï¸  ä¿å­˜éŸ³é¢‘å¤±è´¥: {}", e);
                                    }
                                }
                                tts_success = true;
                            },
                            Err(e2) => {
                                println!("    âŒ å›é€€ä¹Ÿå¤±è´¥: {}", e2);
                                println!("    âš ï¸  æç¤º: éœ€è¦é…ç½®è‹±æ–‡ TTS æ¨¡å‹æ‰èƒ½ç”Ÿæˆæ­£ç¡®çš„è‹±æ–‡è¯­éŸ³");
                            }
                        }
                    }
                }
            }
            
            if !tts_success {
                println!("    âš ï¸  TTS ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡æ­¤æ­¥éª¤");
            }
        }
    }
    
    println!("\nâœ… å¤„ç†å®Œæˆï¼");
    println!("è¯†åˆ«åˆ°çš„æ–‡æœ¬: {:?}", all_transcripts);
    
    // æ¸…ç†
    asr.finalize().await?;
    nmt.finalize().await?;
    tts.close().await?;
    
    Ok(())
}

