# M2M100 翻译重复问题总结

## 一、问题描述

在 S2S（Speech-to-Speech）端到端集成测试中，M2M100 神经机器翻译（NMT）模型出现输出重复的问题。具体表现为：

- **输入**：英文文本 "Nostub, Inge, and so my fellow Americans ask not what your country can do for you, ask what you can do for your country."
- **期望输出**：正确的中文翻译
- **实际输出**：重复的 "我們 我們"（"我们 我们"）

## 二、技术栈和模型

### 2.1 技术栈

- **编程语言**：Rust
- **推理引擎**：ONNX Runtime (ORT)
- **测试框架**：Rust 标准测试框架
- **音频处理**：WAV 格式，PCM 数据

### 2.2 使用的模型

1. **ASR（自动语音识别）**
   - **模型**：Whisper Base
   - **格式**：GGML (GGUF)
   - **功能**：将英语音频转换为文本
   - **输出**：英文文本

2. **NMT（神经机器翻译）**
   - **模型**：M2M100 (Facebook)
   - **格式**：ONNX
   - **语言对**：英语 → 中文 (en-zh)
   - **模型结构**：
     - Encoder: 12 层，16 注意力头，1024 隐藏维度
     - Decoder: 12 层，16 注意力头，64 头维度
   - **Tokenizer**：自定义实现，基于 SentencePiece BPE
   - **功能**：将英文文本翻译为中文文本

3. **TTS（文本转语音）**
   - **模型**：VITS (vits-zh-aishell3)
   - **格式**：ONNX
   - **采样率**：22050 Hz
   - **功能**：将中文文本转换为中文语音

## 三、测试流程

### 3.1 端到端流程

```
英语音频 (WAV)
    ↓
[ASR: Whisper]
    ↓
英语文本
    ↓
[NMT: M2M100]
    ↓
中文文本
    ↓
[TTS: VITS]
    ↓
中文音频 (WAV)
```

### 3.2 详细步骤

1. **ASR 阶段**
   - 读取输入音频文件：`D:\Programs\github\lingua\test_output\s2s_pipeline_output.wav`
   - 使用 Whisper Base 模型进行语音识别
   - 输出英文文本

2. **NMT 阶段**（问题发生在此阶段）
   - 使用 M2M100 tokenizer 将英文文本编码为 token IDs
   - 运行 M2M100 Encoder，生成 encoder hidden states
   - 运行 M2M100 Decoder，进行自回归解码
   - **当前实现**：非增量解码模式
     - 每次传入完整的已生成序列
     - 使用全零 KV cache（不维护状态）
   - 使用 M2M100 tokenizer 将 token IDs 解码为中文文本

3. **TTS 阶段**
   - 使用 VITS 模型将中文文本转换为语音
   - 保存输出音频：`D:\Programs\github\lingua\test_output\s2s_pipeline_output_zh.wav`

## 四、问题分析

### 4.1 现象

从测试日志可以看到：

```
[Step 0] 生成 token 22 (对应 '▁'，SentencePiece 空格标记)
[Step 1] 生成 token 12549 (对应 '我們')
[Step 2] 生成 token 22 (对应 '▁')
[Step 3] 生成 token 12549 (对应 '我們')
[Step 3] ⚠️  检测到 2-token 重复模式: [12549, 22, 12549, 22], 停止解码
```

最终输出：`我們 我們`（"我们 我们"）

### 4.2 根本原因

**问题核心**：M2M100 的 ONNX Decoder 模型在非增量解码模式下，无法正确理解上下文。

**技术细节**：
1. **当前实现**：非增量解码
   - 每次调用 decoder 时，传入完整的已生成序列（如 `[tgt_lang_id, token1, token2, ...]`）
   - 传入全零的 KV cache（Key-Value 缓存）
   - 不维护任何历史状态

2. **问题所在**：
   - M2M100 的 ONNX Decoder 模型期望接收真实的 KV cache 来维护上下文
   - 传入全零 KV cache 导致模型无法正确理解已生成的序列
   - 模型陷入局部最优，重复生成相同的 token 对

3. **为什么会出现重复**：
   - 模型在 Step 2 看到序列 `[tgt_lang_id, 22, 12549]` 时，由于 KV cache 是全零，无法正确理解上下文
   - 模型认为下一个 token 应该是 `22`（空格）
   - 在 Step 3 看到序列 `[tgt_lang_id, 22, 12549, 22]` 时，同样由于 KV cache 是全零，认为下一个 token 应该是 `12549`（"我們"）
   - 形成 `[22, 12549]` 的重复循环

### 4.3 已尝试的解决方案

1. ✅ **添加重复检测机制**：检测到 2-token 重复模式时停止解码
2. ✅ **添加最大长度限制**：达到 128 个 token 时强制停止
3. ✅ **改回非增量解码**：按照要求不使用 KV cache
4. ❌ **问题仍然存在**：非增量解码模式下，模型无法正确生成翻译

## 五、技术难点

### 5.1 M2M100 ONNX 模型的限制

- M2M100 的 ONNX Decoder 模型**必须**接收 `past_key_values` 作为输入
- 模型不支持"跳过 KV cache"的模式
- 传入全零 KV cache 会导致模型行为异常

### 5.2 非增量解码 vs 增量解码

| 模式 | 优点 | 缺点 | 当前状态 |
|------|------|------|----------|
| **非增量解码** | 不需要维护状态，实现简单 | 无法正确工作（KV cache 问题） | ❌ 失败 |
| **增量解码** | 正确维护上下文，翻译质量好 | 需要维护 KV cache 状态 | ⚠️ 未尝试 |

### 5.3 Tokenizer 实现

- 已实现自定义 M2M100 tokenizer，基于 Trie 数据结构的最长匹配算法
- Tokenizer 的编码/解码功能正常
- 问题不在 tokenizer，而在 decoder 的 KV cache 处理

## 六、建议的解决方案

### 方案 1：使用增量解码（推荐）

**描述**：改用增量解码模式，正确维护 KV cache。

**优点**：
- 符合 M2M100 模型的设计预期
- 能够正确理解上下文，生成正确的翻译
- 性能更好（每次只处理一个 token）

**缺点**：
- 需要维护 KV cache 状态
- 实现复杂度稍高

**实施步骤**：
1. 修改解码循环，使用 `decoder_step` 方法
2. 维护 `DecoderState`，包含 decoder 和 encoder 的 KV cache
3. 每次只传入最后一个生成的 token
4. 使用上一步的 KV cache 作为输入

### 方案 2：检查 ONNX 模型导出

**描述**：检查 M2M100 的 ONNX 模型是否正确导出，是否支持非增量解码。

**可能的问题**：
- ONNX 模型导出时可能没有正确处理 `past_key_values` 的默认值
- 模型可能需要特定的输入格式才能支持非增量解码

**实施步骤**：
1. 检查 ONNX 模型的输入/输出定义
2. 参考 Hugging Face Transformers 的实现
3. 可能需要重新导出 ONNX 模型

### 方案 3：使用其他 NMT 模型

**描述**：如果 M2M100 的 ONNX 模型确实不支持非增量解码，考虑使用其他模型。

**备选方案**：
- Marian NMT（之前已实现，但需要切换到 M2M100）
- 其他支持非增量解码的 NMT 模型

## 七、测试信息

### 7.1 测试文件

- **测试文件**：`core/engine/tests/s2s_pipeline_integration_test.rs`
- **输入音频**：`D:\Programs\github\lingua\test_output\s2s_pipeline_output.wav`
- **输出音频**：`D:\Programs\github\lingua\test_output\s2s_pipeline_output_zh.wav`

### 7.2 测试命令

```powershell
cd D:\Programs\github\lingua\core\engine
cargo test --test s2s_pipeline_integration_test -- --nocapture
```

### 7.3 测试结果

- ✅ **测试通过**：所有断言都通过
- ❌ **翻译质量**：输出重复，无法使用

## 八、决策建议

### 8.1 短期方案（推荐）

**采用方案 1：使用增量解码**

- **理由**：这是最符合 M2M100 模型设计的方案，能够正确生成翻译
- **工作量**：中等（需要修改解码循环，维护 KV cache）
- **风险**：低（增量解码是标准做法）

### 8.2 长期方案

**考虑重新评估模型选择**

- 如果项目确实需要非增量解码（例如，为了简化状态管理）
- 可以考虑使用其他支持非增量解码的 NMT 模型
- 或者与模型提供方确认 M2M100 ONNX 模型的正确使用方式

## 九、相关文件

- **NMT 实现**：`core/engine/src/nmt_incremental/m2m100_*.rs`
- **Tokenizer 实现**：`core/engine/src/nmt_incremental/m2m100_tokenizer.rs`
- **测试文件**：`core/engine/tests/s2s_pipeline_integration_test.rs`
- **模型目录**：`core/engine/models/nmt/m2m100-en-zh/`

## 十、时间线

- **2024-XX-XX**：完成 M2M100 模型集成
- **2024-XX-XX**：发现翻译重复问题
- **2024-XX-XX**：添加重复检测和长度限制
- **2024-XX-XX**：确认问题根因（非增量解码 + 全零 KV cache）

---

**文档版本**：v1.0  
**最后更新**：2024-XX-XX  
**作者**：AI Assistant  
**审核状态**：待决策部门审核

