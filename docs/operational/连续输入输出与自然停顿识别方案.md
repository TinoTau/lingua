# 连续输入输出与自然停顿识别方案

## 一、需求概述

1. **自然停顿识别**：通过 VAD（Voice Activity Detection）模型识别用户说话的自然停顿
2. **连续输入和连续输出**：在截取上一段音频进行翻译的同时，继续接收下一段音频输入
3. **备选方案**：如果无法识别自然停顿，使用固定时间间隔截断

## 二、技术可行性分析

### 2.1 连续输入输出的技术可行性

**答案：完全可行！** 这是典型的**全双工音频处理**场景。

#### 技术原理

1. **音频缓冲队列**：
   - 使用**双缓冲**或**循环缓冲**机制
   - 一个缓冲区用于累积当前音频段（等待 VAD 检测到边界）
   - 另一个缓冲区用于接收新的音频输入
   - 当检测到边界时，将当前缓冲区的内容提交给 ASR，同时切换到新缓冲区

2. **异步处理管道**：
   ```
   音频输入 → 缓冲队列 → VAD 检测 → [边界检测] → ASR 推理 → NMT 翻译 → TTS 合成 → 音频输出
                    ↓
                继续接收新音频
   ```

3. **并发处理**：
   - 使用 Rust 的 `tokio` 异步运行时
   - 音频接收、VAD 检测、ASR 推理、NMT 翻译、TTS 合成可以并行进行
   - 通过 `Arc` 和 `RwLock` 实现线程安全的共享状态

### 2.2 实现架构

```
┌─────────────────────────────────────────────────────────────┐
│                    Audio Input Stream                        │
│  (连续接收音频帧，每帧 ~20-50ms)                              │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│              Audio Buffer Manager (双缓冲)                    │
│  ┌──────────────┐  ┌──────────────┐                         │
│  │ Buffer A     │  │ Buffer B     │                         │
│  │ (当前累积)    │  │ (备用)       │                         │
│  └──────────────┘  └──────────────┘                         │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│              VAD (Voice Activity Detector)                   │
│  - 实时检测语音活动                                           │
│  - 检测自然停顿（静音持续时间 > 阈值）                         │
│  - 输出：is_boundary (是否检测到边界)                         │
└──────────────────────┬──────────────────────────────────────┘
                       │
         ┌─────────────┴─────────────┐
         │                           │
    is_boundary=true          is_boundary=false
         │                           │
         ▼                           ▼
┌──────────────────┐        ┌──────────────────┐
│ 提交当前缓冲区    │        │ 继续累积到缓冲区  │
│ 触发 ASR 推理    │        │ 等待边界检测      │
└────────┬─────────┘        └──────────────────┘
         │
         ▼
┌─────────────────────────────────────────────────────────────┐
│              ASR → NMT → TTS Pipeline                        │
│  (异步处理，不阻塞音频接收)                                    │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│              Audio Output Queue                              │
│  (TTS 音频块队列，按顺序播放)                                  │
└─────────────────────────────────────────────────────────────┘
```

## 三、自然停顿识别方案

### 3.1 VAD 模型选择

#### 方案 A：Silero VAD（推荐）

**优点**：
- 轻量级，延迟低（~10-20ms）
- 支持 ONNX Runtime，易于集成
- 准确率高，专门用于语音活动检测
- 支持多种语言

**模型信息**：
- 模型大小：~1.5MB
- 输入：16kHz 单声道 PCM 音频
- 输出：语音概率（0-1）
- 推理延迟：~10-20ms（CPU），~5-10ms（GPU）

**集成方式**：
```rust
// 使用 ONNX Runtime 加载 Silero VAD 模型
// 对每个音频帧（~512 samples @ 16kHz = 32ms）进行推理
// 如果连续 N 帧（例如 10 帧 = 320ms）的语音概率 < 阈值（例如 0.5），判定为静音
// 如果静音持续时间 > 阈值（例如 500ms），判定为自然停顿边界
```

#### 方案 B：WebRTC VAD

**优点**：
- 无需模型文件，纯算法实现
- 延迟极低（~1-5ms）
- 内存占用小

**缺点**：
- 准确率相对较低
- 对噪声敏感

#### 方案 C：基于能量和过零率的简单 VAD

**优点**：
- 实现简单，无需外部依赖
- 延迟极低

**缺点**：
- 准确率最低
- 无法区分语音和噪声

### 3.2 推荐实现：Silero VAD

**实现步骤**：

1. **下载 Silero VAD 模型**：
   ```bash
   # 模型下载地址
   # https://github.com/snakers4/silero-vad
   # 模型文件：silero_vad.onnx
   ```

2. **集成到 Rust 项目**：
   ```rust
   // core/engine/src/vad/silero_vad.rs
   use ort::{Session, Value};
   
   pub struct SileroVad {
       session: Session,
       sample_rate: u32,
       frame_size: usize,  // 512 samples @ 16kHz = 32ms
       silence_threshold: f32,  // 0.5
       min_silence_duration_ms: u64,  // 500ms
       silence_frame_count: usize,  // 连续静音帧数
   }
   
   impl SileroVad {
       pub fn new(model_path: &str) -> Result<Self> {
           // 加载 ONNX 模型
           let session = Session::builder()?
               .with_execution_providers([ort::ExecutionProvider::CUDA(Default::default())])?
               .commit_from_file(model_path)?;
           
           Ok(SileroVad {
               session,
               sample_rate: 16000,
               frame_size: 512,
               silence_threshold: 0.5,
               min_silence_duration_ms: 500,
               silence_frame_count: 0,
           })
       }
       
       async fn detect_voice_activity(&mut self, audio: &[f32]) -> f32 {
           // 预处理：归一化到 [-1, 1]
           let normalized: Vec<f32> = audio.iter()
               .map(|&x| x.clamp(-1.0, 1.0))
               .collect();
           
           // 推理
           let input = Value::from_array(
               self.session.allocator(),
               &ndarray::Array2::from_shape_vec(
                   (1, normalized.len()),
                   normalized
               )?
           )?;
           
           let outputs = self.session.run(vec![input])?;
           let speech_prob = outputs[0].try_extract::<f32>()?[[0, 0]];
           
           speech_prob
       }
   }
   ```

3. **边界检测逻辑**：
   ```rust
   impl VoiceActivityDetector for SileroVad {
       async fn detect(&mut self, frame: AudioFrame) -> EngineResult<DetectionOutcome> {
           // 1. 检测语音活动
           let speech_prob = self.detect_voice_activity(&frame.data).await?;
           
           // 2. 判断是否为静音
           let is_silence = speech_prob < self.silence_threshold;
           
           // 3. 累积静音帧数
           if is_silence {
               self.silence_frame_count += 1;
           } else {
               self.silence_frame_count = 0;
           }
           
           // 4. 计算静音持续时间
           let silence_duration_ms = (self.silence_frame_count * self.frame_size * 1000) 
               / self.sample_rate as usize;
           
           // 5. 判断是否为边界（自然停顿）
           let is_boundary = silence_duration_ms >= self.min_silence_duration_ms;
           
           Ok(DetectionOutcome {
               is_boundary,
               confidence: speech_prob,
               frame,
           })
       }
   }
   ```

## 四、连续输入输出实现方案

### 4.1 音频缓冲管理器

```rust
// core/engine/src/audio_buffer.rs
use std::collections::VecDeque;
use tokio::sync::RwLock;

pub struct AudioBufferManager {
    // 双缓冲：当前缓冲区和备用缓冲区
    current_buffer: Arc<RwLock<VecDeque<AudioFrame>>>,
    next_buffer: Arc<RwLock<VecDeque<AudioFrame>>>,
    
    // 配置
    max_buffer_duration_ms: u64,  // 最大缓冲时长（防止缓冲区溢出）
    min_segment_duration_ms: u64,  // 最小片段时长（防止过短片段）
}

impl AudioBufferManager {
    pub fn new() -> Self {
        Self {
            current_buffer: Arc::new(RwLock::new(VecDeque::new())),
            next_buffer: Arc::new(RwLock::new(VecDeque::new())),
            max_buffer_duration_ms: 10000,  // 10秒
            min_segment_duration_ms: 200,    // 200ms
        }
    }
    
    /// 添加音频帧到当前缓冲区
    pub async fn push_frame(&self, frame: AudioFrame) -> EngineResult<()> {
        let mut buffer = self.current_buffer.write().await;
        
        // 检查缓冲区是否溢出
        if let Some(first_frame) = buffer.front() {
            let duration = frame.timestamp_ms.saturating_sub(first_frame.timestamp_ms);
            if duration > self.max_buffer_duration_ms {
                // 强制截断：即使没有检测到边界，也提交当前缓冲区
                return Err(EngineError::new("Buffer overflow, forcing boundary"));
            }
        }
        
        buffer.push_back(frame);
        Ok(())
    }
    
    /// 获取当前缓冲区的所有帧（用于 ASR 推理）
    pub async fn take_current_buffer(&self) -> Vec<AudioFrame> {
        let mut buffer = self.current_buffer.write().await;
        let frames: Vec<AudioFrame> = buffer.drain(..).collect();
        frames
    }
    
    /// 切换到下一个缓冲区
    pub async fn swap_buffers(&self) {
        let mut current = self.current_buffer.write().await;
        let mut next = self.next_buffer.write().await;
        std::mem::swap(&mut *current, &mut *next);
    }
    
    /// 检查当前缓冲区是否满足最小片段时长
    pub async fn check_min_duration(&self) -> bool {
        let buffer = self.current_buffer.read().await;
        if buffer.len() < 2 {
            return false;
        }
        
        let first = buffer.front().unwrap();
        let last = buffer.back().unwrap();
        let duration = last.timestamp_ms.saturating_sub(first.timestamp_ms);
        
        duration >= self.min_segment_duration_ms
    }
}
```

### 4.2 修改 CoreEngine 以支持连续处理

```rust
// core/engine/src/bootstrap.rs

impl CoreEngine {
    pub async fn process_audio_frame_continuous(
        &self,
        frame: AudioFrame,
        language_hint: Option<String>,
    ) -> EngineResult<Option<ProcessResult>> {
        // 1. 将帧添加到缓冲区
        self.audio_buffer.push_frame(frame.clone()).await?;
        
        // 2. VAD 检测
        let vad_result = self.vad.detect(frame).await?;
        
        // 3. 如果检测到边界，提交当前缓冲区
        if vad_result.is_boundary {
            // 检查最小片段时长
            if !self.audio_buffer.check_min_duration().await {
                // 片段太短，继续累积
                return Ok(None);
            }
            
            // 获取当前缓冲区的所有帧
            let frames = self.audio_buffer.take_current_buffer().await;
            
            // 切换到下一个缓冲区（继续接收新音频）
            self.audio_buffer.swap_buffers().await;
            
            // 异步处理当前片段（不阻塞音频接收）
            let engine_clone = Arc::new(self.clone());
            let frames_clone = frames.clone();
            let language_hint_clone = language_hint.clone();
            
            tokio::spawn(async move {
                // 合并所有帧为单个音频块
                let merged_audio = merge_frames(&frames_clone);
                
                // 创建合并后的 AudioFrame
                let merged_frame = AudioFrame {
                    sample_rate: frames_clone[0].sample_rate,
                    channels: frames_clone[0].channels,
                    data: merged_audio,
                    timestamp_ms: frames_clone[0].timestamp_ms,
                };
                
                // 执行 ASR → NMT → TTS 管道
                let _result = engine_clone.process_audio_segment(
                    merged_frame,
                    language_hint_clone
                ).await;
            });
            
            // 立即返回，继续接收新音频
            return Ok(None);
        }
        
        // 4. 未检测到边界，继续累积
        Ok(None)
    }
    
    /// 处理音频片段（ASR → NMT → TTS）
    async fn process_audio_segment(
        &self,
        frame: AudioFrame,
        language_hint: Option<String>,
    ) -> EngineResult<ProcessResult> {
        // 原有的 process_audio_frame 逻辑
        // ...
    }
}
```

### 4.3 固定时间间隔截断（备选方案）

如果 VAD 不可用或效果不佳，可以使用固定时间间隔：

```rust
// core/engine/src/vad/time_based_vad.rs

pub struct TimeBasedVad {
    segment_duration_ms: u64,  // 例如 3000ms = 3秒
    last_boundary_time: u64,
}

impl VoiceActivityDetector for TimeBasedVad {
    async fn detect(&mut self, frame: AudioFrame) -> EngineResult<DetectionOutcome> {
        let elapsed = frame.timestamp_ms.saturating_sub(self.last_boundary_time);
        let is_boundary = elapsed >= self.segment_duration_ms;
        
        if is_boundary {
            self.last_boundary_time = frame.timestamp_ms;
        }
        
        Ok(DetectionOutcome {
            is_boundary,
            confidence: 1.0,
            frame,
        })
    }
}
```

## 五、实现优先级

### Phase 1：基础连续处理（1-2天）
1. ✅ 实现 `AudioBufferManager`（双缓冲）
2. ✅ 修改 `CoreEngine` 支持异步处理
3. ✅ 实现固定时间间隔 VAD（备选方案）

### Phase 2：自然停顿识别（3-5天）
1. ⏳ 集成 Silero VAD 模型
2. ⏳ 实现边界检测逻辑
3. ⏳ 调优参数（静音阈值、最小静音时长）

### Phase 3：优化和测试（2-3天）
1. ⏳ 性能优化（减少延迟）
2. ⏳ 边界检测准确性测试
3. ⏳ 用户体验测试

## 六、技术难点和解决方案

### 6.1 音频同步问题

**问题**：多个音频片段并发处理，如何保证输出顺序？

**解决方案**：
- 使用**序列号**标记每个片段
- TTS 输出队列按序列号排序
- 如果某个片段处理较慢，等待前面的片段完成后再播放

### 6.2 缓冲区溢出

**问题**：如果用户一直说话不停顿，缓冲区可能溢出。

**解决方案**：
- 设置最大缓冲时长（例如 10 秒）
- 超过最大时长时，强制截断（即使没有检测到边界）
- 记录警告日志

### 6.3 过短片段处理

**问题**：如果检测到很多短片段（例如 < 200ms），会影响性能。

**解决方案**：
- 设置最小片段时长（例如 200ms）
- 如果片段太短，继续累积到下一个边界
- 合并多个短片段

## 七、性能预期

- **VAD 延迟**：~10-20ms（Silero VAD，CPU）
- **音频缓冲延迟**：~32-50ms（1-2 帧）
- **总延迟**：~50-100ms（从音频输入到边界检测）
- **并发处理**：支持同时处理多个音频片段（ASR、NMT、TTS 并行）

## 八、参考资料

1. **Silero VAD**：
   - GitHub: https://github.com/snakers4/silero-vad
   - 模型下载: https://github.com/snakers4/silero-vad/tree/master/models

2. **ONNX Runtime Rust**：
   - 文档: https://docs.rs/ort/
   - 示例: https://github.com/pykeio/ort

3. **全双工音频处理**：
   - WebRTC: https://webrtc.org/
   - 实时音频流处理最佳实践

